{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"MkDocs cheatsheet","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> <li><code>docker run --rm -it -p 8000:8000 -v ${PWD}:/docs squidfunk/mkdocs-material</code></li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"#testing-material-for-mkdocs","title":"Testing Material for MkDocs","text":"<p>Exemple d'utilisation des admonicons</p> <p>Bug</p> <p>note - abstract - info - tip - success - question - warning - failure - danger - bug</p> <p>Exemple de liste de t\u00e2ches</p> <ul> <li> Lorem ipsum dolor sit amet, consectetur adipiscing elit</li> <li> Vestibulum convallis sit amet nisi a tincidunt<ul> <li> In hac habitasse platea dictumst</li> <li> In scelerisque nibh non dolor mollis congue sed et metus</li> <li> Praesent sed risus massa</li> </ul> </li> <li> Aenean pretium efficitur erat, donec pharetra, ligula non scelerisque</li> </ul> <p>Exemple de tableau</p> Method Description <code>GET</code>      Fetch resource <code>PUT</code>  Update resource <code>DELETE</code>      Delete resource <p>Exemple de bouton</p> <p>Subscribe to our newsletter</p> <pre><code>ceci est du code.\net l\u00e0 encore.\n</code></pre> <p>Bloc de code Python avec titre</p> bubble_sort.py<pre><code>def bubble_sort(items):\n    for i in range(len(items)):\n        for j in range(len(items) - 1 - i):\n            if items[j] &gt; items[j + 1]:\n                items[j], items[j + 1] = items[j + 1], items[j]\n</code></pre> <p>Annotations <pre><code>theme:\n  features:\n    - content.code.annotate # (1)\n</code></pre></p> <ol> <li> I'm a code annotation! I can contain <code>code</code>, formatted     text, images, ... basically anything that can be written in Markdown.</li> </ol> <p>Bloc de code avec num\u00e9rotation des lignes  <pre><code>def bubble_sort(items):\n    for i in range(len(items)):\n        for j in range(len(items) - 1 - i):\n            if items[j] &gt; items[j + 1]:\n                items[j], items[j + 1] = items[j + 1], items[j]\n</code></pre></p> <p>https://mermaid.js.org/</p> <pre><code>graph LR\n  A[Start] --&gt; B{Error?};\n  B --&gt;|Yes| C[Hmm...];\n  C --&gt; D[Debug];\n  D --&gt; B;\n  B ----&gt;|No| E[Yay!];</code></pre> <pre><code>sequenceDiagram\n  autonumber\n  Alice-&gt;&gt;John: Hello John, how are you?\n  loop Healthcheck\n      John-&gt;&gt;John: Fight against hypochondria\n  end\n  Note right of John: Rational thoughts!\n  John--&gt;&gt;Alice: Great!\n  John-&gt;&gt;Bob: How about you?\n  Bob--&gt;&gt;John: Jolly good!</code></pre> <pre><code>stateDiagram-v2\n  state fork_state &lt;&lt;fork&gt;&gt;\n    [*] --&gt; fork_state\n    fork_state --&gt; State2\n    fork_state --&gt; State3\n\n    state join_state &lt;&lt;join&gt;&gt;\n    State2 --&gt; join_state\n    State3 --&gt; join_state\n    join_state --&gt; State4\n    State4 --&gt; [*]</code></pre> <pre><code>classDiagram\n  Person &lt;|-- Student\n  Person &lt;|-- Professor\n  Person : +String name\n  Person : +String phoneNumber\n  Person : +String emailAddress\n  Person: +purchaseParkingPass()\n  Address \"1\" &lt;-- \"0..1\" Person:lives at\n  class Student{\n    +int studentNumber\n    +int averageMark\n    +isEligibleToEnrol()\n    +getSeminarsTaken()\n  }\n  class Professor{\n    +int salary\n  }\n  class Address{\n    +String street\n    +String city\n    +String state\n    +int postalCode\n    +String country\n    -validate()\n    +outputAsLabel()  \n  }</code></pre> <p>Exemple de note de bas de page</p> <p>Lorem ipsum<sup>1</sup> dolor sit amet, consectetur adipiscing elit.<sup>2</sup></p> <ol> <li> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit.\u00a0\u21a9</p> </li> <li> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.\u00a0\u21a9</p> </li> </ol>"},{"location":"000_setup/Command_line_tools/","title":"Command Line Tools","text":"<p>Ce howto d\u00e9crit l'installation des CLIs utiles pour un environnement de d\u00e9veloppement Kubernetes sur macOS.</p> <p>Il sera le point d'entr\u00e9e pour les autres howtos.</p>"},{"location":"000_setup/Command_line_tools/#homebrew","title":"Homebrew","text":"<p>Brew est un 'package manager' pour macOS.</p> <p>Info</p> <p>Homebrew</p> <pre><code>/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n</code></pre>"},{"location":"000_setup/Command_line_tools/#kubectl","title":"kubectl","text":"<p>Info</p> <p>Install and Set Up kubectl on macOS|</p> <pre><code>brew install kubectl\nkubectl version --client\n</code></pre>"},{"location":"000_setup/Command_line_tools/#kubectx-et-kubens","title":"kubectx et kubens","text":"<p>Info</p> <p>kubectx github page|</p> <pre><code>brew install kubectx\nwhich kubectx kubens\n</code></pre>"},{"location":"000_setup/Command_line_tools/#fluxctl","title":"fluxctl","text":"<pre><code>brew install fluxcd/tap/flux\n</code></pre>"},{"location":"000_setup/Command_line_tools/#helm","title":"helm","text":"<pre><code>brew install helm\nhelm version\n</code></pre>"},{"location":"000_setup/Command_line_tools/#discord","title":"Discord","text":"<p>Discord est une messagerie instantan\u00e9e permettant de configurer des 'webhooks' sur ses 'channels' sans devoir payer d'abonnement ou de souscription pour autant, comme c'est le cas avec la plateforme 'Slack' par exemple.</p> <p>Pour l'installer, il faut acc\u00e9der au site web discord.com et t\u00e9l\u00e9charger le client :</p> <pre><code>https://discord.com/api/download?platform=osx\n</code></pre>"},{"location":"000_setup/Command_line_tools/#docker-desktop","title":"Docker Desktop","text":"<p>L'alternative possible reste Rancher.</p> <pre><code>https://www.docker.com/products/docker-desktop/\n</code></pre>"},{"location":"000_setup/Command_line_tools/#material-for-mkdocs","title":"Material for MkDocs","text":"<p>MkDocs est tr\u00e8s pratique pour cr\u00e9er de la documentation \u00e0 partir de fichiers \u00e9crits en MarkDown.</p> <p>Info</p> <p>https://squidfunk.github.io/mkdocs-material/</p> <p>Pour voir directement les modifications apport\u00e9es sur une copie locale, il faut se placer dans le r\u00e9pertoire racine du d\u00e9p\u00f4t de la documentation concern\u00e9e et lancer le conteneur suivant :</p> <pre><code>docker run --rm -it -p 8000:8000 -v ${PWD}:/docs squidfunk/mkdocs-material\n</code></pre>"},{"location":"000_setup/Kubernetes_en_local/","title":"Kubernetes en local","text":"<p>Ce howto d\u00e9crit comment pr\u00e9parer un environnement de d\u00e9veloppement 'Kubernetes-ready' avec 'Kind' et l'Ingress controller 'Nginx' sur son poste de travail.</p> <p>Il sera le point d'entr\u00e9e pour les autres howtos.</p>"},{"location":"000_setup/Kubernetes_en_local/#installation-de-kind","title":"Installation de Kind","text":"<p>Tip</p> <p>Kind = Kubernetes in Docker</p> <pre><code>brew upgrade &amp;&amp; brew install kind\nkind version\n</code></pre>"},{"location":"000_setup/Kubernetes_en_local/#deploiement-dun-cluster-avec-lingress-controller-nginx","title":"D\u00e9ploiement d'un cluster avec l'ingress controller Nginx","text":"<p>Info</p> <p>Kind Ingress</p> <pre><code># Cr\u00e9ation d'un cluster Kind avec extraPortMappings et node-labels\n\ncat &lt;&lt;EOF | kind create cluster --config=-\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnodes:\n- role: control-plane\n  kubeadmConfigPatches:\n  - |\n    kind: InitConfiguration\n    nodeRegistration:\n      kubeletExtraArgs:\n        node-labels: \"ingress-ready=true\"\n  extraPortMappings:\n  - containerPort: 80\n    hostPort: 80\n    protocol: TCP\n  - containerPort: 443\n    hostPort: 443\n    protocol: TCP\nEOF\n\n\n# V\u00e9rification de l'installation\nkind get clusters   # -&gt; nouveau cluster nomm\u00e9 'kind'\n\n\n### D\u00e9ploiement d'un Ingress Controller Nginx\nkubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/kind/deploy.yaml\n</code></pre>"},{"location":"000_setup/Kubernetes_en_local/#test-de-lingress","title":"Test de l'Ingress","text":""},{"location":"000_setup/Kubernetes_en_local/#application-de-test","title":"Application de test","text":"<p>Dans l'exemple qui suit, nous allons cr\u00e9r un Ingress pour router vers des services 'http-hello' tr\u00e8s simples.</p> https://kind.sigs.k8s.io/examples/ingress/usage.yaml<pre><code>kind: Pod\napiVersion: v1\nmetadata:\n  name: foo-app\n  labels:\n    app: foo\nspec:\n  containers:\n  - command:\n    - /agnhost\n    - netexec\n    - --http-port\n    - \"8080\"\n    image: registry.k8s.io/e2e-test-images/agnhost:2.39\n    name: foo-app\n---\nkind: Service\napiVersion: v1\nmetadata:\n  name: foo-service\nspec:\n  selector:\n    app: foo\n  ports:\n  # Default port used by the image\n  - port: 8080\n---\nkind: Pod\napiVersion: v1\nmetadata:\n  name: bar-app\n  labels:\n    app: bar\nspec:\n  containers:\n  - command:\n    - /agnhost\n    - netexec\n    - --http-port\n    - \"8080\"\n    image: registry.k8s.io/e2e-test-images/agnhost:2.39\n    name: bar-app\n---\nkind: Service\napiVersion: v1\nmetadata:\n  name: bar-service\nspec:\n  selector:\n    app: bar\n  ports:\n  # Default port used by the image\n  - port: 8080\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: example-ingress\n  annotations:\n    nginx.ingress.kubernetes.io/rewrite-target: /$2\nspec:\n  rules:\n  - http:\n      paths:\n      - pathType: Prefix\n        path: /foo(/|$)(.*)\n        backend:\n          service:\n            name: foo-service\n            port:\n              number: 8080\n      - pathType: Prefix\n        path: /bar(/|$)(.*)\n        backend:\n          service:\n            name: bar-service\n            port:\n              number: 8080\n</code></pre>"},{"location":"000_setup/Kubernetes_en_local/#testons-lingress-controller","title":"Testons l'Ingress controller","text":"Test<pre><code># Test \nkubectl apply -f https://kind.sigs.k8s.io/examples/ingress/usage.yaml\ncurl localhost/foo/hostname     # renvoie 'foo-app'\ncurl localhost/bar/hostname     # renvoie 'bar-app'\n</code></pre>"},{"location":"000_setup/Kubernetes_en_local/#nettoyage","title":"Nettoyage","text":"Cleaning<pre><code>kubectl delete ingress example-ingress\nkubectl delete services foo-service bar-service\nkubectl delete pods foo-app bar-app\n</code></pre>"},{"location":"000_setup/Kubernetes_en_local/#compatibilite-du-cluster-avec-kube-prometheus-stack","title":"Compatibilit\u00e9 du cluster avec 'kube-prometheus-stack'","text":"Doc URL Medium - fix KinD missing Prometheus operator targets https://medium.com/@charled.breteche/kind-fix-missing-prometheus-operator-targets-1a1ff5d8c8ad <p>Pour \u00e9viter de r\u00e9installer compl\u00e8tement notre cluster au moment o\u00f9 nous nous int\u00e9resserons \u00e0 Prometheus, nous allons tout de suite r\u00e9installer ce dernier et appliquer un patch qui permettra de corriger le fait que certaines 'targets' ne soient pas accessibles car elles \u00e9coutent sur l'adresse loopback par d\u00e9faut. Bien s\u00fbr nous conserverons notre Ingress Controller Nginx.</p> <p></p> <pre><code>kind delete cluster --name=kind\n\ncat &lt;&lt;EOF | kind create cluster --config=-\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nkubeadmConfigPatches:\n- |-\n  kind: ClusterConfiguration\n  # configure controller-manager bind address\n  controllerManager:\n    extraArgs:\n      bind-address: 0.0.0.0\n  # configure etcd metrics listen address\n  etcd:\n    local:\n      extraArgs:\n        listen-metrics-urls: http://0.0.0.0:2381\n  # configure scheduler bind address\n  scheduler:\n    extraArgs:\n      bind-address: 0.0.0.0\n- |-\n  kind: KubeProxyConfiguration\n  # configure proxy metrics bind address\n  metricsBindAddress: 0.0.0.0\nnodes:\n- role: control-plane\n  kubeadmConfigPatches:\n  - |\n    kind: InitConfiguration\n    nodeRegistration:\n      kubeletExtraArgs:\n        node-labels: \"ingress-ready=true\"\n  extraPortMappings:\n  - containerPort: 80\n    hostPort: 80\n    protocol: TCP\n  - containerPort: 443\n    hostPort: 443\n    protocol: TCP\nEOF\n\n# V\u00e9rification de l'installation\nkind get clusters   # -&gt; nouveau cluster nomm\u00e9 'kind'\n\n### D\u00e9ploiement d'un Ingress Controller Nginx\nkubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/kind/deploy.yaml\n</code></pre>"},{"location":"Docker/build_pull_image_on_dockerhub/","title":"Cr\u00e9er et publier une image sur DockerHub","text":"<p>Ce howto montre comment cr\u00e9er et publier une image Docker sur DockerHub \u00e0 partir d'un exemple simple.</p>"},{"location":"Docker/build_pull_image_on_dockerhub/#limage-docker","title":"L'image Docker","text":"<p>Le conteneur va afficher une page web contenant le nom d'h\u00f4te du conteneur. Le fond de la page est blanc par d\u00e9faut mais il est possible de le changer en surchargeant la variable HTML_COLOR.</p>"},{"location":"Docker/build_pull_image_on_dockerhub/#dockerfile","title":"Dockerfile","text":"Dockerfile<pre><code>FROM nginx:latest\nCOPY nginx-custom-index.sh /docker-entrypoint.d/\nRUN chmod a+x /docker-entrypoint.d/nginx-custom-index.sh\n</code></pre>"},{"location":"Docker/build_pull_image_on_dockerhub/#script","title":"Script","text":"<p>Le script ci-dessous cr\u00e9\u00e9 la page web. Il doit \u00eatre plac\u00e9 dans le m\u00eame r\u00e9pertoire que le fichier Dockerfile.</p> nginx-custom-index.sh<pre><code>#!/bin/sh\n\n# This script creates a custom html indexpage \n# which displays the host name of the container/pod \n# with a background color given by the HTML_COLOR variable.\n\nDEFAULT_HTML_COLOR=\"white\"\nHTML_ROOT_DIR=\"/usr/share/nginx/html\"\n\n[[ -z ${HTML_COLOR} ]] &amp;&amp; HTML_COLOR=${DEFAULT_HTML_COLOR}\n\nprintf \"&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n  &lt;head&gt;\n    &lt;meta charset=\"utf-8\" /&gt;\n    &lt;title&gt;${HTML_COLOR}&lt;/title&gt;\n    &lt;style&gt;\n      body {\n        background-color: ${HTML_COLOR};\n      }\n    &lt;/style&gt;\n  &lt;/head&gt;\n    &lt;body&gt;\n        &lt;div style=\"text-align:center\"&gt;\n          ${HOSTNAME}\n        &lt;/div&gt;\n    &lt;/body&gt;\n&lt;/html&gt;\\n\" &gt; ${HTML_ROOT_DIR}/index.html\n</code></pre>"},{"location":"Docker/build_pull_image_on_dockerhub/#build-de-limage","title":"Build de l'image","text":"<pre><code># build\ndocker image build -t color-app:v1 .\n\n# check\ndocker image ls color-app\n</code></pre>"},{"location":"Docker/build_pull_image_on_dockerhub/#execution-de-limage","title":"Ex\u00e9cution de l'image","text":""},{"location":"Docker/build_pull_image_on_dockerhub/#comportement-par-defaut","title":"Comportement par d\u00e9faut","text":"<pre><code>## docker run\ndocker container run -d -p 80:80 --name color-app color-app:v1\n\n\n# check the output\ncurl http://localhost:80/\n\n  &lt;!DOCTYPE html&gt;\n  &lt;html&gt;\n    &lt;head&gt;\n      &lt;meta charset=utf-8 /&gt;\n      &lt;title&gt;white&lt;/title&gt;\n      &lt;style&gt;\n        body {\n          background-color: white;\n        }\n      &lt;/style&gt;\n    &lt;/head&gt;\n      &lt;body&gt;\n          &lt;div style=text-align:center&gt;\n            082f7627d71b\n          &lt;/div&gt;\n      &lt;/body&gt;\n  &lt;/html&gt;\n</code></pre>"},{"location":"Docker/build_pull_image_on_dockerhub/#surcharge-de-la-couleur-de-fond","title":"Surcharge de la couleur de fond","text":"<pre><code># docker run\ndocker container run -d -p 80:80 --name color-app -e HTML_COLOR=blue color-app:v1\n\n# check the output\ncurl http://localhost:80/\n\n  &lt;!DOCTYPE html&gt;\n  &lt;html&gt;\n    &lt;head&gt;\n      &lt;meta charset=utf-8 /&gt;\n      &lt;title&gt;blue&lt;/title&gt;\n      &lt;style&gt;\n        body {\n          background-color: blue;\n        }\n      &lt;/style&gt;\n    &lt;/head&gt;\n      &lt;body&gt;\n          &lt;div style=text-align:center&gt;\n            9bc86e325288\n          &lt;/div&gt;\n      &lt;/body&gt;\n  &lt;/html&gt;\n</code></pre>"},{"location":"Docker/build_pull_image_on_dockerhub/#push-de-limage-sur-dockerhub","title":"Push de l'image sur DockerHub","text":"<pre><code>docker tag color-app:v1 &lt;registry&gt;/color-app:v1\ndocker login\ndocker push &lt;registry&gt;/color-app:v1\n</code></pre> <p>Tip</p> <p>Mon Docker ID : zigouigoui</p> <p>Mon image sera accessible \u00e0 l'adresse suivante : <pre><code>https://hub.docker.com/repository/docker/zigouigoui/color-app\n</code></pre></p>"},{"location":"Exposition%20Kubernetes/NginX_Ingress_Controler/","title":"NginX Ingress Controler","text":""},{"location":"Exposition%20Kubernetes/NginX_Ingress_Controler/#exposition-des-applications","title":"Exposition des applications","text":"<p>L'exposition des applications h\u00e9berg\u00e9es sur le cluster doit \u00eatre g\u00e9r\u00e9e en dehors des applications. Nous allons d\u00e9finir les r\u00e8gles de routage de notre Ingress controller Nginx dans le d\u00e9p\u00f4t GitHub d\u00e9di\u00e9 \u00e0 FluxCD :</p> <p>Note</p> <p>Bien que le contr\u00f4leur Ingress puisse \u00eatre d\u00e9ploy\u00e9 dans n'importe quel namespace, il est g\u00e9n\u00e9ralement d\u00e9ploy\u00e9 dans un namespace distinct de vos services d'application (par exemple, ingress ou kube-system). Il peut voir les r\u00e8gles Ingress dans tous les autres espaces de noms et les r\u00e9cup\u00e9rer. Cependant, chaque r\u00e8gle Ingress doit r\u00e9sider dans l'espace de noms o\u00f9 r\u00e9side l'application qu'elle configure.</p> <pre><code>export LOCAL_GITHUB_REPOS=\"${HOME}/code/github\"\n\ncd ${LOCAL_GITHUB_REPOS}/k8s-kind-fluxcd\n\ncat &lt;&lt; EOF &gt;&gt; apps/foo/ingress.yaml\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: foo\n  namespace: foo\n  annotations:\n    nginx.ingress.kubernetes.io/rewrite-target: /$2\nspec:\n  rules:\n  - http:\n      paths:\n      - pathType: Prefix\n        path: /foo(/|$)(.*)\n        backend:\n          service:\n            name: foo\n            port:\n              number: 8080\nEOF\n\ncat &lt;&lt; EOF &gt;&gt; apps/bar/ingress.yaml\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: bar\n  namespace: bar\n  annotations:\n    nginx.ingress.kubernetes.io/rewrite-target: /$2\nspec:\n  rules:\n  - http:\n      paths:\n      - pathType: Prefix\n        path: /bar(/|$)(.*)\n        backend:\n          service:\n            name: bar\n            port:\n              number: 8080\nEOF\n\ngit add .\ngit commit -m \"feat: setting up Ingress routes for foo and bar.\"\ngit push\n\nflux reconcile kustomization flux-system --with-source\n</code></pre> <p>Testons le bon fonctionnement de nos routes :</p> <pre><code>curl http://localhost/foo           # -&gt; NOW: 2024-04-27 18:14:24.152568822 +0000 UTC m=+5403.294573418%\ncurl http://localhost/foo/hostname  # -&gt; foo-9d658c7db-x6v84%\n\ncurl http://localhost/bar           # -&gt; NOW: 2024-04-27 18:14:26.677481395 +0000 UTC m=+5108.710059928%\ncurl http://localhost/bar/hostname  # -&gt; bar-5c7c6495ff-954bh%\n</code></pre> <p>Le routage fonctionne comme attendu ! </p>"},{"location":"FluxCD/","title":"Index","text":"Topic URL Kind install https://kind.sigs.k8s.io/docs/user/quick-start Flux install https://fluxcd.io/flux/installation/ Flux alerts https://fluxcd.io/flux/monitoring/alerts/ Flux - notification controller - discord provider https://fluxcd.io/flux/components/notification/providers/#discord FluxCD - image policies - filter tags https://fluxcd.io/flux/components/image/imagepolicies/#filter-tags FluxCD - Guides - Automate image updates to Git https://fluxcd.io/flux/guides/image-update/ Force FluxCD reconciliation flux reconcile kustomization flux-system --with-source Medium - How to make ahs share your own Helm package https://medium.com/containerum/how-to-make-and-share-your-own-helm-package-50ae40f6c221 GitHub Pages https://pages.github.com/ OCI repositories https://fluxcd.io/flux/components/source/ocirepositories/"},{"location":"FluxCD/#pre-requisites","title":"Pre-requisites","text":""},{"location":"FluxCD/#my-variables","title":"My variables","text":"variable value KIND_CLUSTER_NAME sandbox GITHUB_USERNAME papafrancky GITHUB_PAT \\$( cat ~/secrets/github.${GITHUB_USERNAME}.PAT.FluxCD.txt ) DISCORD_WEBHOOK \\$( cat ${HOME}/secrets/discord.gitops.webhook.txt ) WORKING_DIR ${HOME}/code/github"},{"location":"FluxCD/#kind-install","title":"Kind install","text":"<pre><code>brew upgrade &amp;&amp; brew install kind\nkind version\nkind create cluster --name ${KIND_CLUSTER_NAME}\nkind get clusters\n</code></pre>"},{"location":"FluxCD/#flux-cli-install","title":"Flux CLI install","text":"bashhomebrew <pre><code>curl -s https://fluxcd.io/install.sh | sudo bash\n</code></pre> <pre><code>brew install fluxcd/tap/flux\n</code></pre>"},{"location":"FluxCD/#github-repositories","title":"GitHub repositories :","text":"<p>2 GitHub repos must be created to meet our needs :</p> USAGE URL One for FluxCD itself https://github.com/${GITHUB_USERNAME}/gitops Another one for the apps to be deployed https://github.com/${GITHUB_USERNAME}/gitops-deployments"},{"location":"FluxCD/#make-sure-your-kubernetes-cluster-complies-with-fluxcd","title":"Make sure your Kubernetes cluster complies with FluxCD","text":"<pre><code>flux check --pre\n</code></pre>"},{"location":"FluxCD/#boostrap-fluxcd","title":"Boostrap FluxCD","text":"<p>To do this, you will need a GitHub classic Personal Access Token (PAT) with full repo access.</p> <pre><code>export GITHUB_USER=${GITHUB_USERNAME}\nexport GITHUB_TOKEN=${GITHUB_PAT}\n\nflux bootstrap github \\\n  --token-auth \\\n  --owner ${GITHUB_USER} \\\n  --repository gitops \\\n  --branch=main \\\n  --path=clusters/${KIND_CLUSTER_NAME} \\\n  --personal \\\n  --components-extra=image-reflector-controller,image-automation-controller\n\n-&gt; Check with your browser :\nhttps://github.com/${GITHUB_USERNAME}/gitops/tree/main/clusters/${KIND_CLUSTER_NAME}/flux-system\n\nkubectl -n flux-system get all\n</code></pre>"},{"location":"FluxCD/#alerting-discord","title":"Alerting (Discord)","text":""},{"location":"FluxCD/#local-stuff","title":"Local stuff","text":"<pre><code># Clone 'gitops' repo locally :\ncd ${WORKING_DIR}\ngit clone git@github.com:${GITHUB_USERNAME}/gitops.git\n\n# Create directories related to notifications :\ncd gitops/clusters/${KIND_CLUSTER_NAME}\nmkdir -p notifications/{providers,alerts}\n\n# Create a Kubernetes secrets from the Discord webhook :\nk create secret generic discord-gitops --from-file=address=${HOME}/secrets/discord.gitops.webhook.txt\n</code></pre>"},{"location":"FluxCD/#create-an-alert-provider-on-the-gitops-discord-channel","title":"Create an alert-provider on the #gitops Discord channel","text":"<pre><code>flux create alert-provider discord-gitops \\\n  --type=discord \\\n  --secret-ref=discord-gitops \\\n  --channel=gitops \\\n  --username=FluxCD \\\n  --namespace=default \\\n  --export &gt; notifications/providers/discord-gitops-provider.yaml\n\ncat notifications/providers/discord-gitops-provider.yaml\n  #  ---\n  #  apiVersion: notification.toolkit.fluxcd.io/v1beta3\n  #  kind: Provider\n  #  metadata:\n  #    name: discord-gitops\n  #    namespace: default\n  #  spec:\n  #    channel: gitops\n  #    secretRef:\n  #      name: discord-gitops\n  #    type: discord\n  #    username: FluxCD\n</code></pre>"},{"location":"FluxCD/#create-an-alert-on-git-repos-and-kustomizations","title":"Create an alert on Git repos and Kustomizations","text":"<pre><code>flux create alert discord-gitops-alert \\\n  --event-severity=info \\\n  --event-source='GitRepository/*,Kustomization/*' \\\n  --provider-ref=discord-gitops \\\n  --namespace=default \\\n  --export &gt; notifications/alerts/discord-gitops-alert.yaml\n\ncat notifications/alerts/discord-gitops-alert.yaml\n  # ---\n  # apiVersion: notification.toolkit.fluxcd.io/v1beta3\n  # kind: Alert\n  # metadata:\n  #   name: discord-gitops-alert\n  #   namespace: default\n  # spec:\n  #   eventSeverity: info\n  #   eventSources:\n  #   - kind: GitRepository\n  #     name: '*'\n  #   - kind: Kustomization\n  #     name: '*'\n  #   providerRef:\n  #     name: discord-gitops\n\n\ncd ${WORKING_DIR}/gitops\ngit st\ngit add notifications\ngit commit -m 'feat: discord alerting'\ngit push\n\n\nkubectl get providers,alerts -n default\n  # NAME                                                        AGE\n  # provider.notification.toolkit.fluxcd.io/discord-gitops      32m\n  #\n  # NAME                                                        AGE\n  # alert.notification.toolkit.fluxcd.io/discord-gitops-alert   29s\n</code></pre>"},{"location":"FluxCD/#allow-fluxcd-to-authenticate-to-github","title":"Allow FluxCD to authenticate to GitHub","text":""},{"location":"FluxCD/#secret-creation-ssh-keys","title":"Secret creation (SSH keys)","text":"<pre><code>flux create secret git gitops-deployments-auth \\\n  --url=ssh://github.com/${GITHUB_USERNAME}/gitops-deployments \\\n  --namespace=default\n\n  # \u271a deploy key: ecdsa-sha2-nistp384 AAAAE2VjZHNhLXNoYTItbmlzdHAzODQAAAAIbmlzdHAzODQAAABhBEww+J8GaJDlxQHeB6M+qrWyn3hcv2Jj8IS5gC+O6kQOvu2hKr0iqaduoottECNXEgRbdEqABzY8gZ9Xb77e5wfskVUOqKfdiv12/CVbLFj1eH1WFlUH+Vy7Wff0I0JEAw==\n  #\n  # \u25ba git secret 'gitops-deployments-auth' created in 'default' namespace\n</code></pre>"},{"location":"FluxCD/#add-the-newly-created-public-key-to-the-gitops-deployments-github-repository","title":"Add the newly created public key to the 'gitops-deployments' Github repository","text":"<ul> <li>The public key appears as the 'deploy key' at its creation time.</li> <li>Add the public key (deploy key) in https://github.com/${GITHUB_USERNAME}/gitops-deployments/settings/keys/new; give it the name 'gitops' et check the box 'Allow write access'.</li> </ul>"},{"location":"FluxCD/#clone-the-apps-github-repository-locally","title":"Clone the apps Github repository locally","text":"<pre><code>git clone git@github.com:${GITHUB_USERNAME}/gitops-deployments.git\n</code></pre>"},{"location":"FluxCD/#configure-fluxcd-for-automated-deployments","title":"Configure FluxCD for automated deployments","text":""},{"location":"FluxCD/#create-a-gitrepository-manifest","title":"Create a GitRepository manifest","text":"<pre><code>cd ${WORKING_DIR}/gitops/clusters/${KIND_CLUSTER_NAME}\nmkdir -p {sources,kustomizations}\n\nflux create source git nginxhello \\\n  --url=ssh://github.com/${GITHUB_USERNAME}/gitops-deployments \\\n  --branch=main \\\n  --secret-ref=gitops-deployments-auth \\\n  --namespace=default \\\n  --export &gt; sources/nginxhello-source.yaml\n\ncat sources/nginxhello-source.yaml\n\n  # ---\n  # apiVersion: source.toolkit.fluxcd.io/v1\n  # kind: GitRepository\n  # metadata:\n  #   name: nginxhello\n  #   namespace: default\n  # spec:\n  #   interval: 1m0s\n  #   ref:\n  #     branch: main\n  #   secretRef:\n  #     name: gitops-deployments-auth\n  #   url: ssh://github.com/${GITHUB_USERNAME}/gitops-deployments\n</code></pre>"},{"location":"FluxCD/#create-a-kustomization-manifest","title":"Create a Kustomization manifest","text":"<pre><code>  flux create kustomization nginxhello \\\n    --source=GitRepository/nginxhello.default \\\n    --path=./nginxhello \\\n    --prune=true \\\n    --target-namespace=default \\\n    --namespace=default \\\n    --export &gt; kustomizations/nginxhello-kustomization.yaml\n\n  cat kustomizations/nginxhello-kustomization.yaml\n\n  # ---\n  # apiVersion: kustomize.toolkit.fluxcd.io/v1\n  # kind: Kustomization\n  # metadata:\n  #   name: nginxhello\n  #   namespace: default\n  # spec:\n  #   interval: 1m0s\n  #   path: ./nginxhello\n  #   prune: true\n  #   sourceRef:\n  #     kind: GitRepository\n  #     name: nginxhello\n  #     namespace: default\n  #    targetNamespace: default\n</code></pre>"},{"location":"FluxCD/#push-changes-to-github-repo","title":"Push changes to GitHub repo","text":"<pre><code>git add sources kustomizations\ngit commit -m \"\"\ngit push\n</code></pre> <p>-&gt; From now on, FluxCD will reconcile the apps Github repository.</p>"},{"location":"FluxCD/#check-fluxcd-manages-the-new-objects-as-expected","title":"check FluxCD manages the new objects as expected","text":"<pre><code>kubectl get GitRepositories -n default\n\n  # NAME         URL                                               AGE     READY   STATUS\n  # nginxhello   ssh://github.com/${GITHUB_USERNAME}/gitops-deployments   2m14s   False   git repository is empty\n\n\nkubectl get kustomizations -n default\n\n  # NAME         AGE    READY   STATUS\n  # nginxhello   3m1s   False   Source artifact not found, retrying in 30s\n</code></pre>"},{"location":"FluxCD/#add-an-application-into-the-apps-github-repository","title":"Add an application into the apps Github repository","text":"<p>Copy the 'deployment.yaml' and 'service.yaml' manifests : * from Nigel Brown's example repository ( https://github.com/nbrownuk/gitops-nginxhello/ ) * to our Github repo dedicated to FluxCD ( https://github.com/${GITHUB_USERNAME}/gitops ) </p> <p>Then, push the changes to your 'gitops' repo : </p> <pre><code>cd ${WORKING_DIR}/ gitops\ngit add .\ngit commit -m 'feat: added the application 'nginxhello'.'\ngit push\n</code></pre> <p>Finally, let's check wether FluxCD manages the deployment of the application as expected :</p> <pre><code>kubectl get GitRepositories\n\n  # NAME         URL                                               AGE   READY   STATUS\n  # nginxhello   ssh://github.com/${GITHUB_USERNAME}/gitops-deployments   26m   True    stored artifact for revision 'main@sha1:b223021b6ff0fee832941e0825a6203e4775196c'\n\n\nkubectl get kustomizations\n\n  # NAME         AGE   READY   STATUS\n  # nginxhello   28h   True    Applied revision: main@sha1:3a1755be8df43fc45bb467490aa0adc52117a4e7\n\n\nkubectl get services\n\nkubectl port-forward service/nginxhello 8080:80\n  # -&gt; open a browser and check if you can see nginxhello's page correctly : http://localhost:8080\n</code></pre>"},{"location":"FluxCD/#testing-a-modification-on-the-newly-deployed-application","title":"Testing a modification on the newly deployed application","text":"<p>We will change the number of replicas and the version of the application :</p> <pre><code>vi ${WORKING_DIR}/gitops-deployments/nginxhello/deployment.yaml\n  # -&gt; changer :\n  #      .spec.replicas : 2 -&gt; 4\n  #      .spec.template.spec.containers[0].image : nbrown/nginxhello:1.19.0 -&gt; nbrown/nginxhello:1.24.0\n</code></pre> <p>Push the changes to GitHub :</p> <pre><code>cd ${WORKING_DIR}/gitops-deployments\ngit add .\ngit commit -m 'evol: changed replicas number and app version.'\ngit push\n</code></pre> <p>Let's observe FluxCD's reconciliation :</p> <pre><code># Watch the pods during the operation :\nkubectl get po -w\n\n# Check the _'nginxhello'_ GitRepository status : it should show the SHA1 of the commit (cf. _'git logs'_) :\nkubectl get GitRepositories\n\n# Check you #gitops Discord channel : you should have received 2 new alerts.\n\n# Connect to the _'nginxhello'_ app :\nkubectl port-forward service/nginxhello 8080:80 \n-&gt; open a browser and access the following URL: http://localhost:8080 -&gt; the app version should be 1.24.0\n</code></pre>"},{"location":"FluxCD/#handling-application-updates-with-image-automation","title":"Handling application updates with image automation","text":""},{"location":"FluxCD/#create-an-imagerepository","title":"Create an ImageRepository","text":"<pre><code>cd ${WORKING_DIR}/gitops/clusters/${KIND_CLUSTER_NAME}\nflux create image repository nginxhello \\\n  --image=nbrown/nginxhello \\\n  --interval=5m \\\n  --namespace=default \\\n  --export &gt; sources/nginxhello-imagerepository.yaml\n\ncat sources/nginxhello-imagerepository.yaml\n# ---\n# apiVersion: image.toolkit.fluxcd.io/v1beta2\n# kind: ImageRepository\n# metadata:\n#   name: nginxhello\n#   namespace: default\n# spec:\n#   image: nbrown/nginxhello\n#   interval: 5m0s\n</code></pre> <p>Push the changes to GitHub :</p> <pre><code>cd ${WORKING_DIR}/gitops\ngit add .\ngit commit -m 'feat: added nginxhello image repository manifest.' \ngit push\n\nkubectl get imagerepositories\n</code></pre>"},{"location":"FluxCD/#modify-the-discord-alert-to-manage-image-policies","title":"Modify the Discord alert to manage image policies","text":"<pre><code>cd ${WORKING_DIR}/gitops/clusters/${KIND_CLUSTER_NAME}\nvi notifications/alerts/discord-gitops-alert.yaml\n\n# ---\n# apiVersion: notification.toolkit.fluxcd.io/v1beta3\n# kind: Alert\n# metadata:\n#   name: discord-gitops-alert\n#   namespace: default\n# spec:\n#   eventSeverity: info\n#   eventSources:\n#   - kind: GitRepository\n#     name: '*'\n#   - kind: Kustomization\n#     name: '*'\n#   - kind: ImagePolicy\n#     name: '*'\n#   providerRef:\n#     name: discord-gitops\n</code></pre>"},{"location":"FluxCD/#create-an-image-policy","title":"Create an image policy","text":"<pre><code>cd ${WORKING_DIR}/gitops/clusters/${KIND_CLUSTER_NAME}\nmkdir imagepolicies\nflux create image policy nginxhello \\\n  --image-ref=nginxhello \\\n  --select-semver='&gt;=1.20.x' \\\n  --namespace=default \\\n  --export &gt; imagepolicies/nginxhello-image-policy.yaml\n\ncat imagepolicies/nginxhello-image-policy.yaml\n\n# ---\n# apiVersion: image.toolkit.fluxcd.io/v1beta2\n# kind: ImagePolicy\n# metadata:\n#   name: nginxhello\n#   namespace: default\n# spec:\n#   imageRepositoryRef:\n#     name: nginxhello\n#   policy:\n#     semver:\n#       range: '&gt;=1.20.x'\n</code></pre> <p>Push the changes to GitHub :</p> <pre><code>cd ${WORKING_DIR}/gitops\ngit add .\ngit commit -m \"feat: added a new image policy\"\ngit push\n\nkubectl get imagepolicies\n\n  # NAME         LATESTIMAGE\n  # nginxhello\n\nkubectl describe imagerepository nginxhello\n\n  # Name:         nginxhello\n  # Namespace:    default\n  # Labels:       kustomize.toolkit.fluxcd.io/name=flux-system\n  #               kustomize.toolkit.fluxcd.io/namespace=flux-system\n  # Annotations:  &lt;none&gt;\n  # API Version:  image.toolkit.fluxcd.io/v1beta2\n  # Kind:         ImageRepository\n  # Metadata:\n  #   Creation Timestamp:  2023-12-29T17:27:59Z\n  #   Finalizers:\n  #     finalizers.fluxcd.io\n  #   Generation:        2\n  #   Resource Version:  47347\n  #   UID:               0eaf8380-5fad-4458-aca0-826f02d74abc\n  # Spec:\n  #   Exclusion List:\n  #     ^.*\\.sig$\n  #   Image:     docker.io/nbrown/nginxhello\n  #   Interval:  5m0s\n  #   Provider:  generic\n  # Status:\n  #   Canonical Image Name:  index.docker.io/nbrown/nginxhello\n  #   Conditions:\n  #     Last Transition Time:  2023-12-29T18:16:39Z\n  #     Message:               successful scan: found 45 tags\n  #     Observed Generation:   2\n  #     Reason:                Succeeded\n  #     Status:                True\n  #     Type:                  Ready\n  #   Last Scan Result:\n  #     Latest Tags:\n  #       stable\n  #       mainline\n  #       latest\n  #       e6c463e6\n  #       aad042cb\n  #       1.25.2\n  #       1.25\n  #       1.24.0\n  #       1.24\n  #       1.23.3\n  #     Scan Time:  2023-12-29T18:16:39Z\n  #     Tag Count:  45\n  #   Observed Exclusion List:\n  #     ^.*\\.sig$\n  #   Observed Generation:  2\n  # Events:\n  # (...)\n</code></pre> <p>You should have received an alert on the '#gitops' Discord channel : </p> <pre><code>imagepolicy/nginxhello.default\nLatest image tag for 'docker.io/nbrown/nginxhello' resolved to 1.25.2\n</code></pre> <p>Some additiona checks can be done :</p> <pre><code>kubectl get imagepolicies\n\n  # NAME         LATESTIMAGE\n  # nginxhello   docker.io/nbrown/nginxhello:1.25.2\n\n\nkubectl get deployment nginxhello -o yaml | yq '.spec.template.spec.containers[].image'\n\n  # nbrown/nginxhello:1.20.1    # the deployed version isn't the latest one.\n</code></pre>"},{"location":"FluxCD/#updating-our-application-version-with-the-image-automation","title":"Updating our application version with the image automation","text":""},{"location":"FluxCD/#adding-a-marker-to-the-deployment","title":"Adding a marker to the deployment","text":"<pre><code>cd ${WORKING_DIR}/gitops-deployments\nvi ${WORKING_DIR}/gitops-deployments/nginxhello/deployment.yaml\n\n  # ---\n  # apiVersion: apps/v1\n  # kind: Deployment\n  # metadata:\n  #   labels:\n  #     app: nginxhello\n  #   name: nginxhello\n  #   annotations:\n  #     fluxcd.io/ignore: \"true\"\n  #     fluxcd.io/automated: \"false\"\n  # spec:\n  #   replicas: 4\n  #   selector:\n  #     matchLabels:\n  #       app: nginxhello\n  #   template:\n  #     metadata:\n  #       labels:\n  #         app: nginxhello\n  #     spec:\n  #       containers:\n  #       - image: docker.io/nbrown/nginxhello:1.25.2 # {\"$imagepolicy\": \"default:nginxhello\"}\n  #         name: nginxhello\n  #         ports:\n  #         - containerPort: 80\n  #         env:\n  #         - name: NODE_NAME\n  #           valueFrom:\n  #             fieldRef:\n  #               fieldPath: spec.nodeName\n  #         livenessProbe:\n  #           initialDelaySeconds: 2\n  #           periodSeconds: 2\n  #           httpGet:\n  #             port: 80\n  #             path: /healthz/live\n  #         readinessProbe:\n  #           initialDelaySeconds: 2\n  #           periodSeconds: 2\n  #           httpGet:\n  #             port: 80\n  #             path: /healthz/ready\n\ngit add .\ngit commit -m \"feat: added a version marker to the _'nginxhello'_ deployment manifest.\"\ngit push\n</code></pre> <p>Let's check the SHA1 of our commit :</p> <pre><code>git log\n\n  # commit ed03554ba51d9f1e29f38711d667efebd3c9e33c (HEAD -&gt; main, origin/main)\n  # Author: Franck Levesque &lt;franck.levesque@gmail.com&gt;\n  # Date:   Sat Dec 30 13:29:46 2023 +0100\n  #\n  # feat: added a version marker to the _'nginxhello'_ deployment manifest.\n</code></pre> <p>We should see the same SHA1 commit in the gitRepository now :</p> <pre><code>kubectl get gitrepository nginxhello\n\n  # NAME         URL                                               AGE     READY   STATUS\n  # nginxhello   ssh://github.com/${GITHUB_USERNAME}/gitops-deployments   2d18h   True    stored artifact for revision 'main@sha1:ed03554ba51d9f1e29f38711d667efebd3c9e33c'\n</code></pre> <p>It should also appear in a Discord alert :</p> <pre><code>FluxCD BOT\n \u2014 Aujourd\u2019hui \u00e0 13:31\ngitrepository/nginxhello.default\nstored artifact for commit 'evol: added maker to the deployment manifest'\nrevision\nmain@sha1:ed03554ba51d9f1e29f38711d667efebd3c9e33c\n</code></pre>"},{"location":"FluxCD/#image-update-automation","title":"Image update automation","text":"<p>Write a template in order to format the commits created by FluxCD :</p> <pre><code>cd ${WORKING_DIR}/gitops/clusters/${KIND_CLUSTER_NAME}\nmkdir imageupdateautomations\nvi imageupdateautomations/msg_template\n\n  # Flux automated image update\n  # \n  # Automation name: {{ .AutomationObject }}\n  # \n  # Files:\n  # {{ range $filename, $_ := .Updated.Files -}}\n  # - {{ $filename }}\n  # {{ end -}}\n  # \n  # Objects:\n  # {{ range $resource, $_ := .Updated.Objects -}}\n  # - {{ $resource.Kind }} {{ $resource.Name }}\n  # {{ end -}}\n  # \n  # Images:\n  # {{ range .Updated.Images -}}\n  # - {{.}}\n  # {{ end -}}\n</code></pre> <p>Create the imageUpdateAutomation manufest :</p> <pre><code>  flux create image update nginxhello \\\n    --git-repo-ref=nginxhello \\\n    --git-repo-path=./nginxhello \\\n    --checkout-branch=main \\\n    --push-branch=main \\\n    --author-name=FluxCD \\\n    --author-email=gitops@users.noreply.github.com \\\n    --commit-template=\"$( cat ${WORKING_DIR}/gitops/clusters/${KIND_CLUSTER_NAME}/imageupdateautomations/msg_template )\" \\\n    --namespace=default \\\n    --export &gt; ${WORKING_DIR}/gitops/clusters/${KIND_CLUSTER_NAME}/imageupdateautomations/nginxhello.yaml\n</code></pre> <p>cat ${WORKING_DIR}/gitops/clusters/${KIND_CLUSTER_NAME}/imageupdateautomations/nginxhello.yaml</p> <pre><code>  # ---\n  # apiVersion: image.toolkit.fluxcd.io/v1beta1\n  # kind: ImageUpdateAutomation\n  # metadata:\n  #   name: nginxhello\n  #   namespace: default\n  # spec:\n  #   git:\n  #     checkout:\n  #       ref:\n  #         branch: main\n  #     commit:\n  #       author:\n  #         email: gitops@users.noreply.github.com\n  #         name: FluxCD\n  #       messageTemplate: |-\n  #         Flux automated image update\n  # \n  #         Automation name: {{ .AutomationObject }}\n  # \n  #         Files:\n  #         {{ range $filename, $_ := .Updated.Files -}}\n  #         - {{ $filename }}\n  #         {{ end -}}\n  # \n  #         Objects:\n  #         {{ range $resource, $_ := .Updated.Objects -}}\n  #         - {{ $resource.Kind }} {{ $resource.Name }}\n  #         {{ end -}}\n  # \n  #         Images:\n  #         {{ range .Updated.Images -}}\n  #         - {{.}}\n  #         {{ end -}}\n  #     push:\n  #       branch: main\n  #   interval: 1m0s\n  #   sourceRef:\n  #     kind: GitRepository\n  #     name: gitops-deployments\n  #   update:\n  #     path: ./nginxhello\n  #     strategy: Setters\n</code></pre> <p>Push the changes to GitHub :</p> <pre><code>cd ${WORKING_DIR}/gitops\ngit add .\ngit commit -m \"feat: created an image update automation manifest for nginxhello.\"\ngit push\n</code></pre> <p>Check the changes :</p> <pre><code>kubectl get pods -w\nkubectl get imageupdateautomations\nk get deployment nginxhello -o yaml | yq '.spec.template.spec.containers[].image'\n\n  # nbrown/nginxhello:1.20.1\n  # This is not the latest version.\n\ncd ${WORKING_DIR}/gitops-deployments\ngit fetch      # our branch is not up-to-date (1 commit missing).\ngit pull\ngit log -1     # the commit message looks like our commit template.\n\ncat ${WORKING_DIR}/gitops-deployments/nginxhello/deployment.yaml| grep 'image:'\n\n  # - image: docker.io/nbrown/nginxhello:1.25.2 # {\"$imagepolicy\": \"default:nginxhello\"}\n  # -&gt; latest version : 1.25.2\n\nkubectl describe imageupdateautomations nginxhello\n\n  # Flux automated image update\n  # \n  # Automation name: default/nginxhello\n  # \n  # Files:\n  # - deployment.yaml\n  # Objects:\n  # - Deployment nginxhello\n  # \n  # \n  # Images:\n  # - docker.io/nbrown/nginxhello:1.25.2\n</code></pre>"},{"location":"FluxCD/#automating-packages-releases-with-the-helm-controller","title":"Automating packages releases with the Helm controller","text":""},{"location":"FluxCD/#cleaning","title":"Cleaning","text":"<pre><code>cd ${WORKING_DIR}/gitops/clusters/${KIND_CLUSTER_NAME}\nmkdir _backup\ntar cvzf _backup/imagepolicies.tgz imagepolicies\ntar cvzf _backup/imageupdateautomations.tgz imageupdateautomations\ntar cvzf _backup/kustomizations.tgz kustomizations \ntar cvzf _backup/sources_nginxhello-imagerepository.yaml.tgz sources/nginxhello-imagerepository.yaml\nrm -rf image* kustomizations sources/nginxhello-imagerepository.yaml\n\ntree\n\n  # .\n  # \u251c\u2500\u2500 _backup\n  # \u2502   \u251c\u2500\u2500 imagepolicies.tgz\n  # \u2502   \u251c\u2500\u2500 imageupdateautomations.tgz\n  # \u2502   \u251c\u2500\u2500 kustomizations.tgz\n  # \u2502   \u2514\u2500\u2500 sources_nginxhello-imagerepository.yaml.tgz\n  # \u251c\u2500\u2500 flux-system\n  # \u2502   \u251c\u2500\u2500 gotk-components.yaml\n  # \u2502   \u251c\u2500\u2500 gotk-sync.yaml\n  # \u2502   \u2514\u2500\u2500 kustomization.yaml\n  # \u251c\u2500\u2500 notifications\n  # \u2502   \u251c\u2500\u2500 alerts\n  # \u2502   \u2502   \u2514\u2500\u2500 discord-gitops-alert.yaml\n  # \u2502   \u2514\u2500\u2500 providers\n  # \u2502       \u2514\u2500\u2500 discord-gitops-provider.yaml\n  # \u2514\u2500\u2500 sources\n  #     \u2514\u2500\u2500 nginxhello-source.yaml\n\ncd ${WORKING_DIR}/gitops\ngit add .\ngit commit -m 'feat: cleaning before using Helm charts.'\ngit push\n</code></pre> <p>You should receive an alert in the '#gitops' Discord channel :</p> <pre><code>FluxCD BOT\n \u2014 Aujourd\u2019hui \u00e0 16:04\nkustomization/nginxhello.default\nDeployment/default/nginxhello deleted\nService/default/nginxhello deleted\nrevision\nmain@sha1:5855cb796a4e1b7cd07d3c9da8351debb0dff3f5\n</code></pre> <p>Normally, you shouldn't see any pods again :</p> <pre><code>kubectl get pods -n default\n\n  # No resources found in default namespace.\n</code></pre>"},{"location":"FluxCD/#configuring-a-helm-repository","title":"Configuring a Helm Repository","text":""},{"location":"FluxCD/#discord-alerting","title":"Discord alerting","text":"<p>First, let's modify the Discord alerts to manage the helmRepositories into account :</p> <pre><code>vi notifications/alerts/discord-gitops-alert.yaml\n\n# ---\n# apiVersion: notification.toolkit.fluxcd.io/v1beta3\n# kind: Alert\n# metadata:\n#   name: discord-gitops-alert\n#   namespace: default\n# spec:\n#   eventSeverity: info\n#   eventSources:\n#   - kind: GitRepository\n#     name: '*'\n#   - kind: Kustomization\n#     name: '*'\n#   - kind: ImagePolicy\n#     name: '*'\n#   - kind: HelmRepository\n#     name: '*'\n#   providerRef:\n#     name: discord-gitops\n</code></pre>"},{"location":"FluxCD/#the-podinfo-helm-chart","title":"The 'podinfo' Helm Chart","text":"<p>We will use the 'podinfo' Helm chart as an example.</p> <pre><code>helm show chart oci://ghcr.io/stefanprodan/charts/podinfo\n\n  # Pulled: ghcr.io/stefanprodan/charts/podinfo:6.5.4\n  # Digest: sha256:a961643aa644f24d66ad05af2cdc8dcf2e349947921c3791fc3b7883f6b1777f\n  # apiVersion: v1\n  # appVersion: 6.5.4\n  # description: Podinfo Helm chart for Kubernetes\n  # home: https://github.com/stefanprodan/podinfo\n  # kubeVersion: '&gt;=1.23.0-0'\n  # maintainers:\n  # - email: stefanprodan@users.noreply.github.com\n  #   name: stefanprodan\n  # name: podinfo\n  # sources:\n  # - https://github.com/stefanprodan/podinfo\n  # version: 6.5.4\n</code></pre>"},{"location":"FluxCD/#creating-the-podinfo-helmrepository","title":"Creating the 'podinfo' helmRepository","text":""},{"location":"FluxCD/#authenticating-to-the-helm-repository","title":"Authenticating to the Helm repository","text":"<p>Let's create a new 'Docker registry' type secret allowinf us to retrieve the Helm chart. (ghcr.io belongs to GitHub; they both use the same identity management)</p> <p>NOTE : this repository is a public one; in our case there will be no need to specify credentials in the helmRepository.</p> <pre><code>export GITHUB_USER=${GITHUB_USERNAME}\nexport GITHUB_TOKEN=$(GITHUB_PAT)\n\nkubectl create secret docker-registry ghcr-charts-auth \\\n  --docker-server=ghcr.io \\\n  --docker-username=${GITHUB_USER} \\\n  --docker-password=-{GITHUB_TOKEN}\n</code></pre>"},{"location":"FluxCD/#creating-the-helmrepository-manifest","title":"Creating the helmRepository manifest","text":"<pre><code>flux create source helm podinfo \\\n  --url=https://stefanprodan.github.io/podinfo \\\n  --namespace=default \\\n  --interval=1m \\\n  --export &gt; ${WORKING_DIR}/gitops/clusters/${KIND_CLUSTER_NAME}/sources/podinfo.yaml\n\n\ncat ${WORKING_DIR}/gitops/clusters/${KIND_CLUSTER_NAME}/sources/podinfo.yaml\n\n  # ---\n  # apiVersion: source.toolkit.fluxcd.io/v1beta2\n  # kind: HelmRepository\n  # metadata:\n  #   name: podinfo\n  #   namespace: default\n  # spec:\n  #   interval: 1m0s\n  #   url: https://stefanprodan.github.io/podinfo\n</code></pre>"},{"location":"FluxCD/#pushing-the-changes-to-github","title":"Pushing the changes to GitHub","text":"<pre><code>cd ${WORKING_DIR}/gitops\ngit add .\ngit commit -m \"feat: Defining a 'podinfo' Helm repository.\"\ngit push\n\n\nkubectl get helmrepo\n\n# NAME      URL                                      AGE    READY   STATUS\n# podinfo   https://stefanprodan.github.io/podinfo   116s   True    stored artifact: revision 'sha256:faeeeb1a7a887b5fe4d440164d29f58ba6f186d46fdf069fd227c39e9fc6ae09'\n</code></pre>"},{"location":"FluxCD/#the-podinfo-helmrelease","title":"The 'podinfo' helmRelease","text":"<pre><code>cd ${WORKING_DIR}/gitops/clusters/${KIND_CLUSTER_NAME}/\nmkdir helmreleases\n\ntree\n\n  # .\n  # \u251c\u2500\u2500 _backup\n  # \u2502   \u251c\u2500\u2500 imagepolicies.tgz\n  # \u2502   \u251c\u2500\u2500 imageupdateautomations.tgz\n  # \u2502   \u251c\u2500\u2500 kustomizations.tgz\n  # \u2502   \u2514\u2500\u2500 sources_nginxhello-imagerepository.yaml.tgz\n  # \u251c\u2500\u2500 flux-system\n  # \u2502   \u251c\u2500\u2500 gotk-components.yaml\n  # \u2502   \u251c\u2500\u2500 gotk-sync.yaml\n  # \u2502   \u2514\u2500\u2500 kustomization.yaml\n  # \u251c\u2500\u2500 helmreleases\n  # \u251c\u2500\u2500 notifications\n  # \u2502   \u251c\u2500\u2500 alerts\n  # \u2502   \u2502   \u2514\u2500\u2500 discord-gitops-alert.yaml\n  # \u2502   \u2514\u2500\u2500 providers\n  # \u2502       \u2514\u2500\u2500 discord-gitops-provider.yaml\n  # \u2514\u2500\u2500 sources\n  #     \u251c\u2500\u2500 nginxhello-source.yaml\n  #     \u2514\u2500\u2500 podinfo.yaml\n</code></pre> <p>If you need to customize the configuration of your Helm release, you can find all the parameters for the Values file here :</p> <pre><code>https://artifacthub.io/packages/helm/podinfo/podinfo\n</code></pre> <p>In our case, we will only want to add a simple \"Hello\" message in the UI :</p> <pre><code>echo 'ui.message: Hello' &gt; ${WORKING_DIR}/gitops/clusters/podinfo.values.yaml\n</code></pre> <p>Now, let's create the helmRelease object :</p> <pre><code>flux create helmrelease podinfo \\\n  --source=HelmRepository/podinfo \\\n  --chart=podinfo \\\n  --values=${WORKING_DIR}/gitops/clusters/podinfo.values.yaml \\\n  --namespace=default \\\n  --export &gt; ${WORKING_DIR}/gitops/clusters/${KIND_CLUSTER_NAME}/helmreleases/podinfo.yaml\n\n\ncat ${WORKING_DIR}/gitops/clusters/${KIND_CLUSTER_NAME}/helmreleases/podinfo.yaml\n\n# apiVersion: helm.toolkit.fluxcd.io/v2beta2\n# kind: HelmRelease\n# metadata:\n#   name: podinfo\n#   namespace: default\n# spec:\n#   chart:\n#     spec:\n#       chart: podinfo\n#       reconcileStrategy: ChartVersion\n#       sourceRef:\n#         kind: HelmRepository\n#         name: podinfo\n#   interval: 1m0s\n#   values:\n#     ui.message: Hello\n</code></pre> <p>Push the changes to GitHub :</p> <pre><code>cd ${WORKING_DIR}/gitops\ngit add .\ngit commit -m \"feat: Defining a 'podinfo' Helm release.\"\ngit push\n</code></pre> <p>Let's check the good deployment of the 'podinfo' Helm release :</p> <pre><code>kubectl get helmreleases\n\n# NAME      AGE   READY   STATUS\n# podinfo   78m   True    Helm install succeeded for release default/podinfo.v1 with chart podinfo@6.5.4\n\n\n# Is the pod running ?\nkubectl get po\n\n# NAME                      READY   STATUS    RESTARTS   AGE\n# podinfo-8c4b88bf8-2j8sd   1/1     Running   0          16m\n\n\n# Let's access the _'podinfo'_ web page : \nkubectl port-forward service/podinfo 8080:9898\n\n# Forwarding from 127.0.0.1:8080 -&gt; 9898\n# Forwarding from [::1]:8080 -&gt; 9898\n# Handling connection for 8080\n-&gt; open a browser : http://localhost:8080/\n</code></pre>"},{"location":"FluxCD/#remediation","title":"remediation","text":"<p>Read the proper documentation for more details. Remediation allows to rollback for instance afer 2 unsuccessful update tries :</p> <p>In the example below, we will add to the HelmRelease manifest :</p> <pre><code>.spec.upgrade.remedation.retries: 2\n</code></pre> <p>Let's edit the manifest :</p> <pre><code>vi ${WORKING_DIR}/gitops/clusters/${KIND_CLUSTER_NAME}/helmreleases/podinfo.yaml\n\n  # ---\n  # apiVersion: helm.toolkit.fluxcd.io/v2beta2\n  # kind: HelmRelease\n  # metadata:\n  #   name: podinfo\n  #   namespace: default\n  # spec:\n  #   chart:\n  #     spec:\n  #       chart: podinfo\n  #       reconcileStrategy: ChartVersion\n  #       sourceRef:\n  #         kind: HelmRepository\n  #         name: podinfo\n  #   interval: 1m0s\n  #   values:\n  #     ui.message: Hello\n  #   upgrade:\n  #     remediation:\n  #       retries: 2\n</code></pre>"},{"location":"FluxCD/FlucCD_arborescence_par_produit/","title":"FluxCD - Proposition de nouvelle organisation des manifests","text":"<p>Nous voulons tester une mani\u00e8re d'organiser notre repo Flux en suivant une approche par produit/application.</p> <p>Nous d\u00e9crirons comment installer from scratch un cluster kubernetes (kind) de d\u00e9veloppement qui h\u00e9bergera dans un premier temps une application 'podinfo' r\u00e9cup\u00e9r\u00e9e depuis un repo Git d\u00e9di\u00e9.</p>"},{"location":"FluxCD/FlucCD_arborescence_par_produit/#pre-requis","title":"Pre-requis","text":"<ul> <li>un cluster Kubernetes pret ( kind create cluster --name=development )</li> <li>un repo GitHub nomm\u00e9 'kubernetes-development' dans lequel FluxCD sera bootsrap\u00e9;</li> <li>un channel Discord avec un channel nomm\u00e9 'podinfo-development' (d\u00e9di\u00e9 aux notifications de l'appli podinfo pour l'environnement de d\u00e9veloppement) et un webhook d\u00e9j\u00e0 configur\u00e9.</li> </ul>"},{"location":"FluxCD/FlucCD_arborescence_par_produit/#bootstrap-de-fluxcd","title":"Bootstrap de FluxCD","text":"<pre><code>export GITHUB_USER=papaFrancky\nexport GITHUB_TOKEN=&lt;my_github_personal_access_token&gt;\n\nflux bootstrap github \\\n  --token-auth \\\n  --owner papaFrancky \\\n  --repository kubernetes-development \\\n  --branch=main \\\n  --path=. \\\n  --personal \\\n  --components-extra=image-reflector-controller,image-automation-controller\n</code></pre> <p>-&gt; V\u00e9rification avec un navigateur :</p> <pre><code>https://github.com/papaFrancky/kubernetes-development/tree/main\n</code></pre> <p>-&gt; V\u00e9rification avec kubectl :</p> <pre><code>kubectl -n flux-system get all\n</code></pre>"},{"location":"FluxCD/FlucCD_arborescence_par_produit/#organisation-des-manifests-dans-le-repository-git-dedie-a-fluxcd","title":"Organisation des manifests dans le repository Git d\u00e9di\u00e9 \u00e0 FluxCD","text":"<p>Nous allons organiser les manifests en les regroupant par produit.</p> <p>Au m\u00eame niveau que le r\u00e9pertoire 'kubernetes-development' (ie. le nom donn\u00e9 \u00e0 notre cluster Kubernetes de d\u00e9veloppement), nous allons cr\u00e9er un r\u00e9pertoire 'products' qui contiendra les repos Git clon\u00e9s des produits g\u00e9r\u00e9s par FluxCD. Dans le repo Git de FluxCD, nomm\u00e9 kubernetes-development car il correspond au cluster de d\u00e9veloppement dans notre exemple, nous d\u00e9crirons comment Flux g\u00e8rera nos produits sur le cluster dans le r\u00e9pertoire products et et dans autant de sous-r\u00e9pertoires qu'il y aura de produits \u00e0 g\u00e9rer.</p> <p>L'arborescence ressemblera \u00e0 quelque-chose comme ceci :</p> <pre><code>${WORKING_DIRECTORY}\n\u251c\u2500\u2500 kubernetes-development\n\u2502   \u251c\u2500\u2500 README.md\n\u2502   \u251c\u2500\u2500 flux-system\n\u2502   \u2502   \u251c\u2500\u2500 gotk-components.yaml\n\u2502   \u2502   \u251c\u2500\u2500 gotk-sync.yaml\n\u2502   \u2502   \u2514\u2500\u2500 kustomization.yaml\n\u2502   \u2514\u2500\u2500 products\n\u2502       \u251c\u2500\u2500 nginxhello\n\u2502       \u2502   \u251c\u2500\u2500 ...\n\u2502       \u2502   \u251c\u2500\u2500 ...\n\u2502       \u2502   \u2514\u2500\u2500 ...\n\u2502       \u2514\u2500\u2500 podinfo\n\u2502           \u251c\u2500\u2500 git-repository.yaml\n\u2502           \u251c\u2500\u2500 image-policy.yaml\n\u2502           \u251c\u2500\u2500 image-repository.yaml\n\u2502           \u251c\u2500\u2500 image-update-automation.yaml\n\u2502           \u251c\u2500\u2500 namespace.yaml\n\u2502           \u251c\u2500\u2500 notification-alert.yaml\n\u2502           \u251c\u2500\u2500 notification-provider.yaml\n\u2502           \u2514\u2500\u2500 sync.yaml\n\u251c\u2500\u2500 products\n\u2502   \u251c\u2500\u2500 nginxhello\n\u2502   \u2502   \u251c\u2500\u2500 ...\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u2514\u2500\u2500 podinfo\n\u2502       \u251c\u2500\u2500 ...\n\u2502       \u251c\u2500\u2500 ...\n\u2502       \u251c\u2500\u2500 ...\n</code></pre>"},{"location":"FluxCD/FlucCD_arborescence_par_produit/#mise-en-place-de-la-gestion-dune-application-par-fluxcd","title":"Mise en place de la gestion d'une application par FluxCD","text":"<p>Nous prendrons comme exemple l'application podinfo de Stefan Prodan. La premi\u00e8re chose \u00e0 faire est de cloner en local notre repo Git d\u00e9di\u00e9 \u00e0 FluxCD pour notre cluster de d\u00e9veloppement :</p> <pre><code>cd ${WORKING_DIRECTORY}\ngit clone git@github.com:${GITHUB_USERNAME}/kubernetes-development.git\n</code></pre> <p>Nous allons rassembler tous les manifests de param\u00e9trage dans un r\u00e9pertoire d\u00e9di\u00e9 \u00e0 l'application :</p> <pre><code>cd kubernetes-development\nmkdir -p products/podinfo\ncd products/podinfo\n</code></pre>"},{"location":"FluxCD/FlucCD_arborescence_par_produit/#namespace-dedie-a-lapplication","title":"Namespace d\u00e9di\u00e9 \u00e0 l'application","text":"<p>Podinfo disposera de son propre namespace.</p> <pre><code>kubectl create namespace podinfo --dry-run=client -o yaml &gt; namespace.yaml\n\ncat namespace.yaml\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: podinfo\n\ngit add .\ngit commit -m 'created namespace podinfo.'\ngit push\n\n\nkubectl get namespace podinfo\n\n  # NAME      STATUS   AGE\n  # podinfo   Active   21s\n</code></pre>"},{"location":"FluxCD/FlucCD_arborescence_par_produit/#configuration-des-notifications-provider-discord","title":"Configuration des notifications (provider: Discord)","text":"<p>Nous souhaitons \u00eatre inform\u00e9s via un service de messagerie des modifications apport\u00e9es \u00e0 l'application. Notre choix se porte sur Discord.</p>"},{"location":"FluxCD/FlucCD_arborescence_par_produit/#creation-du-channel-discord-dedie-a-lapplication-podinfo","title":"Cr\u00e9ation du channel Discord d\u00e9di\u00e9 \u00e0 l'application podinfo","text":"<p>Cr\u00e9er un channel nomm\u00e9 podinfo-development dans son 'serveur' Discord et recopier le webhook cr\u00e9\u00e9 par d\u00e9faut.</p>"},{"location":"FluxCD/FlucCD_arborescence_par_produit/#enregistrement-du-webhook-du-channel-discord-dans-un-secret-kubernetes","title":"Enregistrement du webhook du channel Discord dans un Secret Kubernetes","text":"<pre><code>DISCORD_WEBHOOK=https://discord.com/api/webhooks/1204170006032818296/D6-rBzJHb1EAfPOtuVbzIqs2goJTuoCn-1AUCef-HZN2xZvK9Mkjolg29dc3z1vqIPuf\n\nkubectl -n podinfo create secret generic discord-podinfo-development-webhook --from-literal=address=${DISCORD_WEBHOOK} \n\nkubectl -n podinfo get secret discord-podinfo-development-webhook\n\n  # NAME                                  TYPE     DATA   AGE\n  # discord-podinfo-development-webhook   Opaque   1      134m \n</code></pre>"},{"location":"FluxCD/FlucCD_arborescence_par_produit/#creation-du-notification-provider","title":"Cr\u00e9ation du 'notification provider'","text":"<p>Info</p> <p>https://fluxcd.io/flux/components/notification/providers/#discord</p> <pre><code>flux create alert-provider discord \\\n  --type=discord \\\n  --secret-ref=discord-webhook \\\n  --channel=kubernetes-development \\\n  --username=FluxCD \\\n  --namespace=podinfo \\\n  --export &gt; notification-provider.yaml\n\n\ncat notification-provider.yaml\n\n  ---\n  apiVersion: notification.toolkit.fluxcd.io/v1beta3\n  kind: Provider\n  metadata:\n    name: discord\n    namespace: podinfo\n  spec:\n    channel: kubernetes-development\n    secretRef:\n      name: discord-podinfo-development-webhook\n    type: discord\n    username: FluxCD\n</code></pre>"},{"location":"FluxCD/FlucCD_arborescence_par_produit/#configuration-des-alertes-discord","title":"Configuration des alertes Discord","text":"<pre><code>flux create alert discord \\\n  --event-severity=info \\\n  --event-source='GitRepository/*,Kustomization/*,ImageRepository/*,ImagePolicy/*,HelmRepository/*' \\\n  --provider-ref=discord \\\n  --namespace=podinfo \\\n  --export &gt; notification-alert.yaml\n\n\ncat notifications/alerts/discord.yaml\n\n  ---\n  apiVersion: notification.toolkit.fluxcd.io/v1beta3\n  kind: Alert\n  metadata:\n    name: discord\n    namespace: podinfo\n  spec:\n    eventSeverity: info\n    eventSources:\n    - kind: GitRepository\n      name: '*'\n    - kind: Kustomization\n      name: '*'\n    - kind: ImageRepository\n      name: '*'\n    - kind: ImagePolicy\n      name: '*'\n    - kind: HelmRepository\n      name: '*'\n    providerRef:\n      name: discord\n</code></pre>"},{"location":"FluxCD/FlucCD_arborescence_par_produit/#enregistrement-des-modifications","title":"Enregistrement des modifications","text":"<pre><code>cd ${WORKING_DIRECTORY}/kubernetes-development\ngit st\ngit add .\ngit commit -m 'configuring discord alerting.'\ngit push\n\n\nkubectl get providers,alerts -n podinfo\n\n  NAME                                              AGE\n  provider.notification.toolkit.fluxcd.io/discord   54s\n\n  NAME                                              AGE\n  alert.notification.toolkit.fluxcd.io/discord      54s\n</code></pre>"},{"location":"FluxCD/FlucCD_arborescence_par_produit/#creation-du-repository-git-dedie-a-lapplication-podinfo","title":"Cr\u00e9ation du repository Git d\u00e9di\u00e9 \u00e0 l'application 'podinfo'","text":"<p>Pour simuler le d\u00e9veloppement d'une application, nous allons cr\u00e9er un repository Git sur notre compte \u00e0 partir d'une application existante : 'podinfo' de Stefan Prodan.</p> <p>Dans GitHub, on cr\u00e9\u00e9 un nouveau repository nomm\u00e9 'podinfo-development' ( https://github.com/${GITHUB_USERNAME}/podinfo-development.git ).</p> <p>Ensuite on utilise le bouton 'import' pour r\u00e9cup\u00e9rer le projet https://github.com/stefanprodan/podinfo -&gt; Nous avons d\u00e9sormais une copie de l'application 'podinfo' dans notre propre repo GitHub 'podingo-development'.</p>"},{"location":"FluxCD/FlucCD_arborescence_par_produit/#recuperons-le-repository-localement-dans-le-repertoire-dedie-aux-produits-et-renommons-le","title":"R\u00e9cup\u00e9rons le repository localement dans le r\u00e9pertoire d\u00e9di\u00e9 aux produits et renommons-le","text":"<p>Le repository s'appelle podinfo-development car nous partons du principe que le produit disposera d'un repo par environnement.</p> <pre><code>cd ${WORKING_DIRECTORY}\nmkdir products &amp;&amp; cd products\ngit clone git@github.com:${GITHUB_USERNAME}/podinfo-development.git\nmv podinfo-development podinfo\n</code></pre> <p>-&gt; Nous avons d\u00e9sormais un r\u00e9pertoire 'products/podinfo'. Dans le r\u00e9pertoire 'kustomize' se trouvent les manifests qui nous int\u00e9ressent pour d\u00e9ployer le produit.</p>"},{"location":"FluxCD/FlucCD_arborescence_par_produit/#modification-des-manifests-pour-deployer-lapplication-dans-le-namespace-eponyme","title":"Modification des manifests pour d\u00e9ployer l'application dans le namespace \u00e9ponyme","text":"<p>Notez que nous voulons que le produit soit d\u00e9ploy\u00e9 dans le namespace 'podinfo'.</p> <p>Il est donc n\u00e9cessaire d'ajouter dans les manifests deployment.yaml, hpa.yaml et service.yaml le param\u00e8tre suivant : </p> <pre><code>.data.namespace=podinfo\n</code></pre> <p>Une fois les manifests modifi\u00e9s, il faut les commiter et les pousser sur la branche main du repository.</p> <p>!!! ce serait int\u00e9ressant de passer par Flux pour g\u00e9rer ce param\u00e8tre sans modifier les manifests dans leur repo Github !!!</p> <p>Nous allons \u00e9galement profiter de ce moment pour 'downgrader' la version de l'image du conteneur : dans le manifest 'deployment.yaml', nous allons modifier  ''.spec.template.spec.containers[].image'' comme suit :</p> <pre><code>    cr.io/stefanprodan/podinfo:6.5.4 -&gt; ghcr.io/stefanprodan/podinfo:6.5.0\n</code></pre> <p>Cela nous servira plus tard avec l'ImageAutomation.</p>"},{"location":"FluxCD/FlucCD_arborescence_par_produit/#generation-des-deploy-keys-pour-le-repo-github-de-lapplication","title":"G\u00e9n\u00e9ration des deploy keys pour le repo GitHub de l'application","text":"<p>Nous devons d\u00e9sormais cr\u00e9er une paire de cl\u00e9s SSH pour permettre \u00e0 FluxCD de se connecter avec les droits d'\u00e9criture au repo applicatif.</p> <pre><code>flux create secret git podinfo-gitrepository \\\n  --url=ssh://github.com/${GITHUB_USERNAME}/podinfo-development \\\n  --namespace=podinfo\n</code></pre> <p>La cl\u00e9 publique (deploy key) doit \u00eatre ajout\u00e9e dans les settings du repo GitHub :  https://github.com/${GITHUB_USERNAME}/podinfo-development/settings/keys/new</p> <p>Warning</p> <p>Cocher la case 'Allow write access' !!!</p> <p>Cliquer sur le bouton \"Add Key\" et renseigner son mot de passe pour confirmer</p>"},{"location":"FluxCD/FlucCD_arborescence_par_produit/#creation-du-gitrepository-podinfo-development","title":"Cr\u00e9ation du GitRepository 'podinfo-development'","text":"<pre><code>cd ${WORKING_DIRECTORY}/kubernetes-development/products/podinfo/\n\nflux create source git podinfo-development \\\n  --url=ssh://git@github.com/${GITHUB_USERNAME}/podinfo-development.git \\\n  --branch=main \\\n  --secret-ref=podinfo-gitrepository \\\n  --namespace=podinfo \\\n  --export &gt; gitrepository.yaml\n\n\ncat gitrepository.yaml\n\n  ---\n  apiVersion: source.toolkit.fluxcd.io/v1\n  kind: GitRepository\n  metadata:\n    name: podinfo-development\n    namespace: podinfo\n  spec:\n    interval: 1m0s\n    ref:\n      branch: main\n    secretRef:\n      name: podinfo-gitrepository\n    url: ssh://git@github.com/papaFrancky/podinfo-development.git\n</code></pre>"},{"location":"FluxCD/FlucCD_arborescence_par_produit/#definition-de-la-kustomization-liee-au-gitrepo","title":"D\u00e9finition de la Kustomization li\u00e9e au GitRepo","text":"<p>Tip</p> <p>Nommer le manifest 'kustomize.yml' pose des probl\u00e8mes, le nom doit \u00eatre r\u00e9serv\u00e9 pour les besoins internes de Flux. Nous le nommerons 'sync.yaml'.</p> <pre><code>flux create kustomization podinfo \\\n    --source=GitRepository/podinfo-development.podinfo \\\n    --path=\"./kustomize\" \\\n    --prune=true \\\n    --namespace=podinfo \\\n    --export &gt; sync.yaml\n\n\ncat kustomize.yaml\n\n  ---\n  apiVersion: kustomize.toolkit.fluxcd.io/v1\n  kind: Kustomization\n  metadata:\n    name: podinfo\n    namespace: podinfo\n  spec:\n    interval: 1m0s\n    path: ./kustomize\n    prune: true\n    sourceRef:\n      kind: GitRepository\n      name: podinfo-development\n      namespace: podinfo\n\ngit add .\ngit commit -m \"feat: added podinfo GitRepo + Kustomization.\"\ngit push\n\nflux reconcile kustomization flux-system --with-source\n\n\nkubectl get GitRepositories -n podinfo\n\n  NAME                  URL                                                        AGE    READY   STATUS\n  podinfo-development   ssh://git@github.com/papaFrancky/podinfo-development.git   105m   True    stored artifact for     revision     'main@sha1:dc830d02a6e0bcbf63bcc387e8bde57d5627aec2'\n\n\nkubectl get kustomizations -n podinfo\n\n  NAME                  AGE    READY   STATUS\n  podinfo-development   106m   True    Applied revision: main@sha1:dc830d02a6e0bcbf63bcc387e8bde57d5627aec2\n\n\nkubectl get all -n podinfo\n\n  NAME                           READY   STATUS    RESTARTS   AGE\n  pod/podinfo-664f9748d8-2d4nf   1/1     Running   0          2m16s\n  pod/podinfo-664f9748d8-n5gwn   1/1     Running   0          2m1s\n\n  NAME                 TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)             AGE\n  service/kubernetes   ClusterIP   10.96.0.1      &lt;none&gt;        443/TCP             3h50m\n  service/podinfo      ClusterIP   10.96.175.42   &lt;none&gt;        9898/TCP,9999/TCP   2m16s\n\n  NAME                      READY   UP-TO-DATE   AVAILABLE   AGE\n  deployment.apps/podinfo   2/2     2            2           2m16s\n\n  NAME                                 DESIRED   CURRENT   READY   AGE\n  replicaset.apps/podinfo-664f9748d8   2         2         2       2m16s\n\n  NAME                                          REFERENCE            TARGETS         MINPODS   MAXPODS   REPLICAS   AGE\n  horizontalpodautoscaler.autoscaling/podinfo   Deployment/podinfo   &lt;unknown&gt;/99%   2         4         2          2m16s\n</code></pre>"},{"location":"FluxCD/FlucCD_arborescence_par_produit/#mise-a-jour-automatique-de-limage","title":"Mise \u00e0 jour automatique de l'image","text":"<p>Nous allons maintenant mettre en place la mise \u00e0 jour automatique de l'image du conteneur utilis\u00e9e pour l'application podinfo. Pour ce faire, nous allons d\u00e9finir un ImageRepository et une ImagePolicy :</p> <pre><code>cd ${WORKING_DIRECTORY}/kubernetes-development/products/podinfo\n\nflux create image repository podinfo \\\n  --image=ghcr.io/stefanprodan/podinfo \\\n  --interval=5m \\\n  --namespace=podinfo \\\n  --export &gt; imagerepository.yaml\n\n\ncat imagerepository.yaml\n\n  ---\n  apiVersion: image.toolkit.fluxcd.io/v1beta2\n  kind: ImageRepository\n  metadata:\n    name: podinfo\n    namespace: podinfo\n  spec:\n    image: ghcr.io/stefanprodan/podinfo\n    interval: 5m0s\n\ngit add .\ngit commit -m \"feat: defined the podinfo image repository.\"\ngit push\n\n\nkubectl describe imagerepository podinfo\n\n  apiVersion: image.toolkit.fluxcd.io/v1beta2\n  kind: ImageRepository\n  metadata:\n    creationTimestamp: \"2024-02-04T20:45:06Z\"\n    finalizers:\n    - finalizers.fluxcd.io\n    generation: 1\n    labels:\n      kustomize.toolkit.fluxcd.io/name: flux-system\n      kustomize.toolkit.fluxcd.io/namespace: flux-system\n    name: podinfo\n    namespace: podinfo\n    resourceVersion: \"34713\"\n    uid: d1124c6d-58f5-4ba4-9202-0bdc67b6a37f\n  spec:\n    exclusionList:\n    - ^.*\\.sig$\n    image: ghcr.io/stefanprodan/podinfo\n    interval: 5m0s\n    provider: generic\n  status:\n    canonicalImageName: ghcr.io/stefanprodan/podinfo\n    conditions:\n    - lastTransitionTime: \"2024-02-04T20:45:07Z\"\n      message: 'successful scan: found 51 tags'\n      observedGeneration: 1\n      reason: Succeeded\n      status: \"True\"\n      type: Ready\n    lastScanResult:\n      latestTags:\n      - latest\n      - 6.5.4\n      - 6.5.3\n      - 6.5.2\n      - 6.5.1\n      - 6.5.0\n      - 6.4.1\n      - 6.4.0\n      - 6.3.6\n      - 6.3.5\n      scanTime: \"2024-02-04T20:45:07Z\"\n      tagCount: 51\n    observedExclusionList:\n    - ^.*\\.sig$\n    observedGeneration: 1\n\n\nflux create image policy podinfo \\\n  --image-ref=podinfo \\\n  --select-semver='&gt;=5.4.x' \\\n  --namespace=podinfo \\\n  --export &gt; imagepolicy.yaml\n\n\ncat imagepolicy.yaml\n\n  ---\n  apiVersion: image.toolkit.fluxcd.io/v1beta2\n  kind: ImagePolicy\n  metadata:\n    name: nginxhello\n    namespace: podinfo\n  spec:\n    imageRepositoryRef:\n      name: podinfo\n    policy:\n      semver:\n        range: '&gt;=5.4.x'\n\n\ngit add .\ngit commit -m \"feat: defined the podinfo image policy.\"\ngit push\n\n\nkubectl get imagepolicy podinfo -n podinfo\n\n  NAME      LATESTIMAGE\n  podinfo   ghcr.io/stefanprodan/podinfo:6.5.4\n</code></pre>"},{"location":"FluxCD/FlucCD_arborescence_par_produit/#ajout-dun-marqueur-dans-le-manifest-de-deploiement","title":"Ajout d'un marqueur dans le manifest de d\u00e9ploiement","text":"<p>Nous pouvons enfin ajouter un marqueur \u00e0 notre deployment pour permettre la mise \u00e0 jour de l'application podinfo via image automation.</p> <pre><code>cd ${WORKING_DIRECTORY}/products/podinfo-development/kustomize\nvi deployment.yaml\n</code></pre> <p>Nous allons ajouter un marquer sur le param\u00e8tre .spec.template.spec.containers[].image comme suit :</p> <pre><code>    ghcr.io/stefanprodan/podinfo:6.5.0 -&gt; ghcr.io/stefanprodan/podinfo:6.5.0 # {\"$imagepolicy\": \"podinfo:podinfo\"}\n</code></pre> <p>Note</p> <p>\"podinfo.podinfo\" correspond \u00e0 \"&lt;namespace&gt;.&lt;imagepolicy&gt;\"</p> <p>Info</p> <p>https://fluxcd.io/flux/guides/image-update/#configure-image-update-for-custom-resources</p>"},{"location":"FluxCD/FlucCD_arborescence_par_produit/#definition-dune-image-update-automation","title":"D\u00e9finition d'une Image Update Automation","text":"<p>Il nous reste \u00e0 dfinir une ImageUpdateAutomation</p> <p>Info</p> <p>https://fluxcd.io/flux/cmd/flux_create_image_update/#examples</p> <pre><code>    cd ${WORKING_DIRECTORY}/kubernetes-development/products/podinfo\n\n    flux create image update podinfo \\\n        --namespace=podinfo \\\n        --git-repo-ref=podinfo-development \\\n        --git-repo-path=\"./kustomize\" \\\n        --checkout-branch=main \\\n        --author-name=FluxCD \\\n        --author-email=flux@example.com \\\n        --commit-template=\"{{range .Updated.Images}}{{println .}}{{end}}\" \\\n        --export &gt; image-update-automation.yaml\n\n\n    cat image-update-automation.yaml\n\n        ---\n        apiVersion: image.toolkit.fluxcd.io/v1beta1\n        kind: ImageUpdateAutomation\n        metadata:\n          name: podinfo\n          namespace: podinfo\n        spec:\n          git:\n            checkout:\n              ref:\n                branch: main\n            commit:\n              author:\n                email: flux@example.com\n                name: FluxCD\n              messageTemplate: '{{range .Updated.Images}}{{println .}}{{end}}'\n          interval: 1m0s\n          sourceRef:\n            kind: GitRepository\n            name: podinfo-development\n          update:\n            path: ./kustomize\n            strategy: Setters\n          update:\n            path: ./kustomize\n            strategy: Setters\n\n\n    cd ${WORKING_DIRECTORY}/products/podinfo-development/kustomize\n    git fetch     # si le manifest a \u00e9t\u00e9 modifi\u00e9, nous aurons 1 commit de retard sur notre copie locale.\n    git pull\n    grep \"image:\" deployment.yaml\n        image: ghcr.io/stefanprodan/podinfo:6.5.4 # {\"$imagepolicy\": \"podinfo:podinfo\"}\n        -&gt; la version a bien chang\u00e9\n\n\n    git log\n\n      commit 9a10ef5790264c1b415323bc3713c1ee7d5591cb (HEAD -&gt; main, origin/main, origin/HEAD)\n      Author: FluxCD &lt;flux@example.com&gt;\n      Date:   Sun Feb 4 21:35:24 2024 +0000\n\n          ghcr.io/stefanprodan/podinfo:6.5.4\n\n\n    kubectl get gitrepository podinfo-development\n\n        NAME                  URL                                                        AGE     READY   STATUS\n        podinfo-development   ssh://git@github.com/papaFrancky/podinfo-development.git   4h17m   True    stored artifact for     revision     'main@sha1:9a10ef5790264c1b415323bc3713c1ee7d5591cb'\n</code></pre> <p>-&gt; nous retrouvons le m\u00eame SHA1.</p>"},{"location":"FluxCD/FlucCD_arborescence_par_produit/#ajout-dun-nouveau-produit-nginxhello","title":"Ajout d'un nouveau produit : nginxhello","text":""},{"location":"FluxCD/FlucCD_arborescence_par_produit/#creation-du-repository-git-dedie-a-lapplication-nginxhello","title":"Cr\u00e9ation du repository Git d\u00e9di\u00e9 \u00e0 l'application 'nginxhello'","text":"<p>Nous cr\u00e9ons un nouveau repository Git sur notre compte GitHub qui h\u00e9bergera l'application 'nginxhello' de Nigel Brown.</p> <p>-&gt; https://github.com/papafrancky/nginxhello-development</p> <p>Nous clonons le repository en local : </p> <pre><code>cd ${WORKING_DIRECTORY}/products/\ngit clone git@github.com:papafrancky/nginxhello-development.git\n</code></pre> <p>Nous allons y d\u00e9poser les manifests suivants r\u00e9cup\u00e9r\u00e9s depuis le repo git de Nigel Brown : https://github.com/nbrownuk/gitops-nginxhello/</p> <ul> <li>deployment.yaml</li> <li>service.yaml</li> </ul> <p>Tip</p> <p>Modifier les 2 manifests et ajouter : .metadata.namespace:nginxhello !!!</p> <p>Une fois les manifests recopi\u00e9s, on pousse les ajouts dans GitHub :</p> <pre><code>cd ${WORKING_DIRECTORY}/products/nginxhello-development\ngit add deployment\ngit commit -m \"added nginxhello application.\"\ngit push\n</code></pre>"},{"location":"FluxCD/FlucCD_arborescence_par_produit/#creation-du-namespace-dedie-a-lapplication","title":"Cr\u00e9ation du namespace d\u00e9di\u00e9 \u00e0 l'application","text":"<pre><code>kubectl create ns nginxhello\n</code></pre>"},{"location":"FluxCD/FlucCD_arborescence_par_produit/#creations-de-la-paire-de-cles-ssh","title":"Cr\u00e9ations de la paire de cl\u00e9s SSH","text":"<p>FluxCD les utilisera pour interagir avec le repo GitHub nouvellement cr\u00e9\u00e9.</p> <pre><code>flux create secret git nginxhello-gitrepository \\\n  --url=ssh://github.com/papafrancky/nginxhello-development \\\n  --namespace=nginxhello\n\n  \u271a deploy key: ecdsa-sha2-nistp384 AAAAE2VjZHNhLXNoYTItbmlzdHAzODQAAAAIbmlzdHAzODQAAABhBFbZqQ6mc2ZAljuZoxlRNJmv1/lUWbmL2sdPGGNf10ynh/BtH4DSZHGFz3RWIHOpGXmGJjX1ZN2pLvi/uGzvSVTAJFBMWrbqljnGWCpbB9fL8UlfokYrdRdIr/7aZnR9ZQ==\n\n  \u25ba git secret 'nginxhello-gitrepository' created in 'podinfo' namespace\n\n\nkubectl -n nginxhello get secret nginxhello-gitrepository\n\n  NAME                       TYPE     DATA   AGE\n  nginxhello-gitrepository   Opaque   3      116s\n</code></pre> <p>La cl\u00e9 publique (deploy key) doit \u00eatre ajout\u00e9e dans les settings du repo GitHub :  ```  https://github.com/papafrancky/nginxhello-development/settings/keys/new ````</p> <p>Warning</p> <p>Cocher la case 'Allow write access' !!!</p> <p>Cliquer sur le bouton 'Add Key' et renseigner son mot de passe pour confirmer</p>"},{"location":"FluxCD/FlucCD_arborescence_par_produit/#configuration-des-notifications-provider-discord_1","title":"Configuration des notifications (provider: Discord)","text":""},{"location":"FluxCD/FlucCD_arborescence_par_produit/#creation-du-channel-discord-dedie-a-lapplication-podinfo_1","title":"Cr\u00e9ation du channel Discord d\u00e9di\u00e9 \u00e0 l'application podinfo","text":"<p>Cr\u00e9er un channel nomm\u00e9 'nginxhello-development' dans son 'serveur' Discord et recopier le webhook cr\u00e9\u00e9 par d\u00e9faut.</p>"},{"location":"FluxCD/FlucCD_arborescence_par_produit/#enregistrement-du-webhook-du-channel-discord-dans-un-secret-kubernetes_1","title":"Enregistrement du webhook du channel Discord dans un Secret Kubernetes","text":"<pre><code>DISCORD_WEBHOOK=https://discord.com/api/webhooks/1204543090371592212/mCphzp07-orqFvRKGdtxjbq0T9OHC8whuUgSpzuhn2PGol8Kr2MHm4OKAorFSQCom7Ou\n\nkubectl -n nginxhello create secret generic discord-nginxhello-development-webhook --from-literal=address=${DISCORD_WEBHOOK} \n\n\nkubectl -n nginxhello get secret discord-nginxhello-development-webhook\n\n  NAME                                     TYPE     DATA   AGE\n  discord-nginxhello-development-webhook   Opaque   1      11s\n</code></pre>"},{"location":"FluxCD/FlucCD_arborescence_par_produit/#creation-du-notification-provider_1","title":"Cr\u00e9ation du 'notification provider'","text":"<p>Info</p> <p>https://fluxcd.io/flux/components/notification/providers/#discord</p> <pre><code>flux create alert-provider discord \\\n  --type=discord \\\n  --secret-ref=discord-nginxhello-development-webhook \\\n  --channel=nginxello-development \\\n  --username=FluxCD \\\n  --namespace=nginxhello \\\n  --export &gt; notification-provider.yaml\n\n\ncat notification-provider.yaml\n\n  ---\n  apiVersion: notification.toolkit.fluxcd.io/v1beta3\n  kind: Provider\n  metadata:\n    name: discord\n    namespace: nginxhello\n  spec:\n    channel: nginxello-development\n    secretRef:\n      name: discord-nginxhello-development-webhook\n    type: discord\n    username: FluxCD\n</code></pre>"},{"location":"FluxCD/FlucCD_arborescence_par_produit/#configuration-des-alertes-discord_1","title":"Configuration des alertes Discord","text":"<pre><code>flux create alert discord \\\n  --event-severity=info \\\n  --event-source='GitRepository/*,Kustomization/*,ImageRepository/*,ImagePolicy/*,HelmRepository/*' \\\n  --provider-ref=discord \\\n  --namespace=nginxhello \\\n  --export &gt; notification-alert.yaml\n\n\ncat notifications/alerts/discord.yaml\n\n  ---\n  apiVersion: notification.toolkit.fluxcd.io/v1beta3\n  kind: Alert\n  metadata:\n    name: discord\n    namespace: nginxhello\n  spec:\n    eventSeverity: info\n    eventSources:\n    - kind: GitRepository\n      name: '*'\n    - kind: Kustomization\n      name: '*'\n    - kind: ImageRepository\n      name: '*'\n    - kind: ImagePolicy\n      name: '*'\n    - kind: HelmRepository\n      name: '*'\n    providerRef:\n      name: discord\n</code></pre>"},{"location":"FluxCD/FlucCD_arborescence_par_produit/#enregistrement-des-modifications_1","title":"Enregistrement des modifications","text":"<pre><code>cd ${WORKING_DIRECTORY}/kubernetes-development\ngit st\ngit add .\ngit commit -m 'configuring discord alerting.'\ngit push\n\n\nkubectl get providers,alerts -n nginxhello\n\n  NAME                                              AGE\n  provider.notification.toolkit.fluxcd.io/discord   9s\n\n  NAME                                           AGE\n  alert.notification.toolkit.fluxcd.io/discord   9s\n</code></pre>"},{"location":"FluxCD/FlucCD_arborescence_par_produit/#definition-du-gitrepository","title":"D\u00e9finition du GitRepository","text":"<pre><code>cd ${WORKING_DIRECTORY}/kubernetes-development/products/nginxhello\nvi git-repository.yaml\n\n  ---\n  apiVersion: source.toolkit.fluxcd.io/v1\n  kind: GitRepository\n  metadata:\n    name: nginxhello-development\n    namespace: nginxhello\n  spec:\n    interval: 1m0s\n    ref:\n      branch: main\n    secretRef:\n      name: nginxhello-gitrepository\n    url: ssh://git@github.com/papafrancky/nginxhello-development.git\n\ngit add .\ngit commit -m \"defined nginxhello namespace and git repository.\"\ngit push\n</code></pre>"},{"location":"FluxCD/FlucCD_arborescence_par_produit/#definition-de-la-kustomization-liee-au-gitrepo_1","title":"D\u00e9finition de la Kustomization li\u00e9e au GitRepo","text":"<pre><code>cd ${WORKING_DIRECTORY}/kubernetes-development/products/nginxhello\n\nflux create kustomization nginxhello \\\n  --source=GitRepository/nginxhello-development.nginxhello \\\n  --path=\".\" \\\n  --prune=true \\\n  --namespace=nginxhello \\\n  --export &gt; sync2.yaml\n\n\ncat sync.yaml\n\n  ---\n  apiVersion: kustomize.toolkit.fluxcd.io/v1\n  kind: Kustomization\n  metadata:\n    name: nginxhello\n    namespace: nginxhello\n  spec:\n    interval: 1m0s\n    path: ./\n    prune: true\n    sourceRef:\n      kind: GitRepository\n      name: nginxhello-development\n      namespace: nginxhello\n\n\ngit add .\ngit commit -m \"feat: added nginxhello GitRepo + Kustomization.\"\ngit push\n\nflux reconcile kustomization flux-system --with-source\n\n\nkubectl get GitRepositories -n nginxhello\n\n  NAME                  URL                                                        AGE    READY   STATUS\n  podinfo-development   ssh://git@github.com/papaFrancky/podinfo-development.git   105m   True    stored artifact for     revision     'main@sha1:dc830d02a6e0bcbf63bcc387e8bde57d5627aec2'\n\n\nkubectl get kustomizations -n nginxhello\n\n        NAME         AGE   READY   STATUS\n        nginxhello   20m   True    Applied revision: main@sha1:4915acffa3c3d0ef38b2985e35db8ec1d4294cc9\n\n\nkubectl get all -n nginxhello\n\n  NAME                              READY   STATUS    RESTARTS   AGE\n  pod/nginxhello-75dfc9cd44-6sxx7   1/1     Running   0          2m44s\n  pod/nginxhello-75dfc9cd44-f78d6   1/1     Running   0          2m44s\n  pod/nginxhello-75dfc9cd44-hwfwf   1/1     Running   0          2m44s\n  pod/nginxhello-75dfc9cd44-p74mr   1/1     Running   0          2m44s\n  pod/nginxhello-75dfc9cd44-r88mb   1/1     Running   0          2m44s\n\n  NAME                 TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE\n  service/nginxhello   ClusterIP   10.96.173.138   &lt;none&gt;        80/TCP    2m44s\n\n  NAME                         READY   UP-TO-DATE   AVAILABLE   AGE\n  deployment.apps/nginxhello   5/5     5            5           2m44s\n\n  NAME                                    DESIRED   CURRENT   READY   AGE\n  replicaset.apps/nginxhello-75dfc9cd44   5         5         5       2m44s\n</code></pre>"},{"location":"FluxCD/FlucCD_arborescence_par_produit/#mise-a-jour-automatique-de-limage_1","title":"Mise \u00e0 jour automatique de l'image","text":""},{"location":"FluxCD/FlucCD_arborescence_par_produit/#definition-de-limage-repository","title":"D\u00e9finition de l'Image Repository","text":"<pre><code>cd ${WORKING_DIRECTORY}/kubernetes-development/products/nginxhello\n\nflux create image repository nginxhello \\\n  --image=nbrown/nginxhello \\\n  --interval=5m \\\n  --namespace=nginxhello \\\n  --export &gt; image-repository.yaml\n\n\ncat image-repository.yaml\n\n  ---\n  apiVersion: image.toolkit.fluxcd.io/v1beta2\n  kind: ImageRepository\n  metadata:\n    name: nginxhello\n    namespace: nginxhello\n  spec:\n    image: nbrown/nginxhello\n    interval: 5m0s\n\n\ngit add .\ngit commit -m \"feat: defined the podinfo image repository.\"\ngit push\n\n\nkubectl -n nginxhello get imagerepository nginxhello\n\n  NAME         LAST SCAN              TAGS\n  nginxhello   2024-02-07T19:41:44Z   45\n\n\nkubectl -n nginxhello get imagerepository nginxhello -o yaml\n\n  apiVersion: image.toolkit.fluxcd.io/v1beta2\n  kind: ImageRepository\n  metadata:\n    creationTimestamp: \"2024-02-06T23:06:10Z\"\n    finalizers:\n    - finalizers.fluxcd.io\n    generation: 2\n    labels:\n      kustomize.toolkit.fluxcd.io/name: flux-system\n      kustomize.toolkit.fluxcd.io/namespace: flux-system\n    name: nginxhello\n    namespace: nginxhello\n    resourceVersion: \"193248\"\n    uid: ffb33fdf-4425-4e09-bda1-9b941fa6ce48\n  spec:\n    exclusionList:\n    - ^.*\\.sig$\n    image: nbrown/nginxhello\n    interval: 5m0s\n    provider: generic\n  status:\n    canonicalImageName: index.docker.io/nbrown/nginxhello\n    conditions:\n    - lastTransitionTime: \"2024-02-07T19:41:44Z\"\n      message: 'successful scan: found 45 tags'\n      observedGeneration: 2\n      reason: Succeeded\n      status: \"True\"\n      type: Ready\n    lastScanResult:\n      latestTags:\n      - stable\n      - mainline\n      - latest\n      - e6c463e6\n      - aad042cb\n      - 1.25.2\n      - \"1.25\"\n      - 1.24.0\n      - \"1.24\"\n      - 1.23.3\n      scanTime: \"2024-02-07T19:41:44Z\"\n      tagCount: 45\n    observedExclusionList:\n    - ^.*\\.sig$\n    observedGeneration: 2\n</code></pre>"},{"location":"FluxCD/FlucCD_arborescence_par_produit/#definition-de-limage-policy","title":"D\u00e9finition de l'Image Policy","text":"<pre><code>flux create image policy nginxhello \\\n  --image-ref=nginxhello \\\n  --select-semver='&gt;=1.19.0 &lt;1.24.0' \\\n  --namespace=nginxhello \\\n  --export &gt; image-policy.yaml\n\n\ncat image-policy.yaml\n\n  ---\n  apiVersion: image.toolkit.fluxcd.io/v1beta2\n  kind: ImagePolicy\n  metadata:\n    name: nginxhello\n    namespace: nginxhello\n  spec:\n    imageRepositoryRef:\n      name: nginxhello\n    policy:\n      semver:\n        range: '&gt;=1.19.0 &lt;1.24.0'\n\n\ngit add .\ngit commit -m \"feat: defined the nginxhello image policy.\"\ngit push\n\n\nkubectl -n nginxhello get imagepolicy nginxhello\n\n  NAME         LATESTIMAGE\n  nginxhello   nbrown/nginxhello:1.23.3\n</code></pre>"},{"location":"FluxCD/FlucCD_arborescence_par_produit/#ajout-dun-marqueur-dans-le-manifest-de-deploiement_1","title":"Ajout d'un marqueur dans le manifest de d\u00e9ploiement","text":"<p>Info</p> <p>https://fluxcd.io/flux/guides/image-update/#configure-image-update-for-custom-resources</p> <p>Nous pouvons enfin ajouter un marqueur \u00e0 notre deployment pour permettre la mise \u00e0 jour de l'application podinfo via image automation.</p> <pre><code>cd ${WORKING_DIRECTORY}/products/nginxhello-development\nvi deployment.yaml\n</code></pre> <p>Nous allons ajouter un marquer sur le param\u00e8tre .spec.template.spec.containers[].image comme suit :</p> <pre><code>nbrown/nginxhello:1.19.0 -&gt; nbrown/nginxhello:1.19.0 # {\"$imagepolicy\": \"nginxhello:nginxhello\"}\n</code></pre> <p>Info</p> <p>\"nginxhello:nginxhello\" correspond \u00e0 \"&lt;namespace&gt;:&lt;imagepolicy&gt;\"</p> <p>Note</p> <p>https://fluxcd.io/flux/guides/image-update/#configure-image-update-for-custom-resources</p>"},{"location":"FluxCD/FlucCD_arborescence_par_produit/#definition-dune-image-update-automation_1","title":"D\u00e9finition d'une Image Update Automation","text":"<p>Il nous reste \u00e0 d\u00e9finir une ImageUpdateAutomation</p> <p>Info</p> <p>https://fluxcd.io/flux/cmd/flux_create_image_update/#examples</p> <pre><code>cd ${WORKING_DIRECTORY}/kubernetes-development/products/podinfo\n\nflux create image update nginxhello \\\n  --namespace=nginxhello \\\n  --git-repo-ref=nginxhello-development \\\n  --git-repo-path=\"./\" \\\n  --checkout-branch=main \\\n  --author-name=FluxCD \\\n  --author-email=flux@example.com \\\n  --commit-template=\"{{range .Updated.Images}}{{println .}}{{end}}\" \\\n  --export &gt; image-update-automation.yaml\n\n\ncat image-update-automation.yaml\n\n  ---\n  apiVersion: image.toolkit.fluxcd.io/v1beta1\n    kind: ImageUpdateAutomation\n    metadata:\n    name: nginxhello\n    namespace: nginxhello\n  spec:\n    git:\n      checkout:\n        ref:\n          branch: main\n      commit:\n        author:\n          email: flux@example.com\n          name: FluxCD\n        messageTemplate: '{{range .Updated.Images}}{{println .}}{{end}}'\n    interval: 1m0s\n    sourceRef:\n      kind: GitRepository\n      name: nginxhello-development\n    update:\n      path: ./\n      strategy: Setters\n\ngit add image-update-automation.yaml\ngit commit -m 'defined the nginxhello image update automation.' \ngit push\n\n\ncd ${WORKING_DIRECTORY}/products/nginxhello-development\ngit fetch     # si le manifest a \u00e9t\u00e9 modifi\u00e9, nous aurons 1 commit de retard sur notre copie locale.\ngit pull\ngrep \"image:\" deployment.yaml\n  - image: nbrown/nginxhello:1.23.3 # {\"$imagepolicy\": \"nginxhello:nginxhello\"}\n# -&gt; la version a bien chang\u00e9, image update automation a r\u00e9\u00e9crit le manifest.\n\n\nkubectl -n nginxhello events\n# -&gt; retrace l'ensemble des op\u00e9rations d\u00e9clench\u00e9es depuis l'ImageUpdateAutomation (la liste est longue).\n</code></pre>"},{"location":"FluxCD/FlucCD_arborescence_par_produit/#test-complementaire","title":"Test compl\u00e9mentaire ^^","text":"<p>Nous allons modifier l'ImagePolicy pour utiliser l'image Docker la plus r\u00e9cente de nginxhello.</p>"},{"location":"FluxCD/FlucCD_arborescence_par_produit/#identification-de-limage-la-plus-recente","title":"Identification de l'image la plus r\u00e9cente","text":"<pre><code>kubectl -n nginxhello get imagerepository nginxhello -o yaml | yq '.status.lastScanResult.latestTags'\n\n  - stable\n  - mainline\n  - latest\n  - e6c463e6\n  - aad042cb\n  - 1.25.2\n  - \"1.25\"\n  - 1.24.0\n  - \"1.24\"\n  - 1.23.3\n</code></pre> <p>-&gt; la version la plus r\u00e9cente semble \u00eatre la 1.25.2</p>"},{"location":"FluxCD/FlucCD_arborescence_par_produit/#modification-de-limage-policy","title":"Modification de l'Image Policy","text":"<pre><code>cd ${WORKING_DIRECTORY}/kubernetes-development/products/nginxhello\ngsed -i \"s/range: .*$/range: \\'&gt;=1.23.0\\'/\" image-policy.yaml\n\ncat image-policy.yaml\n\n  ---\n  apiVersion: image.toolkit.fluxcd.io/v1beta2\n  kind: ImagePolicy\n  metadata:\n    name: nginxhello\n    namespace: nginxhello\n  spec:\n    imageRepositoryRef:\n      name: nginxhello\n    policy:\n      semver:\n        range: '&gt;=1.23.0'\n\n\ngit add .\ngit commit -m 'modified the nginxhello image policy to get its latest image version.'\ngit push \n</code></pre>"},{"location":"FluxCD/FlucCD_arborescence_par_produit/#cleaning","title":"Cleaning","text":"<p>Nous allons nous concentrer sur la gestion des Helm Charts. Avant cela, pour \u00e9conomiser des ressources, nous allons d\u00e9sactiver la gestion des ressources pr\u00e9c\u00e9dentes. Je ne sais pas s'il existe une m\u00e9thode plus acad\u00e9mique, ausi nous allons simplement renommer les manifests de kustomization.</p> <pre><code>cd ${WORKING_DIRECTORY}products\nmv nginxhello/sync.yaml nginxhello/sync.yaml.BKP\nmv podinfo/sync.yaml podinfo/sync.yaml.BKP\n\ngit add .\ngit commit -m 'disabling flux management for nginxhello and podinfo products.'\ngit push\n</code></pre> <p>-&gt; les pods, deployments, services sont supprim\u00e9s dnas les namespaces nginxhello et podinfo !</p>"},{"location":"FluxCD/FlucCD_arborescence_par_produit/#gestion-des-applications-packagees-avec-helm","title":"Gestion des applications packag\u00e9es avec Helm","text":""},{"location":"FluxCD/FlucCD_arborescence_par_produit/#arborescence-daccueil-pour-la-nouvelle-application","title":"Arborescence d'accueil pour la nouvelle application","text":"<p>Nous allons d\u00e9ployer la m\u00eame application 'podinfo' mais cette fois-ci, via un Helm Chart. Pour \u00e9viter toute confusion avec notre premier d\u00e9ploiement, nous h\u00e9bergerons cette nouvelle application dans une r\u00e9pertoire et un namespace nomm\u00e9s podinfo2</p> <pre><code>mkdir ${WORKING_DIRECTORY}/kubernetes-development/products/podinfo2\nmkdir ${WORKING_DIRECTORY}/products/podinfo2\n</code></pre>"},{"location":"FluxCD/FlucCD_arborescence_par_produit/#namespace-dedie","title":"Namespace d\u00e9di\u00e9","text":"<pre><code>kubectl create namespace podinfo2 --dry-run=client -o yaml | grep -vE 'creationTimestamp|spec|status' &gt; ${WORKING_DIRECTORY}/kubernetes-development/products/podinfo2/namespace.yaml\nkubectl apply -f ${WORKING_DIRECTORY}/kubernetes-development/products/podinfo2/namespace.yaml\n</code></pre>"},{"location":"FluxCD/FlucCD_arborescence_par_produit/#notifications-discord","title":"Notifications Discord","text":""},{"location":"FluxCD/FlucCD_arborescence_par_produit/#creation-du-channel-discord-dedie-a-lapplication-podinfo_2","title":"Cr\u00e9ation du channel Discord d\u00e9di\u00e9 \u00e0 l'application podinfo","text":"<p>Cr\u00e9er un channel nomm\u00e9 'podinfo2-development' dans son 'serveur' Discord et recopier le webhook cr\u00e9\u00e9 par d\u00e9faut.</p>"},{"location":"FluxCD/FlucCD_arborescence_par_produit/#enregistrement-du-webhook-du-channel-discord-dans-un-secret-kubernetes_2","title":"Enregistrement du webhook du channel Discord dans un Secret Kubernetes","text":"<pre><code>DISCORD_WEBHOOK=https://discord.com/api/webhooks/1205276611159789689/6UTavz1aoiaEtxTpDmN-hBYP9vRVpXaD-XJK4K0cI0hscdVd3XFrNR32o9vJ_VStB1Hl\nkubectl -n podinfo2 create secret generic discord-podinfo2-development-webhook --from-literal=address=${DISCORD_WEBHOOK}\n</code></pre>"},{"location":"FluxCD/FlucCD_arborescence_par_produit/#creation-du-notification-provider_2","title":"Cr\u00e9ation du 'notification provider'","text":"<p>Info</p> <p>https://fluxcd.io/flux/components/notification/providers/#discord</p> <pre><code>cd ${WORKING_DIRECTORY}/kubernetes-development/products/podinfo2/\nflux create alert-provider discord \\\n  --type=discord \\\n  --secret-ref=discord-podinfo2-development-webhook \\\n  --channel=podinfo2-development \\\n  --username=FluxCD \\\n  --namespace=podinfo2 \\\n  --export &gt; notification-provider.yaml\n</code></pre>"},{"location":"FluxCD/FlucCD_arborescence_par_produit/#configuration-des-alertes-discord_2","title":"Configuration des alertes Discord","text":"<pre><code>flux create alert discord \\\n  --event-severity=info \\\n  --event-source='GitRepository/*,Kustomization/*,ImageRepository/*,ImagePolicy/*,HelmRepository/*' \\\n  --provider-ref=discord \\\n  --namespace=podinfo2 \\\n  --export &gt; notification-alert.yaml\n</code></pre>"},{"location":"FluxCD/FlucCD_arborescence_par_produit/#enregistrement-des-modifications_2","title":"Enregistrement des modifications","text":"<pre><code>cd ${WORKING_DIRECTORY}/kubernetes-development\ngit add .\ngit commit -m 'configuring discord alerting.'\ngit push\n\nkubectl get providers,alerts -n podinfo2\n</code></pre>"},{"location":"FluxCD/FlucCD_arborescence_par_produit/#the-podinfo-helm-chart","title":"The 'podinfo' Helm Chart","text":"<p>We will use the 'podinfo' Helm chart as an example.</p> <pre><code>helm show chart oci://ghcr.io/stefanprodan/charts/podinfo\n\n  Pulled: ghcr.io/stefanprodan/charts/podinfo:6.5.4\n  Digest: sha256:a961643aa644f24d66ad05af2cdc8dcf2e349947921c3791fc3b7883f6b1777f\n  apiVersion: v1\n  appVersion: 6.5.4\n  description: Podinfo Helm chart for Kubernetes\n  home: https://github.com/stefanprodan/podinfo\n  kubeVersion: '&gt;=1.23.0-0'\n  maintainers:\n    - email: stefanprodan@users.noreply.github.com\n  name: stefanprodan\n  name: podinfo\n  sources:\n    - https://github.com/stefanprodan/podinfo\n  version: 6.5.4\n</code></pre>"},{"location":"FluxCD/FlucCD_arborescence_par_produit/#creating-the-podinfo2-helmrepository","title":"Creating the 'podinfo2' helmRepository","text":""},{"location":"FluxCD/FlucCD_arborescence_par_produit/#authenticating-to-the-helm-repository","title":"Authenticating to the Helm repository","text":"<p>Let's create a new 'Docker registry' type secret allowinf us to retrieve the Helm chart. (ghcr.io belongs to GitHub; they both use the same identity management)</p> <p>Note</p> <p>This repository is a public one; in our case there will be no need to specify credentials in the helmRepository.</p> <pre><code>export GITHUB_USER=papafrancky\nexport GITHUB_TOKEN=${GITHUB_PAT}\n\nkubectl create secret docker-registry ghcr-charts-auth \\\n  --docker-server=ghcr.io \\\n  --docker-username=${GITHUB_USER} \\\n  --docker-password=-{GITHUB_TOKEN}\n</code></pre>"},{"location":"FluxCD/FlucCD_arborescence_par_produit/#creating-the-helmrepository-manifest","title":"Creating the helmRepository manifest","text":"<pre><code>flux create source helm podinfo2 \\\n  --url=https://stefanprodan.github.io/podinfo \\\n  --namespace=podinfo2 \\\n  --interval=1m \\\n  --export &gt; ${WORKING_DIRECTORY}/kubernetes-development/products/podinfo2/helm-repository.yaml\n</code></pre>"},{"location":"FluxCD/FlucCD_arborescence_par_produit/#la-helm-release-podinfo2","title":"La Helm Release podinfo2","text":"<p>Si l'on veut personnaliser la configuration de la Helm Release, on peut se r\u00e9f\u00e9rer aux param\u00e8tres ici :</p> <p>Info</p> <p>https://artifacthub.io/packages/helm/podinfo/podinfo</p> <p>Dans notre cas, nous souhaitons simplement afficher le message 'Hello' dans la UI :</p> <pre><code>echo 'ui.message: Hello' &gt; ${WORKING_DIRECTORY}/kubernetes-development/products/podinfo2/helm-release.values.yaml\n\nflux create helmrelease podinfo2 \\\n  --source=HelmRepository/podinfo2 \\\n  --chart=podinfo \\\n  #--version=\"&gt;6.0.0\" \\\n  --values=${WORKING_DIRECTORY}/helmrelease_values/podinfo2/values.yaml \\\n  --namespace=podinfo2 \\\n  --export &gt; ${WORKING_DIRECTORY}/kubernetes-development/products/podinfo2/helm-release.yaml\n\nflux create hr podinfo \\\n  --interval=10m \\\n  --source=HelmRepository/podinfo \\\n  --chart=podinfo \\\n  --version=\"&gt;6.0.0\" \\\n  -export &gt; ${WORKING_DIRECTORY}/kubernetes-development/products/podinfo2/helm-release.yaml\n\n\ncat ${WORKING_DIRECTORY}/kubernetes-development/products/podinfo2/helm-release.yaml\n\n  ---\n  apiVersion: helm.toolkit.fluxcd.io/v2beta2\n  kind: HelmRelease\n  metadata:\n    name: podinfo2\n    namespace: podinfo2\n  spec:\n    chart:\n      spec:\n        chart: podinfo\n        reconcileStrategy: ChartVersion\n        sourceRef:\n          kind: HelmRepository\n          name: podinfo2\n    interval: 1m0s\n    values:\n      ui.message: Hello\n\n\ncd ${WORKING_DIRECTORY}/kubernetes-development\ngit add .\ngit commit -m \"feat: Defining a 'podinfo' Helm release.\"\ngit push\n\n\nkubectl -n podinfo2 get helmrelease\n\n  NAME       AGE   READY   STATUS\n  podinfo2   40h   True    Helm install succeeded for release podinfo2/podinfo2.v1 with chart podinfo@6.5.4\n\n\nkubectl -n podinfo2 get all\n\n  NAME                            READY   STATUS    RESTARTS   AGE\n  pod/podinfo2-7479bb6f76-lfxsz   1/1     Running   0          13s\n\n  NAME               TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)             AGE\n  service/podinfo2   ClusterIP   10.96.217.241   &lt;none&gt;        9898/TCP,9999/TCP   13s\n\n  NAME                       READY   UP-TO-DATE   AVAILABLE   AGE\n  deployment.apps/podinfo2   1/1     1            1           13s\n\n  NAME                                  DESIRED   CURRENT   READY   AGE\n  replicaset.apps/podinfo2-7479bb6f76   1         1         1       13s\n</code></pre>"},{"location":"FluxCD/FluxCD_demonstration_par_l_exemple/","title":"FluxCD - D\u00e9monstration par l'exemple","text":""},{"location":"FluxCD/FluxCD_demonstration_par_l_exemple/#abstract","title":"Abstract","text":"<p>Sur un cluster Rancher Desktop fra\u00eechement d\u00e9ploy\u00e9, nous installerons FluxCD (ie. 'bootstrap') pour g\u00e9rer le d\u00e9ploiement et la mise \u00e0 jour de 2 applications tr\u00e8s simples. La premi\u00e8re voit son code source h\u00e9berg\u00e9 dans un d\u00e9p\u00f4t Git tandis que la seconde est packag\u00e9e et mise \u00e0 disposition dans un d\u00e9p\u00f4t Helm. Nous couvrirons donc ces deux types de d\u00e9ploiement.</p> <p>Nous mettrons \u00e9galement en place des notifications pour nous alerter via une messagerie instantan\u00e9e (Discord) des \u00e9volutions \u00e9ventuelles de nos applications.</p> <pre><code>\ngraph TD\n\nA(utilisateurs)\nB((ingress controler))\nC{service 'agnhost'}\nD{service 'podinfor'}\nE[deployment 'agnhost']\nF[deployment 'podinfo']\n\nA -.-&gt; B\nB --&gt; C &amp; D\n\nsubgraph application 'agnhost'\nC --&gt; E\nend\n\nsubgraph application 'podinfo'\nD --&gt; F\nend</code></pre>"},{"location":"FluxCD/FluxCD_demonstration_par_l_exemple/#pre-requis","title":"Pr\u00e9-requis","text":""},{"location":"FluxCD/FluxCD_demonstration_par_l_exemple/#preparation-de-notre-environnement-de-travail-en-local","title":"Pr\u00e9paration de notre environnement de travail en local","text":"<p>Nous aurons d\u00e9j\u00e0 suivi les howtos suivant pour pr\u00e9parer notre environnement de travail sur notre laptop avec les CLIs et un cluster Kins op\u00e9rationnel :</p> howto Link Command Line Tools https://papafrancky.github.io/000_setup/Command_line_tools/ Kubernetes en local https://papafrancky.github.io/000_setup/Kubernetes_en_local/"},{"location":"FluxCD/FluxCD_demonstration_par_l_exemple/#creation-des-depots-github","title":"Cr\u00e9ation des d\u00e9p\u00f4ts GitHub","text":"<p>Commen\u00e7ons par nous authentifier sur GitHub et cr\u00e9ons deux nouveaux d\u00e9p\u00f4ts priv\u00e9s :</p> D\u00e9p\u00f4t Usage k8s-kind-fluxcd d\u00e9p\u00f4t GitHub d\u00e9di\u00e9 \u00e0 FluxCD sur notre cluster k8s-kind-apps d\u00e9p\u00f4t GitHub d\u00e9di\u00e9 \u00e0 l'h\u00e9bergement des applications \u00e0 d\u00e9ployer via FluxCD <p>Note</p> <p>Nos d\u00e9p\u00f4ts ont pour pr\u00e9fixe 'k8s-kind-' parce que nous utilisions pr\u00e9alablement pour nos travaux pratiques un cluster local 'KinD' (ie. 'Kubernetes in Docker'). Nous avons depuis opt\u00e9 pour 'Rancher Desktop'.</p> <p>Cr\u00e9ation du d\u00e9p\u00f4t GitHub d\u00e9di\u00e9 \u00e0 FluxCD : </p> <p>Cr\u00e9ation du d\u00e9p\u00f4t GitHub d\u00e9di\u00e9 aux applications : </p>"},{"location":"FluxCD/FluxCD_demonstration_par_l_exemple/#clonage-des-depots-en-local","title":"Clonage des d\u00e9p\u00f4ts en local","text":"<p>Une fois les d\u00e9p\u00f4ts cr\u00e9\u00e9s, nous les clonons sur notre laptop :</p> <p>Note</p> <p>Nous clonerons tous nos d\u00e9p\u00f4ts dans le r\u00e9pertoire renseign\u00e9 dans la variable ${LOCAL_GITHUB_REPOS}.</p> <pre><code>export LOCAL_GITHUB_REPOS=\"${HOME}/code/github\"\n\ncd ${LOCAL_GITHUB_REPOS}\ngit clone git@github.com:papafrancky/k8s-kind-fluxcd.git\ngit clone git@github.com:papafrancky/k8s-kind-apps.git\n</code></pre>"},{"location":"FluxCD/FluxCD_demonstration_par_l_exemple/#bootstrap-de-fluxcd","title":"Bootstrap de FluxCD","text":"<p>Le projet Flux est compos\u00e9 d'un outil en ligne de commande (le Flux CLI) et d'une s\u00e9rie de contr\u00f4leurs Kubernetes.</p> <p>Pour installer FluxCD, vous devez d'abord t\u00e9l\u00e9charger le CLI de Flux. Ensuite, \u00e0 l'aide de la CLI, vous pouvez d\u00e9ployer les contr\u00f4leurs Flux sur vos clusters et configurer votre premier pipeline de livraison GitOps.</p> <p>La commande 'flux bootstrap github' d\u00e9ploie les contr\u00f4leurs Flux sur un cluster Kubernetes et configure ces derniers pour synchroniser l'\u00e9tat du cluster \u00e0 partir d'un d\u00e9p\u00f4t GitHub. En plus d'installer les contr\u00f4leurs, la commande bootstrap pousse les manifestes de Flux vers le d\u00e9p\u00f4t GitHub et configure Flux pour qu'il se mette \u00e0 jour \u00e0 partir de Git.</p> Doc Link Install the Flux controllers https://fluxcd.io/flux/installation/#install-the-flux-controllers Flux bootstrap for GitHub https://fluxcd.io/flux/installation/bootstrap/github/ GitHub default environment variables https://docs.github.com/en/actions/learn-github-actions/variables#default-environment-variables codeoutput <pre><code>export GITHUB_USER=papaFrancky\nexport GITHUB_TOKEN=&lt;my_github_personal_access_token&gt;\nexport FLUXCD_GITHUB_REPO=k8s-kind-fluxcd\n\nflux bootstrap github \\\n  --token-auth \\\n  --owner ${GITHUB_USER} \\\n  --repository ${FLUXCD_GITHUB_REPO} \\\n  --branch=main \\\n  --path=. \\\n  --personal \\\n  --components-extra=image-reflector-controller,image-automation-controller\n</code></pre> <pre><code>\u25ba connecting to github.com\n\u25ba cloning branch \"main\" from Git repository \"https://github.com/papaFrancky/k8s-kind-fluxcd.git\"\n\u2714 cloned repository\n\u25ba generating component manifests\n\u2714 generated component manifests\n\u2714 component manifests are up to date\n\u25ba installing components in \"flux-system\" namespace\n\u2714 installed components\n\u2714 reconciled components\n\u25ba determining if source secret \"flux-system/flux-system\" exists\n\u25ba generating source secret\n\u25ba applying source secret \"flux-system/flux-system\"\n\u2714 reconciled source secret\n\u25ba generating sync manifests\n\u2714 generated sync manifests\n\u2714 sync manifests are up to date\n\u25ba applying sync manifests\n\u2714 reconciled sync configuration\n\u25ce waiting for Kustomization \"flux-system/flux-system\" to be reconciled\n\u2714 Kustomization reconciled successfully\n\u25ba confirming components are healthy\n\u2714 helm-controller: deployment ready\n\u2714 image-automation-controller: deployment ready\n\u2714 image-reflector-controller: deployment ready\n\u2714 kustomize-controller: deployment ready\n\u2714 notification-controller: deployment ready\n\u2714 source-controller: deployment ready\n\u2714 all components are healthy\n</code></pre> <p>V\u00e9rifions dans les \u00e9v\u00e9nements de FluxCD :</p> codeoutput <pre><code>flux events\n</code></pre> <pre><code>LAST SEEN          TYPE    REASON                  OBJECT                          MESSAGE\n15m                     Normal  NewArtifact             GitRepository/flux-system       stored artifact for commit 'Add Flux sync manifests'\n15m                     Normal  ReconciliationSucceeded Kustomization/flux-system       Reconciliation finished in 2.536346081s, next run in 10m0s\n15m                     Normal  Progressing             Kustomization/flux-system       CustomResourceDefinition/alerts.notification.toolkit.fluxcd.io configured\n                                                                                        CustomResourceDefinition/buckets.source.toolkit.fluxcd.io configured\n                                                                                        CustomResourceDefinition/gitrepositories.source.toolkit.fluxcd.io configured\n                                                                                        CustomResourceDefinition/helmcharts.source.toolkit.fluxcd.io configured\n                                                                                        CustomResourceDefinition/helmreleases.helm.toolkit.fluxcd.io configured\n                                                                                        CustomResourceDefinition/helmrepositories.source.toolkit.fluxcd.io configured\n                                                                                        CustomResourceDefinition/imagepolicies.image.toolkit.fluxcd.io configured\n                                                                                        CustomResourceDefinition/imagerepositories.image.toolkit.fluxcd.io configured\n                                                                                        CustomResourceDefinition/imageupdateautomations.image.toolkit.fluxcd.io configured\n                                                                                        CustomResourceDefinition/kustomizations.kustomize.toolkit.fluxcd.io configured\n                                                                                        CustomResourceDefinition/ocirepositories.source.toolkit.fluxcd.io configured\n                                                                                        CustomResourceDefinition/providers.notification.toolkit.fluxcd.io configured\n                                                                                        CustomResourceDefinition/receivers.notification.toolkit.fluxcd.io configured\n                                                                                        Namespace/flux-system configured\n                                                                                        ServiceAccount/flux-system/helm-controller configured\n                                                                                        ServiceAccount/flux-system/image-automation-controller configured\n                                                                                        ServiceAccount/flux-system/image-reflector-controller configured\n                                                                                        ServiceAccount/flux-system/kustomize-controller configured\n                                                                                        ServiceAccount/flux-system/notification-controller configured\n                                                                                        ServiceAccount/flux-system/source-controller configured\n                                                                                        ClusterRole/crd-controller-flux-system configured\n                                                                                        ClusterRole/flux-edit-flux-system configured\n                                                                                        ClusterRole/flux-view-flux-system configured\n                                                                                        ClusterRoleBinding/cluster-reconciler-flux-system configured\n                                                                                        ClusterRoleBinding/crd-controller-flux-system configured\n                                                                                        Service/flux-system/notification-controller configured\n                                                                                        Service/flux-system/source-controller configured\n                                                                                        Service/flux-system/webhook-receiver configured\n                                                                                        Deployment/flux-system/helm-controller configured\n                                                                                        Deployment/flux-system/image-automation-controller configured\n                                                                                        Deployment/flux-system/image-reflector-controller configured\n                                                                                        Deployment/flux-system/kustomize-controller configured\n                                                                                        Deployment/flux-system/notification-controller configured\n                                                                                        Deployment/flux-system/source-controller configured\n                                                                                        Kustomization/flux-system/flux-system configured\n                                                                                        NetworkPolicy/flux-system/allow-egress configured\n                                                                                        NetworkPolicy/flux-system/allow-scraping configured\n                                                                                        NetworkPolicy/flux-system/allow-webhooks configured\n                                                                                        GitRepository/flux-system/flux-system configured\n5m29s                   Normal  ReconciliationSucceeded Kustomization/flux-system       Reconciliation finished in 761.801712ms, next run in 10m0s\n22s (x15 over 14m)      Normal  GitOperationSucceeded   GitRepository/flux-system       no changes since last reconcilation: observed revision 'main@sha1:1258fc09abf6cd1bd639cd18ce4a2e9e4c1a7a9b'\n</code></pre> <p>Notre d\u00e9p\u00f4t GitHub doit \u00e9galement avoir \u00e9volu\u00e9. Mettons notre copie locale \u00e0 jour pour nous en assurer :</p> codeoutput <pre><code>export LOCAL_GITHUB_REPOS=\"${HOME}/code/github\"\ncd ${LOCAL_GITHUB_REPOS}/k8s-kind-fluxcd\n\ngit pull\ntree\n</code></pre> <pre><code>.\n\u2514\u2500\u2500 flux-system\n    \u251c\u2500\u2500 gotk-components.yaml\n    \u251c\u2500\u2500 gotk-sync.yaml\n    \u2514\u2500\u2500 kustomization.yaml\n\n2 directories, 3 files\n</code></pre> <p>Listons les objets cr\u00e9es sur notre cluster (dans le namespace flux-system) :</p> codeoutput <pre><code>kubectl -n flux-system get all\n</code></pre> <pre><code>NAME                                               READY   STATUS    RESTARTS   AGE\npod/helm-controller-57694fc9d6-pbl5c               1/1     Running   0          19m\npod/image-automation-controller-5f7d999559-49fms   1/1     Running   0          19m\npod/image-reflector-controller-58db7c9785-mjfh5    1/1     Running   0          19m\npod/kustomize-controller-7f689848b9-k7hmd          1/1     Running   0          19m\npod/notification-controller-6cffcffd7d-rkmwl       1/1     Running   0          19m\npod/source-controller-7f95c446b6-b8gcd             1/1     Running   0          19m\n\nNAME                              TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)   AGE\nservice/notification-controller   ClusterIP   10.96.206.29   &lt;none&gt;        80/TCP    19m\nservice/source-controller         ClusterIP   10.96.94.126   &lt;none&gt;        80/TCP    19m\nservice/webhook-receiver          ClusterIP   10.96.125.18   &lt;none&gt;        80/TCP    19m\n\nNAME                                          READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/helm-controller               1/1     1            1           19m\ndeployment.apps/image-automation-controller   1/1     1            1           19m\ndeployment.apps/image-reflector-controller    1/1     1            1           19m\ndeployment.apps/kustomize-controller          1/1     1            1           19m\ndeployment.apps/notification-controller       1/1     1            1           19m\ndeployment.apps/source-controller             1/1     1            1           19m\n\nNAME                                                     DESIRED   CURRENT   READY   AGE\nreplicaset.apps/helm-controller-57694fc9d6               1         1         1       19m\nreplicaset.apps/image-automation-controller-5f7d999559   1         1         1       19m\nreplicaset.apps/image-reflector-controller-58db7c9785    1         1         1       19m\nreplicaset.apps/kustomize-controller-7f689848b9          1         1         1       19m\nreplicaset.apps/notification-controller-6cffcffd7d       1         1         1       19m\nreplicaset.apps/source-controller-7f95c446b6             1         1         1       19m\n</code></pre>"},{"location":"FluxCD/FluxCD_demonstration_par_l_exemple/#integration-continue-avec-fluxcd","title":"Int\u00e9gration continue avec FluxCD","text":"<p>FluxCD peut g\u00e9rer l'automatisation des d\u00e9ploiements d'applications packag\u00e9es avec Helm ou bien directement depuis un d\u00e9p\u00f4t Git. Nous allons d'abord nous concentrer sur le d\u00e9ploiement d'applications depuis un d\u00e9p\u00f4t Git (GitHub dans notre cas).</p> <p>Les objets de FluxCD sont un peu comme des poup\u00e9es Russes, il est important de garder en t\u00eate leurs interd\u00e9pendances pour comprendre l'ordre dans lequel nous devrons les cr\u00e9er.</p> <pre><code>graph RL\n\nImagePolicy(\"Image Policy\")\nImageRepository(\"Image Repository\")\nImageRegistry(\"Docker image registry\")\nDeployment(\"Deployment\")\nImageUpdateAutomation(\"Image Update Automation\")\nGithubRepository(\"GitHub Repository\")\nKustomization(\"Kustomization\")\nGitRepository(\"Git Repository\")\nDeployKeys(\"(secret) deploy keys\")\nHelmRelease(\"Helm Release\")\nHelmRepository(\"Helm Repository\")\nhelmrepository(\"Helm Registry\")\nAlert(\"Alert\")\nProvider(\"Provider\")\nInstantMessaging(\"Discord Instant Messaging\")\nWebhook(\"(secret) Discord Webhook\")\n\nclassDef FluxCDObject fill:olivedrab,stroke:darkolivegreen,stroke-width:3px;\nclass ImagePolicy,ImageRepository,ImageUpdateAutomation,Kustomization,GitRepository,HelmRelease,HelmRepository,Alert,Provider FluxCDObject\n\nImageRegistry ----&gt; ImageRepository\nImageRegistry --&gt; Deployment\nImageRepository --&gt; ImagePolicy\nDeployment --&gt; GithubRepository\nGithubRepository --&gt; ImageUpdateAutomation &amp; GitRepository\nDeployKeys --&gt; GitRepository\nGitRepository --&gt; Kustomization\n\nHelmRepository --&gt; HelmRelease\nhelmrepository ----&gt; HelmRepository\n\nInstantMessaging &amp; Webhook ----&gt; Provider\nProvider --&gt; Alert</code></pre>"},{"location":"FluxCD/FluxCD_demonstration_par_l_exemple/#gestion-des-applications-depuis-un-depot-git","title":"Gestion des applications depuis un d\u00e9p\u00f4t Git","text":""},{"location":"FluxCD/FluxCD_demonstration_par_l_exemple/#namespace-dedie-a-lapplication","title":"Namespace d\u00e9di\u00e9 \u00e0 l'application","text":"<p>Pour illustrer l'int\u00e9gration continue d'une application dont le code source est h\u00e9berg\u00e9 dans un d\u00e9p\u00f4t Git, nous prendrons une application simple : 'agnhost'.</p> <p>Info</p> <p>Informations sur l'application 'agnhost'</p> <p>L'application sera ex\u00e9cut\u00e9e dans un namespace \u00e9ponyme d\u00e9di\u00e9 que nous devons cr\u00e9er. Le manifest YAML de cr\u00e9ation du namespace sera plac\u00e9 dans le d\u00e9p\u00f4t GitHub d\u00e9di\u00e9 \u00e0 FluxCD.</p> <p>Note</p> <p>Nous allons demander \u00e0 FluxCD de cr\u00e9er ce namespace tout de suite car nous devrons vite y rattacher de nouveaux objets en lien avec l'application. Si le namespace n'existe pas, nous ne pourrons tout simplement pas les cr\u00e9er.</p> <pre><code>export LOCAL_GITHUB_REPOS=\"${HOME}/code/github\"\n\ncd ${LOCAL_GITHUB_REPOS}/k8s-kind-fluxcd\nmkdir -p apps/agnhost\n\nkubectl create namespace agnhost --dry-run=client -o yaml &gt; apps/agnhost/agnhost.namespace.yaml\n\ngit add .\ngit commit -m 'Creating agnhost namespace.'\ngit push\n\nflux reconcile kustomization flux-system --with-source\n</code></pre> <p>V\u00e9rifions la cr\u00e9ation du namespace :</p> codenamespace 'agnhost' <pre><code>kubectl get namespace agnhost\n</code></pre> <pre><code>NAME      STATUS   AGE\nagnhost   Active   12s\n</code></pre>"},{"location":"FluxCD/FluxCD_demonstration_par_l_exemple/#depot-github-dedie-aux-applications","title":"D\u00e9p\u00f4t GitHub d\u00e9di\u00e9 aux applications","text":"<p>Nous d\u00e9cidons ici d'h\u00e9berger toutes nos applications dans un seul d\u00e9p\u00f4t GitHub (mais nous aurions tr\u00e8s bien pu d\u00e9cider que chaque application disposait d'un d\u00e9p\u00f4t GitHub qui serait d\u00e9di\u00e9) : <code>github.com/${GITHUB_USERNAME}/k8s-kind-apps</code>.</p>"},{"location":"FluxCD/FluxCD_demonstration_par_l_exemple/#le-micro-service-agnhost","title":"Le micro-service 'agnhost'","text":"<p>Plus haut, nous avons montr\u00e9 comment le d\u00e9finir sur la plate-forme GitHub, puis comment cr\u00e9er une copie de ce dernier en local avec la commande <code>git clone</code>.</p> <p>Dans cette copie locale, nous allons d\u00e9finir le micro-service 'agnhost' compos\u00e9 d'un 'deployment' et d'un 'service' :</p> codeoutput <pre><code>export LOCAL_GITHUB_REPOS=\"${HOME}/code/github\"\n\ncd ${LOCAL_GITHUB_REPOS}/k8s-kind-apps\nmkdir agnhost\n\n# deployment :\ncat &lt;&lt; EOF &gt;&gt; agnhost/agnhost.deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: agnhost\n  name: agnhost\n  namespace: agnhost\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: agnhost\n  template:\n    metadata:\n      labels:\n        app: agnhost\n    spec:\n      containers:\n      - name: agnhost\n        image: registry.k8s.io/e2e-test-images/agnhost:2.39\n        command:\n        - /agnhost\n        - netexec\n        - --http-port\n        - \"8080\"\nEOF\n\n# service : \ncat &lt;&lt; EOF &gt;&gt; agnhost/agnhost.service.yaml\n---\nkind: Service\napiVersion: v1\nmetadata:\n  name: agnhost\n  namespace: agnhost\nspec:\n  selector:\n    app: agnhost\n  ports:\n  # Default port used by the image\n  - port: 8080\nEOF\n\n# Arborescence des fichiers g\u00e9n\u00e9r\u00e9s :\ntree\n</code></pre> <pre><code>.\n\u2514\u2500\u2500 agnhost\n    \u251c\u2500\u2500 agnhost.deployment.yaml\n    \u2514\u2500\u2500 agnhost.service.yaml\n\n2 directories, 2 files\n</code></pre> <p>Poussons nos d\u00e9finitions sur le d\u00e9p\u00f4t GitHub :</p> <pre><code>export LOCAL_GITHUB_REPOS=\"${HOME}/code/github\"\n\ncd ${LOCAL_GITHUB_REPOS}/k8s-kind-apps\n\ngit add .\ngit commit -m 'Added agnhost application manifests.'\ngit push\n</code></pre>"},{"location":"FluxCD/FluxCD_demonstration_par_l_exemple/#definition-du-gitrepository","title":"D\u00e9finition du GitRepository","text":"<p>Doc</p> <p>https://fluxcd.io/flux/components/source/gitrepositories/</p> <p>Nous allons d\u00e9finir au niveau de FluxCD le d\u00e9p\u00f4t GitHub qui h\u00e9bergera nos applications et lui permettre de s'y connecter avec des droits d'\u00e9criture : <code>k8s-kind-apps</code>.</p>"},{"location":"FluxCD/FluxCD_demonstration_par_l_exemple/#deploy-keys","title":"Deploy Keys","text":"<p>Pour permettre \u00e0 FluxCD de se connecter au d\u00e9p\u00f4t GitHub des applications dont il doit g\u00e9rer l'int\u00e9gration continue, nous devons cr\u00e9er une paire de cl\u00e9s SSH et d\u00e9ployer la cl\u00e9 publique sur les d\u00e9p\u00f4ts concern\u00e9s.</p> <p>Nous avons besoin de d\u00e9finir les 'deploy keys' avant de pouvoir d\u00e9finir un 'GitRepository'.</p> <p>S'agissant de 'secrets', nous ne conserverons pas le manifest YAML un le d\u00e9p\u00f4t GitHub. La solution id\u00e9ale serait d'utiliser un coffre (ou 'vault') pour g\u00e9rer les 'secrets' en toute s\u00e9curit\u00e9, sujet que nous couvrirons dans un autre HOWTO.</p>"},{"location":"FluxCD/FluxCD_demonstration_par_l_exemple/#creation-de-la-deploy-key","title":"Cr\u00e9ation de la 'Deploy Key'","text":"<pre><code>export GITHUB_USERNAME=papafrancky\n\nflux create secret git k8s-kind-apps-gitrepository-deploykeys \\\n  --url=ssh://github.com/${GITHUB_USERNAME}/k8s-kind-apps \\\n  --namespace=agnhost\n</code></pre> <p>V\u00e9rifions la bonne cr\u00e9ation de la 'deploy key' pour de d\u00e9p\u00f4t des applications :</p> codeoutput <pre><code>kubectl -n agnhost get secret k8s-kind-apps-gitrepository-deploykeys\n</code></pre> <pre><code>NAME                                     TYPE     DATA   AGE\nk8s-kind-apps-gitrepository-deploykeys   Opaque   3      35s\n</code></pre> <p>De ce 'secret' qui contient un jeu de cl\u00e9s priv\u00e9e et publique, nous devons extraire la cl\u00e9 publique pour la renseigner sur notre d\u00e9p\u00f4t GitHub :</p> codeoutput <pre><code>kubectl -n agnhost get secret k8s-kind-apps-gitrepository-deploykeys -o jsonpath='{.data.identity\\.pub}' | base64 -D\n</code></pre> <pre><code>ecdsa-sha2-nistp384 AAAAE2VjZHNhLXNoYTItbmlzdHAzODQAAAAIbmlzdHAzODQAAABhBN8q8Xb3gUEQkhoLmYDlAnYdom1GBC+mJ//OH1r4OJvYszU0zBhq2+Xa9P3O6CywbRYIaP8yCtO+NBpZGx8ZDPP1WfgPDs5BPjLVE6Q+HNskPsx4sNHkM3SIc/BcFnzMUw==\n</code></pre>"},{"location":"FluxCD/FluxCD_demonstration_par_l_exemple/#deploiement-de-la-deploy-key-sur-le-depot-github","title":"D\u00e9ploiement de la 'Deploy Key' sur le d\u00e9p\u00f4t GitHub","text":"<p>Une fois sur la page de d\u00e9p\u00f4t, cliquer sur le bouton Settings, puis dans la colonne de gauche sur la page suivante, sur le line Deploy Keys dans la partie 'Security' :</p> <p></p> <p></p> <p></p> <p>Warning</p> <p>La case 'Allow write access' doit \u00eatre coch\u00e9e pour permettre \u00e0 FluxCD d'apporter des modifications dans le d\u00e9p\u00f4t !</p> <p></p> <p></p>"},{"location":"FluxCD/FluxCD_demonstration_par_l_exemple/#definition-du-gitrepository-k8s-kind-apps","title":"D\u00e9finition du GitRepository \"k8s-kind-apps\"","text":"<p>Doc</p> <p>https://fluxcd.io/flux/components/source/gitrepositories/</p> <p>Voici les informations qu'il faudra donner pour d\u00e9finir un 'GitRepository' :</p> <ul> <li>Le nom que nous souhaitons lui donner : k8s-kind-apps;</li> <li>L'URL du d\u00e9p\u00f4t GitHub : ssh://git@github.com/${GITHUB_USERNAME}/k8s-kind-apps.git;</li> <li>La branche du d\u00e9p\u00f4t d'o\u00f9 r\u00e9cup\u00e9rer le code : main;</li> <li>Le secret d'o\u00f9 extraire la cl\u00e9 priv\u00e9e pour se connecter au d\u00e9p\u00f4t GitHub : k8s-kind-apps-gitrepository-deploykeys.</li> </ul> code'k8s-kind-apps' GitRepository <pre><code>export LOCAL_GITHUB_REPOS=\"${HOME}/code/github\"\nexport GITHUB_USERNAME=papafrancky\n\ncd ${LOCAL_GITHUB_REPOS}/k8s-kind-fluxcd\n\nflux create source git k8s-kind-apps \\\n  --url=ssh://git@github.com/${GITHUB_USERNAME}/k8s-kind-apps.git \\\n  --branch=main \\\n  --secret-ref=k8s-kind-apps-gitrepository-deploykeys \\\n  --namespace=agnhost \\\n  --export &gt; apps/agnhost/k8s-kind-apps.gitrepository.yaml\n</code></pre> <pre><code>---\napiVersion: source.toolkit.fluxcd.io/v1\nkind: GitRepository\nmetadata:\n  name: k8s-kind-apps\n  namespace: agnhost\nspec:\n  interval: 1m0s\n  ref:\n    branch: main\n  secretRef:\n    name: k8s-kind-apps-gitrepository-deploykeys\n  url: ssh://git@github.com/papafrancky/k8s-kind-apps.git\n</code></pre>"},{"location":"FluxCD/FluxCD_demonstration_par_l_exemple/#definition-de-la-kustomization-pour-notre-application-agnhost","title":"D\u00e9finition de la 'Kustomization' pour notre application 'agnhost'","text":"<p>Doc</p> <p>https://fluxcd.io/flux/cmd/flux_create_kustomization/ https://fluxcd.io/flux/components/kustomize/kustomizations/</p> <p>Tip</p> <p>Nommer le manifest 'kustomize.yml' pose des probl\u00e8mes, le nom doit \u00eatre r\u00e9serv\u00e9 pour les besoins internes de Flux. Nous le nommerons 'sync.yaml'.</p> codekustomization 'agnhost' <pre><code>export LOCAL_GITHUB_REPOS=\"${HOME}/code/github\"\ncd ${LOCAL_GITHUB_REPOS}/k8s-kind-fluxcd\n\nflux create kustomization agnhost \\\n    --source=GitRepository/k8s-kind-apps.agnhost \\\n    --path=./agnhost \\\n    --prune=true \\\n    --namespace=agnhost \\\n    --export  &gt; apps/agnhost/agnhost.kustomization.yaml\n</code></pre> <pre><code>---\napiVersion: kustomize.toolkit.fluxcd.io/v1\nkind: Kustomization\nmetadata:\n  name: agnhost\n  namespace: agnhost\nspec:\n  interval: 1m0s\n  path: ./agnhost\n  prune: true\n  sourceRef:\n    kind: GitRepository\n    name: k8s-kind-apps\n    namespace: agnhost\n</code></pre> <p>Note</p> <p>Nous pourrions ne d\u00e9finir qu'une 'kustomization' pour l'ensemble du d\u00e9p\u00f4t GitHub d\u00e9di\u00e9 aux applications. Nous privil\u00e9gions ici une approche plus atomique en cr\u00e9ant une 'kustomization' pour chaque application.</p> <p>Il est temps de pousser nos modifications dans le d\u00e9p\u00f4t GitHub :</p> <pre><code>export LOCAL_GITHUB_REPOS=\"${HOME}/code/github\"\ncd ${LOCAL_GITHUB_REPOS}/k8s-kind-fluxcd\n\ngit status\ngit add .\ngit commit -m \"feat: added GitRepository and Kustomization for 'agnhost' app.\"\ngit push\n\nflux reconcile kustomization flux-system --with-source\n</code></pre> <p>For\u00e7ons la r\u00e9conciliation :</p> <pre><code>flux reconcile kustomization flux-system --with-source\nflux -n agnhost reconcile kustomization agnhost --with-source\n</code></pre> <p>Nous devrions d\u00e9sormais voir le GitRepository d\u00e9fini au niveau du cluster :</p> codeoutput  <pre><code>kubectl -n agnhost get gitrepository k8s-kind-apps\n</code></pre> <pre><code>NAME            URL                                                  AGE   READY   STATUS\nk8s-kind-apps   ssh://git@github.com/papafrancky/k8s-kind-apps.git   36s   True    stored artifact for revision 'main@sha1:fe6dda52ecf1c3d031aea013e1b9f4a2ed8fba9c'\n</code></pre> <p>Surtout, nous devrions voir notre application 'agnhost' \u00e9ploy\u00e9e sur le cluster :</p> codeoutput <pre><code>kubectl -n agnhost get deployments,services,pods\n</code></pre> <pre><code>NAME                      READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/agnhost   1/1     1            1           1m20s\n\nNAME              TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE\nservice/agnhost   ClusterIP   10.43.179.127   &lt;none&gt;        8080/TCP   1m20s\n\nNAME                           READY   STATUS    RESTARTS   AGE\npod/agnhost-597fb984fd-brr4d   1/1     Running   0          1m21s\n</code></pre>"},{"location":"FluxCD/FluxCD_demonstration_par_l_exemple/#automatisation-de-la-mise-a-jour-des-images","title":"Automatisation de la mise \u00e0 jour des images","text":"<p>Info</p> <p>https://pkg.go.dev/k8s.io/kubernetes/test/images/agnhost#section-readme</p> <p>Notre application est conteneuris\u00e9e et utilise par cons\u00e9quent une image Docker : <code>e2e-test-images/agnhost:2.39</code>.</p> <p>Nous souhaiterions que Flux la mette \u00e0 jour automatiquement si une nouvelle image venait \u00e0 \u00eatre publi\u00e9e. La mise en place d'un tel process d'automatisation n\u00e9cessite la d\u00e9finition pr\u00e9alable d'un 'ImageRepository' auquel nous associerons une 'ImagePolicy'.</p>"},{"location":"FluxCD/FluxCD_demonstration_par_l_exemple/#imagerepository","title":"ImageRepository","text":"<p>Comme son nom l'indique, cet objet d\u00e9finit un d\u00e9p\u00f4t d'images Docker.</p> code <pre><code>export LOCAL_GITHUB_REPOS=\"${HOME}/code/github\"\nexport GITHUB_USERNAME=papafrancky\n\ncd ${LOCAL_GITHUB_REPOS}/k8s-kind-fluxcd\n\nflux create image repository agnhost \\\n  --image=registry.k8s.io/e2e-test-images/agnhost \\\n  --interval=5m \\\n  --namespace=agnhost \\\n  --export &gt; apps/agnhost/agnhost.imagerepository.yaml\n</code></pre>"},{"location":"FluxCD/FluxCD_demonstration_par_l_exemple/#imagepolicy","title":"ImagePolicy","text":"<p>Doc</p> <p>https://fluxcd.io/flux/components/image/imagepolicies/#policy</p> <p>https://github.com/Masterminds/semver#checking-version-constraints</p> <p>Warning</p> <p>Les images dans le d\u00e9p\u00f4t ne sont suivent pas le versionnement 'SemVer'. Nous devons ici choisir une policy de type numerical (autres choix possibles: semver et alphabetical) et trier les tags de l'image agnhost par ordre croissant pour arriver \u00e0 nos fins.</p> <p>Une 'image policy' dicter la r\u00e8gle de s\u00e9lection l'image, comme par exemple l'image la plus r\u00e9cente, ou la derni\u00e8re image \u00e0 une version majeure donn\u00e9es, etc...</p> <p>Doc</p> <p>https://fluxcd.io/flux/cmd/flux_create_image_policy/</p> <p>Dans cet exemple, nous d\u00e9cidons de mettre \u00e0 jour notre application \u00e0 sa version la plus ancienne. Bien \u00e9videmment, en r\u00e9alit\u00e9 cela n'a pas de sens car nous attendons de l'int\u00e9gration continue que nos applications restent le plus \u00e0 jour. Mais nous le faisons volontairement car nous mettrons bientot du monitoring en place et validerons son bon fonctionnement en corrigeant notre 'ImagePolicy'.</p> <pre><code>cd ${LOCAL_GITHUB_REPOS}/k8s-kind-fluxcd\n\nflux create image policy agnhost \\\n  --image-ref=agnhost \\\n  --namespace=agnhost \\\n  --select-numeric=desc \\\n  --filter-regex='\\d\\.\\d\\d' \\\n  --export &gt; apps/agnhost/agnhost.imagepolicy.yaml\n\ngit add .\ngit commit -m \"feat: defined an image repository and image policy for the agnhost application.\"\ngit push\n\nflux reconcile kustomization flux-system --with-source\n</code></pre> <p>V\u00e9rifions la bonne cr\u00e9ation de l'Image Repository :</p> codeImage Repository 'agnhost' <pre><code>kubectl -n agnhost get imagerepository agnhost -o yaml\n</code></pre> <pre><code>apiVersion: v1\nitems:\n- apiVersion: image.toolkit.fluxcd.io/v1beta2\n  kind: ImageRepository\n  metadata:\n    creationTimestamp: \"2025-10-02T19:57:16Z\"\n    finalizers:\n    - finalizers.fluxcd.io\n    generation: 1\n    labels:\n      kustomize.toolkit.fluxcd.io/name: flux-system\n      kustomize.toolkit.fluxcd.io/namespace: flux-system\n    name: agnhost\n    namespace: agnhost\n    resourceVersion: \"328135\"\n    uid: 8f3b17ff-4c2c-4360-8e5b-53b7e3c9455a\n  spec:\n    exclusionList:\n    - ^.*\\.sig$\n    image: registry.k8s.io/e2e-test-images/agnhost\n    interval: 5m0s\n    provider: generic\n  status:\n    canonicalImageName: registry.k8s.io/e2e-test-images/agnhost\n    conditions:\n    - lastTransitionTime: \"2025-10-02T19:57:17Z\"\n      message: 'successful scan: found 33 tags'\n      observedGeneration: 1\n      reason: Succeeded\n      status: \"True\"\n      type: Ready\n    lastScanResult:\n      latestTags:\n      - \"2.9\"\n      - \"2.57\"\n      - \"2.56\"\n      - \"2.55\"\n      - \"2.54\"\n      - \"2.53\"\n      - \"2.52\"\n      - \"2.51\"\n      - \"2.50\"\n      - \"2.48\"\n      scanTime: \"2025-10-02T19:57:17Z\"\n      tagCount: 33\n    observedExclusionList:\n    - ^.*\\.sig$\n    observedGeneration: 1\nkind: List\nmetadata:\n  resourceVersion: \"\"\n</code></pre> <p>Nous constatons que le d\u00e9p\u00f4t contient 33 versions diff\u00e9rents de l'inage 'agnhost' et que la version la plus r\u00e9cente est la 2.57.</p> <p>N'oublions pas de v\u00e9rifier la bonne cr\u00e9ation de l'Image Policy :</p> codeImage Policy 'agnhost' <pre><code>kubectl -n agnhost get imagepolicy\n</code></pre> <pre><code>NAME      LATESTIMAGE\nagnhost   registry.k8s.io/e2e-test-images/agnhost:2.10\n</code></pre> <p>Notre image policy telle que nous l'avons d\u00e9finie va rechercher la version la plus ancienne de l'image Docker : <code>e2e-test-images/agnhost:2.10</code>. Lorsque l'automatisation de la mise \u00e0 jour de l'image sera compl\u00e8tement en place, notre application devrait donc \u00eatre 'downgrad\u00e9e'.</p>"},{"location":"FluxCD/FluxCD_demonstration_par_l_exemple/#reecriture-du-tag-de-limage-docker","title":"R\u00e9\u00e9criture du tag de l'image Docker","text":"<p>Nous devons maintenant indiquer \u00e0 FluxCD o\u00f9 mettre le tag de l'image \u00e0 jour dans le manifest YAML de d\u00e9ploiement de l'application 'agnhost'.</p> <p>En effet, si FluxCD est capable de d\u00e9tecter dans l'Image Repository la version de l'image souhait\u00e9e telle que d\u00e9finie dans notre 'Image Policy' (dans notre cas, la plus ancienne des versions '2.xy'), il doit aussi modifier le manifest YAML du 'deployment' en cons\u00e9quence.</p> <p>Dans le cas de nos applications, l'image Docker et sa version sont d\u00e9finis dans les manifests qui d\u00e9crivent les 'deployments' : </p> <p>Configure image update for custom resources</p> <p>https://fluxcd.io/flux/guides/image-update/#configure-image-update-for-custom-resources</p> <p>FluxCD doit \u00eatre aid\u00e9 d'un marqueur que nous apposerons \u00e0 l'endroit ad\u00e9quate.</p> <p>Le format du marqueur de l'image policy est le suivant : <pre><code>* {\"$imagepolicy\": \"&lt;policy-namespace&gt;:&lt;policy-name&gt;\"}\n* {\"$imagepolicy\": \"&lt;policy-namespace&gt;:&lt;policy-name&gt;:tag\"}\n* {\"$imagepolicy\": \"&lt;policy-namespace&gt;:&lt;policy-name&gt;:name\"}\n</code></pre></p> codeagnhost/deployment.yaml <pre><code>export LOCAL_GITHUB_REPOS=\"${HOME}/code/github\"\nexport GITHUB_USERNAME=papafrancky\n\ncd ${LOCAL_GITHUB_REPOS}/k8s-kind-apps\n\ngsed -i 's/agnhost:2\\.39/agnhost:2\\.39 # {\"$imagepolicy\": \"agnhost:agnhost\"}/' agnhost/agnhost.deployment.yaml\n\ngit add .\ngit commit -m \"feat: added a marker on foobar's pods manifests.\"\ngit push\n</code></pre> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: agnhost\n  name: agnhost\n  namespace: agnhost\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: agnhost\n  template:\n    metadata:\n      labels:\n        app: agnhost\n    spec:\n      containers:\n      - name: agnhost\n        image: registry.k8s.io/e2e-test-images/agnhost:2.39 # {\"$imagepolicy\": \"agnhost:agnhost\"}\n        command:\n        - /agnhost\n        - netexec\n        - --http-port\n        - \"8080\"\n</code></pre>"},{"location":"FluxCD/FluxCD_demonstration_par_l_exemple/#image-update-automation","title":"Image Update Automation","text":"<p>Il ne nous reste plus qu'\u00e0 tout mettre en musique en cr\u00e9ant une 'ImageUpdateAutomation' pour notre application.</p> <p>flux create image update</p> <p>https://fluxcd.io/flux/cmd/flux_create_image_update/#examples</p> codeImageUpdateAutomation 'agnhost' <pre><code>export LOCAL_GITHUB_REPOS=\"${HOME}/code/github\"\nexport AUTHOR_EMAIL=\"19983231-papafrancky@users.noreply.github.com\"\n\ncd ${LOCAL_GITHUB_REPOS}/k8s-kind-fluxcd\n\nflux create image update agnhost \\\n    --namespace=agnhost \\\n    --git-repo-ref=k8s-kind-apps \\\n    --git-repo-path=\"./agnhost\" \\\n    --checkout-branch=main \\\n    --author-name=FluxCD \\\n    --author-email=${AUTHOR_EMAIL} \\\n    --commit-template=\"{{range .Updated.Images}}{{println .}}{{end}}\" \\\n    --export &gt; apps/agnhost/agnhost.imageupdateautomation.yaml\n\n\ngit add .\ngit commit -m \"feat: defined ImageUpdateAutomations for agnhost application.\"\ngit push\n\nflux reconcile kustomization flux-system --with-source\n</code></pre> <pre><code>---\napiVersion: image.toolkit.fluxcd.io/v1beta2\nkind: ImageUpdateAutomation\nmetadata:\n  name: agnhost\n  namespace: agnhost\nspec:\n  git:\n    checkout:\n      ref:\n        branch: main\n    commit:\n      author:\n        email: 19983231-papafrancky@users.noreply.github.com\n        name: FluxCD\n      messageTemplate: '{{range .Updated.Images}}{{println .}}{{end}}'\n  interval: 1m0s\n  sourceRef:\n    kind: GitRepository\n    name: k8s-kind-apps\n  update:\n    path: ./agnhost\n    strategy: Setters\n</code></pre> <p>D\u00e9finir son adresse email de commit</p> <p>https://docs.github.com/en/account-and-profile/setting-up-and-managing-your-personal-account-on-github/managing-email-preferences/setting-your-commit-email-address#about-commit-email-addresses</p> <p>Comment r\u00e9cup\u00e9rer l'ID de son compte GitHub</p> <p>https://api.github.com/users/${GITHUB_USERNAME}</p> <p>V\u00e9rifions si la version de l'image Docker de notre application a chang\u00e9 :</p> codeoutput <pre><code>kubectl -n agnhost get pod -o jsonpath='{.items[*].spec.containers[*].image}'\n</code></pre> <pre><code>registry.k8s.io/e2e-test-images/agnhost:2.10\n</code></pre> <p>L'application 'agnhost' d\u00e9ploy\u00e9e sur notre cluster est pass\u00e9e de la version 2.39 \u00e0 2.10. Flux a bien r\u00e9pondu \u00e0 nos attentes.</p> <p>Pour constater le changement de version dans le manifest YAML de 'deployment', nous pouvons regarder soit directement sur le site GitHub, soit sur notre copie locale, \u00e0 condition de la remettre \u00e0 jour au pr\u00e9alable:</p> codeoutput <pre><code>   export LOCAL_GITHUB_REPOS=\"${HOME}/code/github\"\n\n   cd ${LOCAL_GITHUB_REPOS}/k8s-kind-apps\n   git fetch\n   git pull\n</code></pre> <pre><code>Mise \u00e0 jour 66838d6..dfa98e7\nFast-forward\nagnhost/agnhost.deployment.yaml | 2 +-\n1 file changed, 1 insertion(+), 1 deletion(-)\n</code></pre> <p>Les manifests ont bien \u00e9t\u00e9 modifi\u00e9s. V\u00e9rifions la mise \u00e0 jour de la version des images :</p> codeoutput <pre><code>export LOCAL_GITHUB_REPOS=\"${HOME}/code/github\"\n\ncd ${LOCAL_GITHUB_REPOS}/k8s-kind-apps\n\ncat agnhost/agnhost.deployment.yaml | grep image:\n</code></pre> <pre><code>image: registry.k8s.io/e2e-test-images/agnhost:2.10 # {\"$imagepolicy\": \"agnhost:agnhost\"}\n</code></pre> <p>Le tag de l'image Docker a bien \u00e9t\u00e9 r\u00e9\u00e9crit pour passer de la version 2.39 \u00e0 2.10.</p> <p>Tout fonctionne comme attendu ! </p>"},{"location":"FluxCD/FluxCD_demonstration_par_l_exemple/#gestion-des-applications-depuis-un-depot-helm","title":"Gestion des applications depuis un d\u00e9p\u00f4t Helm","text":"<p>Nous venons de couvrir l'int\u00e9gration continue d'une application dont le code source est h\u00e9berg\u00e9 dans un d\u00e9p\u00f4t Git (GitHub dans notre cas).</p> <p>FluxCD est \u00e9galement capable de g\u00e9rer des applications packag\u00e9es avec Helm directement depuis leur d\u00e9p\u00f4t de packages (et non plus de code source). C'est ce sur quoi nous allons nous concentrer \u00e0 pr\u00e9sent.</p> <p>Pour illustrer l'int\u00e9gration continue d'applications packag\u00e9es avec Helm, nous d\u00e9ploierons l'application 'podinfo' utilis\u00e9e par le projet CNCF FluxCD pour faire des tests end-to-end et des workshops.</p> <p>Info</p> <p>https://github.com/stefanprodan/podinfo</p>"},{"location":"FluxCD/FluxCD_demonstration_par_l_exemple/#namespace-dedie-a-lapplication-podinfo","title":"Namespace d\u00e9di\u00e9 \u00e0 l'application 'podinfo'","text":"<p>Comme pour la pr\u00e9c\u00e9dente application, nous devons cr\u00e9er le namespace qui h\u00e9bergera l'application 'podinfo'.</p> <p>Si nous ne cr\u00e9ons pas le namespace, il nous sera impossible de d\u00e9finir de nouveaux objets Kubernetes qui y refont r\u00e9f\u00e9rence.</p> codenamespace 'podinfo' <pre><code>   export LOCAL_GITHUB_REPOS=\"${HOME}/code/github\"\n\n   cd ${LOCAL_GITHUB_REPOS}/k8s-kind-fluxcd\n   mkdir apps/podinfo\n\n   kubectl create namespace podinfo --dry-run=client -o yaml &gt; ./apps/podinfo/namespace.yaml\n   kubectl apply -f ./apps/podinfo/podinfo.namespace.yaml\n</code></pre> <pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  creationTimestamp: null\n  name: podinfo\nspec: {}\n status: {}\n</code></pre>"},{"location":"FluxCD/FluxCD_demonstration_par_l_exemple/#le-helm-chart-podinfo","title":"Le Helm Chart 'podinfo'","text":"<p>Le d\u00e9p\u00f4t GitHub de l'application 'podinfo' propose un Helm Chart.</p> <p>Il fait donc office de 'Helm Repository' :</p> <p></p>"},{"location":"FluxCD/FluxCD_demonstration_par_l_exemple/#adresse-du-helm-chart-podinfo","title":"Adresse du Helm Chart 'podinfo'","text":"<p>Pour retrouver l'adresse o\u00f9 r\u00e9cup\u00e9rer le 'Helm Chart', cliquons sur le lien 'charts/podinfo' dans la partie 'Packages' de la colonne de droite de la UI GitHub : </p> <p></p> <p>L'URL du Helm Chart de 'podinfo' est donc : <code>oci://ghcr.io/stefanprodan/charts/podinfo</code>.</p>"},{"location":"FluxCD/FluxCD_demonstration_par_l_exemple/#authentification-aupres-du-helm-repository","title":"Authentification aupr\u00e8s du Helm Repository","text":"<p>Nous devons commencer par nous authentifier aupr\u00e8s de la 'GitHub Container Registry' (GHCR) :</p> <p>Doc</p> <p>Authenticating to the GitHub container registry with a personal access token</p> <p>Puisque notre Helm Repository est GitHub, nous utiliserons nos 'credentials' sur cette plateforme, c'est \u00e0 dire notre login et notre 'Personal Access Token' (ou 'PAT').</p> <p>Testons tout de suite avant d'\u00e9crire nos manifests YAML que nous confierons \u00e0 FluxCD :</p> codeoutput <pre><code>export GITHUB_USER=papaFrancky\nexport GITHUB_TOKEN=&lt;my_github_personal_access_token&gt;\n\necho ${GITHUB_TOKEN} | docker login ghcr.io -u ${GITHUB_USER} --password-stdin\n</code></pre> <pre><code>Login Succeeded\n</code></pre> <p>Nous sommes d\u00e9sormais en mesure de l'interroger :</p> code'podinfo' Helm Chart information'podinfo' Helm Chart values <pre><code>helm show chart oci://ghcr.io/stefanprodan/charts/podinfo\nhelm show values oci://ghcr.io/stefanprodan/charts/podinfo\n</code></pre> <pre><code>Pulled: ghcr.io/stefanprodan/charts/podinfo:6.9.2\nDigest: sha256:971fef0d04d5b3d03d035701dad59411ea0f60e28d16190f02469ddfe5587588\napiVersion: v1\nappVersion: 6.9.2\ndescription: Podinfo Helm chart for Kubernetes\nhome: https://github.com/stefanprodan/podinfo\nkubeVersion: '&gt;=1.23.0-0'\nmaintainers:\n- email: stefanprodan@users.noreply.github.com\n  name: stefanprodan\nname: podinfo\nsources:\n- https://github.com/stefanprodan/podinfo\nversion: 6.9.2    \n</code></pre> <pre><code>Pulled: ghcr.io/stefanprodan/charts/podinfo:6.9.2\nDigest: sha256:971fef0d04d5b3d03d035701dad59411ea0f60e28d16190f02469ddfe5587588\n# Default values for podinfo.\n\nreplicaCount: 1\nlogLevel: info\nhost: #0.0.0.0\nbackend: #http://backend-podinfo:9898/echo\nbackends: []\n\nimage:\n  repository: ghcr.io/stefanprodan/podinfo\n  tag: 6.9.2\n  pullPolicy: IfNotPresent\n\nui:\n  color: \"#34577c\"\n  message: \"\"\n  logo: \"\"\n\n# failure conditions\nfaults:\n  delay: false\n  error: false\n  unhealthy: false\n  unready: false\n  testFail: false\n  testTimeout: false\n\n# Kubernetes Service settings\nservice:\n  enabled: true\n  annotations: {}\n  type: ClusterIP\n  metricsPort: 9797\n  httpPort: 9898\n  externalPort: 9898\n  grpcPort: 9999\n  grpcService: podinfo\n  nodePort: 31198\n  # the port used to bind the http port to the host\n  # NOTE: requires privileged container with NET_BIND_SERVICE capability -- this is useful for testing\n  # in local clusters such as kind without port forwarding\n  hostPort:\n\n# enable h2c protocol (non-TLS version of HTTP/2)\nh2c:\n  enabled: false\n\n# config file settings\nconfig:\n  # config file path\n  path: \"\"\n  # config file name\n  name: \"\"\n\n# Additional command line arguments to pass to podinfo container\nextraArgs: []\n\n# enable tls on the podinfo service\ntls:\n  enabled: false\n  # the name of the secret used to mount the certificate key pair\n  secretName:\n  # the path where the certificate key pair will be mounted\n  certPath: /data/cert\n  # the port used to host the tls endpoint on the service\n  port: 9899\n  # the port used to bind the tls port to the host\n  # NOTE: requires privileged container with NET_BIND_SERVICE capability -- this is useful for testing\n  # in local clusters such as kind without port forwarding\n  hostPort:\n\n# create a certificate manager certificate (cert-manager required)\ncertificate:\n  create: false\n  # the issuer used to issue the certificate\n  issuerRef:\n    kind: ClusterIssuer\n    name: self-signed\n  # the hostname / subject alternative names for the certificate\n  dnsNames:\n    - podinfo\n\n# metrics-server add-on required\nhpa:\n  enabled: false\n  maxReplicas: 10\n  # average total CPU usage per pod (1-100)\n  cpu:\n  # average memory usage per pod (100Mi-1Gi)\n  memory:\n  # average http requests per second per pod (k8s-prometheus-adapter)\n  requests:\n\n# Redis address in the format tcp://&lt;host&gt;:&lt;port&gt;\ncache: \"\"\n# Redis deployment\nredis:\n  enabled: false\n  repository: redis\n  tag: 7.0.7\n\nserviceAccount:\n  # Specifies whether a service account should be created\n  enabled: false\n  # The name of the service account to use.\n  # If not set and create is true, a name is generated using the fullname template\n  name:\n  # List of image pull secrets if pulling from private registries\n  imagePullSecrets: []\n\n# set container security context\nsecurityContext: {}\n\n# set pod security context\npodSecurityContext: {}\n\ningress:\n  enabled: false\n  className: \"\"\n  additionalLabels: {}\n  annotations: {}\n    # kubernetes.io/ingress.class: nginx\n    # kubernetes.io/tls-acme: \"true\"\n  hosts:\n    - host: podinfo.local\n      paths:\n        - path: /\n          pathType: ImplementationSpecific\n  tls: []\n  #  - secretName: chart-example-tls\n  #    hosts:\n  #      - chart-example.local\n\nlinkerd:\n  profile:\n    enabled: false\n\n# create Prometheus Operator monitor\nserviceMonitor:\n  enabled: false\n  interval: 15s\n  additionalLabels: {}\n\nresources:\n  limits:\n  requests:\n    cpu: 1m\n    memory: 16Mi\n\n# Extra environment variables for the podinfo container\nextraEnvs: []\n# Example on how to configure extraEnvs\n#  - name: OTEL_EXPORTER_OTLP_TRACES_ENDPOINT\n#    value: \"http://otel:4317\"\n#  - name: MULTIPLE_VALUES\n#    value: TEST\n\nnodeSelector: {}\n\ntolerations: []\n\naffinity: {}\n\npodAnnotations: {}\n\n# https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/\ntopologySpreadConstraints: []\n\n# Disruption budget will be configured only when the replicaCount is greater than 1\npodDisruptionBudget: {}\n#  maxUnavailable: 1\n\n\n# https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes\nprobes:\n  readiness:\n    initialDelaySeconds: 1\n    timeoutSeconds: 5\n    failureThreshold: 3\n    successThreshold: 1\n    periodSeconds: 10\n  liveness:\n    initialDelaySeconds: 1\n    timeoutSeconds: 5\n    failureThreshold: 3\n    successThreshold: 1\n    periodSeconds: 10\n  startup:\n    enable: false\n    initialDelaySeconds: 10\n    timeoutSeconds: 5\n    failureThreshold: 20\n    successThreshold: 1\n    periodSeconds: 10\n</code></pre> <p>Nos 'credentials' ainsi que l'adresse du Helm Chart sont v\u00e9rifi\u00e9s et exploitables, nous pouvons continuer.</p>"},{"location":"FluxCD/FluxCD_demonstration_par_l_exemple/#le-helmrepository-podinfo","title":"Le HelmRepository 'podinfo'","text":""},{"location":"FluxCD/FluxCD_demonstration_par_l_exemple/#authentification-au-helm-repository","title":"Authentification au Helm Repository","text":"<p>Nous devons cr\u00e9er un 'secret' de type 'Docker registry' pour nous y authentifier, comme nous venons de le faire pour r\u00e9cup\u00e9rer des informations \u00e0 propos du Helm Chart. S'agissant d'un 'secret', nous ne le placerons pas dans notre d\u00e9p\u00f4t GitHub.</p> codeoutput <pre><code>export GITHUB_USER=papafrancky\nexport GITHUB_TOKEN=&lt;my_github_personal_access_token&gt;\n\nkubectl create secret docker-registry podinfo-helmrepository \\\n  --namespace=podinfo \\\n  --docker-server=ghcr.io \\\n  --docker-username=${GITHUB_USER} \\\n  --docker-password=${GITHUB_TOKEN}\n\nkubectl -n podinfo get secret podinfo-helmrepository\n</code></pre> <pre><code>apiVersion: v1\nitems:\n- apiVersion: v1\n  data:\n    .dockerconfigjson: eyJhdXRocyI6eyJnaGNyLmlvIjp7InVzZXJuYW1lIjoicGFwYWZyYX5ja9kiLCJwYXNzd29yZCI6ImdocF9vTmNQZHlQRU04SlllT4diN0VQYWZ6Yk1XQk5zNmQ0MFRuMUciLCJhdXRoIjoiY0dGd1lXWnlZVzVqYTNrNloyaHdYMjlPWTFCa2VWQkZUVGhLV1dWaloySTNSVkJoWm5waVRWZENUbk0yWkRRd0KHNHhSdz09In19fQ==\n  kind: Secret\n  metadata:\n    creationTimestamp: \"2025-10-04T09:14:05Z\"\n    name: podinfo-helmrepository\n    namespace: podinfo\n    resourceVersion: \"356702\"\n    uid: 431c3c9f-0790-49ed-928c-2e0f774b1da6\n  type: kubernetes.io/dockerconfigjson\nkind: List\nmetadata:\n  resourceVersion: \"\"\n</code></pre>"},{"location":"FluxCD/FluxCD_demonstration_par_l_exemple/#definition-du-helmrepository","title":"D\u00e9finition du HelmRepository","text":"codepodinfo HelmRepository <pre><code>export LOCAL_GITHUB_REPOS=\"${HOME}/code/github\"\n\ncd ${LOCAL_GITHUB_REPOS}/k8s-kind-fluxcd\n\nflux create source helm podinfo \\\n--namespace=podinfo \\\n--url=https://stefanprodan.github.io/podinfo \\\n--secret-ref=podinfo-helmrepository \\\n--interval=10m \\\n--export &gt; ./apps/podinfo/podinfo.helmrepository.yaml\n\n#git add .\n#git commit -m 'Defined helmrepository for podinfo application.'\n#git push\n</code></pre> <pre><code>---\napiVersion: source.toolkit.fluxcd.io/v1\nkind: HelmRepository\nmetadata:\n  name: podinfo\n  namespace: podinfo\nspec:\n  interval: 10m0s\n  secretRef:\n    name: podinfo-helmrepository\n  url: https://stefanprodan.github.io/podinfo\n</code></pre>"},{"location":"FluxCD/FluxCD_demonstration_par_l_exemple/#le-gitrepository-pour-lapplication-podinfo","title":"Le GitRepository pour l'application 'podinfo'","text":"<p>Dans la s\u00e9gr\u00e9gation des r\u00f4les entre Devs et Ops, la d\u00e9finition de la HelmRelease incombera \u00e0 l'\u00e9quipe de Dev en charge de l'application.</p> <p>C'est elle qui personnalisera son application en surchargeant les valeurs par d\u00e9faut de la Helm Chart utilis\u00e9e (nous couvrirons la d\u00e9finition de la HelmRelease ensuite).</p> <p>Ces objets seront donc d\u00e9finis dans des manifests YAML dans le d\u00e9p\u00f4t GitHub d\u00e9di\u00e9 aux applications : <code>https://github.com/${GITHUB_USER}/k8s-kind-apps</code>.</p>"},{"location":"FluxCD/FluxCD_demonstration_par_l_exemple/#deploy-key","title":"Deploy Key","text":"<p>Pour permettre \u00e0 FluxCD de se connecter au d\u00e9p\u00f4t GitHub des applications dont il doit g\u00e9rer l'int\u00e9gration continue, nous devons cr\u00e9er une paire de cl\u00e9s SSH et d\u00e9ployer la cl\u00e9 publique sur les d\u00e9p\u00f4ts concern\u00e9s.</p> <p>Nous avons besoin de d\u00e9finir les 'deploy keys' avant de pouvoir d\u00e9finir un 'GitRepository'.</p> <p>S'agissant de 'secrets', nous ne conserverons pas le manifest YAML un le d\u00e9p\u00f4t GitHub. La solution id\u00e9ale serait d'utiliser un coffre (ou 'vault') pour g\u00e9rer les 'secrets' en toute s\u00e9curit\u00e9, sujet que nous couvrirons dans un autre HOWTO.</p>"},{"location":"FluxCD/FluxCD_demonstration_par_l_exemple/#creation-de-la-deploy-key_1","title":"Cr\u00e9ation de la 'Deploy Key'","text":"<pre><code>export GITHUB_USERNAME=papafrancky\n\nflux create secret git k8s-kind-apps-gitrepository-deploykeys \\\n  --url=ssh://github.com/${GITHUB_USERNAME}/k8s-kind-apps \\\n  --namespace=podinfo\n</code></pre> <p>V\u00e9rifions la bonne cr\u00e9ation de la 'deploy key' pour de d\u00e9p\u00f4t des applications :</p> codeoutput <pre><code>kubectl -n podinfo get secret k8s-kind-apps-gitrepository-deploykeys\n</code></pre> <pre><code>NAME                                     TYPE     DATA   AGE\nk8s-kind-apps-gitrepository-deploykeys   Opaque   3      3s\n</code></pre> <p>De ce 'secret' qui contient un jeu de cl\u00e9s priv\u00e9e et publique, nous devons extraire la cl\u00e9 publique pour la renseigner sur notre d\u00e9p\u00f4t GitHub :</p> codeoutput <pre><code>kubectl -n podinfo get secret k8s-kind-apps-gitrepository-deploykeys -o jsonpath='{.data.identity\\.pub}' | base64 -D\n</code></pre> <pre><code>ecdsa-sha2-nistp384 AAAAE2VjZHNhLXNoYTItbmlzdHAzODQAAAAIbmlzdHAzODQAAABhBKbw9yMbN8SPIJQ9P8DZwGU7WJZuwKtrc4KM23aHLFBCQyx8op4/v62/ehtiaZnrH45g9OLGCwhq583Sc/DrgCl/cPJSaWbDVkXGoxfwPYG5uhrS0kejbPXWtlSAUZwOEg==\n</code></pre>"},{"location":"FluxCD/FluxCD_demonstration_par_l_exemple/#deploiement-de-la-deploy-key-sur-le-depot-github_1","title":"D\u00e9ploiement de la 'Deploy Key' sur le d\u00e9p\u00f4t GitHub","text":"<p>Nous devons ensuite d\u00e9ployer la cl\u00e9 publique sur le d\u00e9p\u00f4t GitHub comme nous l'avons fait pour l'application 'agnhost'.</p> <p></p>"},{"location":"FluxCD/FluxCD_demonstration_par_l_exemple/#definition-du-gitrepository_1","title":"D\u00e9finition du GitRepository","text":"<p>Maintenant que nous avons d\u00e9fini la 'Deploy Key', nous pouvons nous atteler \u00e0 la d\u00e9finition du GitRepository o\u00f9 seront plac\u00e9s la HelmRelease de 'podinfo' ainsi que ses '**values' customis\u00e9es :</p> codeoutput <pre><code>export GITHUB_USERNAME=papafrancky\nexport LOCAL_GITHUB_REPOS=\"${HOME}/code/github\"\n\ncd ${LOCAL_GITHUB_REPOS}/k8s-kind-fluxcd\n\nflux create source git k8s-kind-apps \\\n  --url=ssh://git@github.com/${GITHUB_USERNAME}/k8s-kind-apps.git \\\n  --branch=main \\\n  --secret-ref=k8s-kind-apps-gitrepository-deploykeys \\\n  --namespace=podinfo \\\n  --export &gt; apps/podinfo/k8s-kind-apps.gitrepository.yaml\n</code></pre> <pre><code>---\napiVersion: source.toolkit.fluxcd.io/v1\nkind: GitRepository\nmetadata:\n  name: k8s-kind-apps\n  namespace: podinfo\nspec:\n  interval: 1m0s\n  ref:\n    branch: main\n  secretRef:\n    name: k8s-kind-apps-gitrepository-deploykeys\n  url: ssh://git@github.com//k8s-kind-apps.git\n</code></pre>"},{"location":"FluxCD/FluxCD_demonstration_par_l_exemple/#kustomization-du-gitrepository-pour-lapplication-podinfo","title":"Kustomization du GitRepository pour l'application 'podinfo'","text":"<p>Nous devons indiquer \u00e0 FluxCD qu'il doit g\u00e9rer les manifests qu'il trouvera dansle GitRepository 'k8s-kind-apps ', dans le sous-r\u00e9pertoire d\u00e9di\u00e9 \u00e0 l'application 'podinfo' (puisque nous avons pris le parti de partager un m\u00eame d\u00e9p\u00f4t GitHub pour nos applications) : <code>./podinfo</code></p> <p>C'est le r\u00f4le de la 'Kustomization'.</p> <p>Doc</p> <p>https://fluxcd.io/flux/cmd/flux_create_kustomization/</p> codeoutput <pre><code>export LOCAL_GITHUB_REPOS=\"${HOME}/code/github\"\n\ncd ${LOCAL_GITHUB_REPOS}/k8s-kind-fluxcd\n\nflux create kustomization podinfo \\\n  --source=GitRepository/k8s-kind-apps.podinfo \\\n  --path=\"./podinfo\" \\\n  --prune=true \\\n  --interval=1m \\\n  --namespace=podinfo \\\n  --export  &gt; apps/podinfo/podinfo.kustomization.yaml\n</code></pre> <pre><code>---\napiVersion: kustomize.toolkit.fluxcd.io/v1\nkind: Kustomization\nmetadata:\n  name: podinfo\n  namespace: podinfo\nspec:\n  interval: 1m0s\n  path: ./podinfo\n  prune: true\n  sourceRef:\n    kind: GitRepository\n    name: k8s-kind-apps\n    namespace: podinfo\n</code></pre> <p>Poussons les nouveaux manifests YAML dans notre d\u00e9p\u00f4t GitHub :</p> <pre><code>export LOCAL_GITHUB_REPOS=\"${HOME}/code/github\"\n\ncd ${LOCAL_GITHUB_REPOS}/k8s-kind-fluxcd\n\ngit add .\ngit commit -m 'Added GitRepository, HelmRepository and Kustomization for podinfo app.'\ngit push\n\nflux reconcile kustomization flux-system --with-source\n</code></pre> <p>V\u00e9rifions la cr\u00e9ation des nouveaux objets Kubernetes :</p> codeoutput <pre><code>kubectl -n podinfo get gitrepo,helmrepo,ks\n</code></pre> <pre><code>NAME                                                   URL                                                  AGE   READY   STATUS\ngitrepository.source.toolkit.fluxcd.io/k8s-kind-apps   ssh://git@github.com/papafrancky/k8s-kind-apps.git   1m   True    stored artifact for revision 'main@sha1:dfa98e76317ed1c3d1901d721e53d55e4f61f96c'\n\nNAME                                              URL                                      AGE   READY   STATUS\nhelmrepository.source.toolkit.fluxcd.io/podinfo   https://stefanprodan.github.io/podinfo   1m   True    stored artifact: revision 'sha256:c0d4535103105a4bb59954a178f24bdb7dbac3072758312c8b3f09fb3d85f192'\n\nNAME                                                AGE   READY   STATUS\nkustomization.kustomize.toolkit.fluxcd.io/podinfo   1m   False   kustomization path not found: stat /tmp/kustomization-2457791261/podinfo: no such file or directory\n</code></pre> <p>Nous constatons que la 'kustomization' ne se trouve pas dans l'\u00e9tat attendu. Et pour cause : nous n'avons pas encore d\u00e9fini aucun objet (HelmRelease et custom values) dans le d\u00e9p\u00f4t GitHub des applications 'k8s-kind-apps', qui ne contient en cons\u00e9quence pas de sous-r\u00e9pertoire 'podinfo' !</p> <p>Il est temps de s'en occuper \u00e0 pr\u00e9sent.</p>"},{"location":"FluxCD/FluxCD_demonstration_par_l_exemple/#la-helm-release","title":"La Helm Release","text":"<p>Une 'Helm Release' est une instance d'une 'Helm Chart' d\u00e9ploy\u00e9e sur un cluster Kubernetes.</p> <p>Doc</p> <p>https://helm.sh/docs/glossary/#release</p>"},{"location":"FluxCD/FluxCD_demonstration_par_l_exemple/#personnalisation-de-la-helm-release-podinfo","title":"Personnalisation de la Helm Release 'podinfo'","text":"<p>L'application 'podinfo' est param\u00e9tr\u00e9e avec des valeurs par d\u00e9faut d\u00e9finies dans ce qu'on appelle les 'values' de la Helm Chart.</p> <p>Pour conna\u00eetre les valeurs par d\u00e9faut de l'application : </p> codepodinfo default values <pre><code>export GITHUB_USER=papafrancky\nexport GITHUB_TOKEN=&lt;my_github_personal_access_token&gt;\n\n\necho ${GITHUB_TOKEN} | docker login ghcr.io -u ${GITHUB_USER} --password-stdin\n\nhelm show values oci://ghcr.io/stefanprodan/charts/podinfo\n</code></pre> <pre><code># Default values for podinfo.\n\nreplicaCount: 1\nlogLevel: info\nhost: #0.0.0.0\nbackend: #http://backend-podinfo:9898/echo\nbackends: []\n\nimage:\n  repository: ghcr.io/stefanprodan/podinfo\n  tag: 6.9.2\n  pullPolicy: IfNotPresent\n\nui:\n  color: \"#34577c\"\n  message: \"\"\n  logo: \"\"\n\n# failure conditions\nfaults:\n  delay: false\n  error: false\n  unhealthy: false\n  unready: false\n  testFail: false\n  testTimeout: false\n\n# Kubernetes Service settings\nservice:\n  enabled: true\n  annotations: {}\n  type: ClusterIP\n  metricsPort: 9797\n  httpPort: 9898\n  externalPort: 9898\n  grpcPort: 9999\n  grpcService: podinfo\n  nodePort: 31198\n  # the port used to bind the http port to the host\n  # NOTE: requires privileged container with NET_BIND_SERVICE capability -- this is useful for testing\n  # in local clusters such as kind without port forwarding\n  hostPort:\n\n# enable h2c protocol (non-TLS version of HTTP/2)\nh2c:\n  enabled: false\n\n# config file settings\nconfig:\n  # config file path\n  path: \"\"\n  # config file name\n  name: \"\"\n\n# Additional command line arguments to pass to podinfo container\nextraArgs: []\n\n# enable tls on the podinfo service\ntls:\n  enabled: false\n  # the name of the secret used to mount the certificate key pair\n  secretName:\n  # the path where the certificate key pair will be mounted\n  certPath: /data/cert\n  # the port used to host the tls endpoint on the service\n  port: 9899\n  # the port used to bind the tls port to the host\n  # NOTE: requires privileged container with NET_BIND_SERVICE capability -- this is useful for testing\n  # in local clusters such as kind without port forwarding\n  hostPort:\n\n# create a certificate manager certificate (cert-manager required)\ncertificate:\n  create: false\n  # the issuer used to issue the certificate\n  issuerRef:\n    kind: ClusterIssuer\n    name: self-signed\n  # the hostname / subject alternative names for the certificate\n  dnsNames:\n    - podinfo\n\n# metrics-server add-on required\nhpa:\n  enabled: false\n  maxReplicas: 10\n  # average total CPU usage per pod (1-100)\n  cpu:\n  # average memory usage per pod (100Mi-1Gi)\n  memory:\n  # average http requests per second per pod (k8s-prometheus-adapter)\n  requests:\n\n# Redis address in the format tcp://&lt;host&gt;:&lt;port&gt;\ncache: \"\"\n# Redis deployment\nredis:\nenabled: false\nrepository: redis\ntag: 7.0.7\n\nserviceAccount:\n  # Specifies whether a service account should be created\n  enabled: false\n  # The name of the service account to use.\n  # If not set and create is true, a name is generated using the fullname template\n  name:\n  # List of image pull secrets if pulling from private registries\n  imagePullSecrets: []\n\n# set container security context\nsecurityContext: {}\n\n# set pod security context\npodSecurityContext: {}\n\ningress:\n  enabled: false\n  className: \"\"\n  additionalLabels: {}\n  annotations: {}\n    # kubernetes.io/ingress.class: nginx\n    # kubernetes.io/tls-acme: \"true\"\n  hosts:\n    - host: podinfo.local\n      paths:\n        - path: /\n          pathType: ImplementationSpecific\n  tls: []\n  #  - secretName: chart-example-tls\n  #    hosts:\n  #      - chart-example.local\n\nlinkerd:\n  profile:\n    enabled: false\n\n# create Prometheus Operator monitor\nserviceMonitor:\n  enabled: false\n  interval: 15s\n  additionalLabels: {}\n\nresources:\n  limits:\n  requests:\n    cpu: 1m\n    memory: 16Mi\n\n# Extra environment variables for the podinfo container\nextraEnvs: []\n# Example on how to configure extraEnvs\n#  - name: OTEL_EXPORTER_OTLP_TRACES_ENDPOINT\n#    value: \"http://otel:4317\"\n#  - name: MULTIPLE_VALUES\n#    value: TEST\n\nnodeSelector: {}\n\ntolerations: []\n\naffinity: {}\n\npodAnnotations: {}\n\n# https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/\ntopologySpreadConstraints: []\n\n# Disruption budget will be configured only when the replicaCount is greater than 1\npodDisruptionBudget: {}\n#  maxUnavailable: 1\n\n# https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes\nprobes:\n  readiness:\n    initialDelaySeconds: 1\n    timeoutSeconds: 5\n    failureThreshold: 3\n    successThreshold: 1\n    periodSeconds: 10\n  liveness:\n    initialDelaySeconds: 1\n    timeoutSeconds: 5\n    failureThreshold: 3\n    successThreshold: 1\n    periodSeconds: 10\n  startup:\n    enable: false\n    initialDelaySeconds: 10\n    timeoutSeconds: 5\n    failureThreshold: 20\n    successThreshold: 1\n    periodSeconds: 10\n</code></pre> <p>Tip</p> <p>Il est \u00e9galement possible de consulter les 'default values' directement sur le site artifacthub.io :</p> <p>https://artifacthub.io/packages/helm/podinfo/podinfo?modal=values</p> <p>Par exemple, souhaitons apporter un peu plus de r\u00e9silience \u00e0 l'application en ex\u00e9cutant 2 ReplicaSets plut\u00f4t qu'un. Nous souhaitons \u00e9galement g\u00e9rer finement les ressources de l'application, et changer le message d'accueil de la UI.</p> <p>Doc</p> <p>https://github.com/stefanprodan/podinfo?tab=readme-ov-file#continuous-delivery</p> <p>Note</p> <p>Nous \u00e9crirons les 'values' que nous souhaitons surcharger aux 'default values' dans une ConfigMap Kubernetes.</p> codepodinfo-values.yaml <pre><code>export LOCAL_GITHUB_REPOS=\"${HOME}/code/github\"\n\ncd ${LOCAL_GITHUB_REPOS}/k8s-kind-apps\nmkdir podinfo\n\n# Cr\u00e9ation d'un fichier temporaire 'values.yaml' contenant les param\u00e8tres \u00e0 surcharger :\ncat &lt;&lt; EOF &gt; values.yaml\nreplicaCount: 2\nresources:\n  limits:\n    memory: 256Mi\n  requests:\n    cpu: 100m\n    memory: 64Mi\nui:\n  message: \"Hello from PodInfo ! ^^\"  \nEOF\n\n# Cr\u00e9ation de la ConfigMap \u00e0 partir du fichier 'values.yaml' :\nkubectl create configmap podinfo-values \\\n  --namespace=podinfo \\\n  --from-file=values.yaml \\\n  --dry-run=client -o yaml &gt; podinfo/podinfo.values.yaml\n\n\n# Suppression du fichier 'values.yaml' :\n/bin/rm values.yaml\n</code></pre> <pre><code>apiVersion: v1\ndata:\n  values.yaml: |+\n    replicaCount: 2\n    resources:\n      limits:\n        memory: 256Mi\n      requests:\n        cpu: 100m\n        memory: 64Mi\n    ui:\n      message: \"Hello from PodInfo ! ^^\"\n</code></pre>"},{"location":"FluxCD/FluxCD_demonstration_par_l_exemple/#definition-de-la-helmrelease","title":"D\u00e9finition de la HelmRelease","text":"<p>D\u00e9finissons maintenant la HelmRelease :</p> <p>Doc</p> <p>https://fluxcd.io/flux/cmd/flux_create_helmrelease/</p> <p>Note</p> <p>Au moment de la r\u00e9daction de ce HOWTO, la derni\u00e8re version de 'podinfo' \u00e9tait la 6.9.2. Nous choisissons ici de limiter notre 'Helm Release' \u00e0 la version 5 (<code>--chart-version=\"&lt;6.0.0\"</code>), pour tester ensuite les notifications via un service de messagerie instantan\u00e9e que nous allons tr\u00e8s vite mettre en place.</p> codepodinfo HelmRelease <pre><code>export LOCAL_GITHUB_REPOS=\"${HOME}/code/github\"\n\ncd ${LOCAL_GITHUB_REPOS}/k8s-kind-apps\n\nflux create helmrelease podinfo \\\n  --namespace=podinfo \\\n  --source=HelmRepository/podinfo.podinfo \\\n  --chart=podinfo \\\n  --chart-version=\"&lt;6.0.0\" \\\n  --values-from=ConfigMap/podinfo-values \\\n  --interval=10m \\\n  --export &gt; podinfo/podinfo.helmrelease.yaml\n\ngit add .\ngit commit -m 'Defined podinfo HelmRelease with custom values as a ConfigMap.'\ngit push\n\nflux reconcile kustomization flux-system --with-source\n</code></pre> <pre><code>---\napiVersion: helm.toolkit.fluxcd.io/v2\nkind: HelmRelease\nmetadata:\n  name: podinfo\n  namespace: podinfo\nspec:\n  chart:\n    spec:\n      chart: podinfo\n      reconcileStrategy: ChartVersion\n      sourceRef:\n        kind: HelmRepository\n        name: podinfo\n        namespace: podinfo\n  interval: 10m\n  valuesFrom:\n  - kind: ConfigMap\n    name: podinfo-values\n</code></pre> <p>Regardons si la magie op\u00e8re :</p> codeoutput <pre><code>kubectl -n podinfo get kustomization,helmrelease\n</code></pre> <pre><code>NAME                                                AGE   READY   STATUS\nkustomization.kustomize.toolkit.fluxcd.io/podinfo   29m   True    Applied revision: main@sha1:36cdfa300f5d4ad6787264956be13816ab683800\n\nNAME                                         AGE   READY   STATUS\nhelmrelease.helm.toolkit.fluxcd.io/podinfo   65s   True    Helm install succeeded for release podinfo/podinfo.v1 with chart podinfo@5.2.1 \n</code></pre> <p>Bonne nouvelle : notre 'kustomization' est d\u00e9sormais dans l'\u00e9tat attendu !</p> <p>Par ailleurs, notre 'HelmRelease' nouvellement d\u00e9finie semble elle-aussi d\u00e9ploy\u00e9e avec succ\u00e8s et \u00e0 une version inf\u00e9rieure \u00e0 6.</p> <p>V\u00e9rifions si l'application est bel et bien d\u00e9ploy\u00e9e :</p> codeoutput <pre><code>kubectl -n podinfo get all\n</code></pre> <pre><code>NAME                          READY   STATUS    RESTARTS   AGE\npod/podinfo-db75857bd-7z9vw   1/1     Running   0          5m43s\npod/podinfo-db75857bd-n6mt6   1/1     Running   0          5m43s\n\nNAME              TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)             AGE\nservice/podinfo   ClusterIP   10.43.60.99   &lt;none&gt;        9898/TCP,9999/TCP   5m43s\n\nNAME                      READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/podinfo   2/2     2            2           5m43s\n\nNAME                                DESIRED   CURRENT   READY   AGE\nreplicaset.apps/podinfo-db75857bd   2         2         2       5m43s\n</code></pre> <p>Nous observons 2 Replicasets comme nous l'avons demand\u00e9 dans nos 'custom values'.</p> <p>Utilisons la redirection de ports pour acc\u00e9der \u00e0 l'application avec un navigateur :</p> code <pre><code>kubectl -n podinfo port-forward service/podinfo 9898:9898\n</code></pre> <p> Nous voyons bien notre message d'accueil personnalis\u00e9 : 'Hello PodInfo! ^^'.</p>"},{"location":"FluxCD/FluxCD_demonstration_par_l_exemple/#notifications-discord","title":"Notifications Discord","text":"<p>Nous avons configur\u00e9 FluxCD pour g\u00e9rer automatiquement la mise \u00e0 jour de nos applications 'agnhost' et 'podinfo'.</p> <p>Nous aimerions maintenant \u00eatre alert\u00e9s lorsqu'un changement affecte nos applications. Plut\u00f4t qu'une messagerie mail classique, nous privil\u00e9gions une messagerie instantan\u00e9e. Notre choix s'est port\u00e9 sur la plateforme 'Discord' car elle permet de configurer des 'webhooks' sur des 'channels' sans pour autant devoir payer un abonnement, comme ce serait le cas avec Slack.</p>"},{"location":"FluxCD/FluxCD_demonstration_par_l_exemple/#installation-et-configuration-du-client-discord","title":"Installation et configuration du client 'Discord'","text":""},{"location":"FluxCD/FluxCD_demonstration_par_l_exemple/#installation-du-client-discord","title":"Installation du client 'Discord'","text":"<p>Pour l'installer, il faut acc\u00e9der au site web discord.com et t\u00e9l\u00e9charger le client :</p> <pre><code>https://discord.com/api/download?platform=osx\n</code></pre>"},{"location":"FluxCD/FluxCD_demonstration_par_l_exemple/#creation-dun-serveur-discord","title":"Cr\u00e9ation d'un serveur Discord","text":"<p>Discord permet la cr\u00e9ation de 'serveurs' que nous pouvons restreindre pour notre usage personnel et qui h\u00e9bergeront les 'salons' (ou 'rooms') que nous d\u00e9dierons \u00e0 l'envoi de notifications de FluxCD concernant nos applications.</p> <p>Une fois le client Discord d\u00e9marr\u00e9, cliquons sur le '+' situ\u00e9 dans la colonne de gauche. Nous r\u00e9pondrons ensuite aux diff\u00e9rentes questions et donnerons \u00e0 notre serveur le nom de notre cluster Kubernetes : k8s-kind.</p> <p></p> <p></p> <p></p> <p></p> <p>Note</p> <p>J'ai choisi l'ic\u00f4ne de 'Kubernetes' pour mon serveur Discord que j'ai nomm\u00e9 'k8s'.</p>"},{"location":"FluxCD/FluxCD_demonstration_par_l_exemple/#creation-dun-channel-dans-notre-serveur-discord","title":"Cr\u00e9ation d'un channel dans notre serveur Discord","text":"<p>Nous souhaitons cr\u00e9er un channel pour chacune de nos applications. La proc\u00e9dure \u00e9tant la m\u00eame, nous montrerons la cr\u00e9ation du salon pour l'application 'agnhost'. Vous devrez faire la m\u00eame chose pour les autres applications.</p> <p>S\u00e9lectionnez le serveur 'k8s-kind' dans la colonne de gauche, puis dans la partie 'salons textuels', cliquez sur le '+' :</p> <p></p> <p>Pr\u00e9cisez qu'il s'agit bien d'un salon textuel, pr\u00e9cisez son nom 'foo' et choisissez de le rendre priv\u00e9 :</p> <p></p> <p>Passez l'\u00e9tape d'ajout de membres :</p> <p></p> <p>Votre channel 'agnhost' est pr\u00eat. Il vous reste \u00e0 cr\u00e9er le channel 'podinfo'.</p> <p></p>"},{"location":"FluxCD/FluxCD_demonstration_par_l_exemple/#creation-dun-webhook-pour-chaque-salon","title":"Cr\u00e9ation d'un 'webhook' pour chaque salon","text":"<p>Cliquez sur la roue dent\u00e9e 'param\u00e8tres' \u00e0 droite du nom du salon, puis sur 'int\u00e9grations' et enfin sur le bouton 'Cr\u00e9er un webhook'.</p> <p></p> <p></p> <p></p> <p>Un nom lui est donn\u00e9 de mani\u00e8re al\u00e9atoire (ex: 'Spidey Bot'). Pour changer le nom du 'webhook' par 'FluxCD', copiez l'URL en cliquant sur le bouton idoine, et enregistrez les modifications :</p> <p></p> <p></p> <p>Nous r\u00e9p\u00e9tons les m\u00eames op\u00e9rations pour la cr\u00e9ation du 'channel' _'podinfo'_.</p> <p>Les URLs des webhooks des salons sont les suivants :</p> Channel Webhook URL agnhost https://discord.com/api/webhooks/1424064102791123036/JDExJGHiqCP9qcJiybnBhe8MTT_mu8lFlLM6QdYBh0RJ-5E4QUa4aGDflMjRATJwQq57 podinfo https://discord.com/api/webhooks/1424065205444415508/xEeNJVhhRpyu_mFOMWyYLJMuNhQjgr3tKPJYzs5eUHYYiNWnYgh_hMuZIABkJw8syPl4"},{"location":"FluxCD/FluxCD_demonstration_par_l_exemple/#creation-dun-secret-pour-chaque-webhook","title":"Cr\u00e9ation d'un secret pour chaque webhook","text":"<p>Ces informations sont consid\u00e9r\u00e9es comme sensibles dans la mesure o\u00f9 quiconque en disposerait pourrait publier des informations dans nos channels priv\u00e9s. Nous les enregistrerons dans Kubernetes comme des 'secrets'.</p> codewebhook 'agnhost'webhook 'podinfo' <pre><code>export WEBHOOK_AGNHOST=\"https://discord.com/api/webhooks/1424064102791123036/JDExJGHiqCP9qcJiybnBhe8MTT_mu8lFlLM6QdYBh0RJ-5E4QUa4aGDflMjRATJwQq57\"\nexport WEBHOOK_PODINFO=\"https://discord.com/api/webhooks/1424065205444415508/xEeNJVhhRpyu_mFOMWyYLJMuNhQjgr3tKPJYzs5eUHYYiNWnYgh_hMuZIABkJw8syPl4\"\n\nkubectl -n agnhost create secret generic discord-webhook --from-literal=address=${WEBHOOK_AGNHOST}\nkubectl -n podinfo create secret generic discord-webhook --from-literal=address=${WEBHOOK_PODINFO}\n</code></pre> <pre><code>apiVersion: v1\ndata:\n  address: \"\"\nkind: Secret\nmetadata:\n  creationTimestamp: \"2025-10-04T16:12:57Z\"\n  name: discord-webhook\n  namespace: agnhost\n  resourceVersion: \"392125\"\n  uid: 3959e6fd-1d9\n</code></pre> <pre><code>apiVersion: v1\ndata:\n  address: \"\"\nkind: Secret\nmetadata:\n  creationTimestamp: \"2025-10-04T16:13:03Z\"\n  name: discord-webhook\n  namespace: podinfo\n  resourceVersion: \"392141\"\n  uid: ef9c6856-c629-46ca-9cfd-6bd7ce8865e3\n\u2021type: Opaque\n</code></pre>"},{"location":"FluxCD/FluxCD_demonstration_par_l_exemple/#definition-des-notification-providers","title":"D\u00e9finition des 'notification providers'","text":"<p>Doc</p> <p>https://fluxcd.io/flux/components/notification/providers/#discord</p> codeagnhostpodinfo <pre><code>export LOCAL_GITHUB_REPOS=\"${HOME}/code/github\"\n\ncd ${LOCAL_GITHUB_REPOS}/k8s-kind-fluxcd\n\nflux create alert-provider discord \\\n  --type=discord \\\n  --secret-ref=discord-webhook \\\n  --channel=agnhost \\\n  --username=FluxCD \\\n  --namespace=agnhost \\\n  --export &gt; ./apps/agnhost/discord.notification-provider.yaml\n\nflux create alert-provider discord \\\n  --type=discord \\\n  --secret-ref=discord-webhook \\\n  --channel=podinfo \\\n  --username=FluxCD \\\n  --namespace=podinfo \\\n  --export &gt; ./apps/podinfo/discord.notification-provider.yaml\n</code></pre> <pre><code>---\napiVersion: notification.toolkit.fluxcd.io/v1beta3\nkind: Provider\nmetadata:\n  name: discord\n  namespace: agnhost\nspec:\n  channel: agnhost\n  secretRef:\n    name: discord-webhook\n  type: discord\n  username: FluxCD\n</code></pre> <pre><code>---\napiVersion: notification.toolkit.fluxcd.io/v1beta3\nkind: Provider\nmetadata:\n  name: discord\n  namespace: podinfo\nspec:\n  channel: podinfo\n  secretRef:\n    name: discord-webhook\n  type: discord\n  username: FluxCD\n</code></pre> <p>```</p>"},{"location":"FluxCD/FluxCD_demonstration_par_l_exemple/#configuration-des-alertes","title":"Configuration des alertes","text":"codeagnhostpodinfo <pre><code>export LOCAL_GITHUB_REPOS=\"${HOME}/code/github\"\n\ncd ${LOCAL_GITHUB_REPOS}/k8s-kind-fluxcd\n\nflux create alert discord \\\n  --event-severity=info \\\n  --event-source='GitRepository/*,Kustomization/*,ImageRepository/*,ImagePolicy/*,HelmRepository/*,HelmRelease/*' \\\n  --provider-ref=discord \\\n  --namespace=agnhost \\\n  --export &gt; apps/agnhost/discord.alert.yaml\n\n\nflux create alert discord \\\n  --event-severity=info \\\n  --event-source='GitRepository/*,Kustomization/*,ImageRepository/*,ImagePolicy/*,HelmRepository/*,HelmRelease/*' \\\n  --provider-ref=discord \\\n  --namespace=podinfo \\\n  --export &gt; apps/podinfo/discord.alert.yaml\n</code></pre> <pre><code>---\napiVersion: notification.toolkit.fluxcd.io/v1beta3\nkind: Alert\nmetadata:\n  name: discord\n  namespace: agnhost\nspec:\n  eventSeverity: info\n  eventSources:\n  - kind: GitRepository\n    name: '*'\n  - kind: Kustomization\n    name: '*'\n  - kind: ImageRepository\n    name: '*'\n  - kind: ImagePolicy\n    name: '*'\n  - kind: HelmRepository\n    name: '*'\n  - kind: HelmRelease\n    name: '*'\n  providerRef:\n    name: discord\n</code></pre> <pre><code>---\napiVersion: notification.toolkit.fluxcd.io/v1beta3\nkind: Alert\nmetadata:\n  name: discord\n  namespace: podinfo\nspec:\n  eventSeverity: info\n  eventSources:\n  - kind: GitRepository\n    name: '*'\n  - kind: Kustomization\n    name: '*'\n  - kind: ImageRepository\n    name: '*'\n  - kind: ImagePolicy\n    name: '*'\n  - kind: HelmRepository\n    name: '*'\n  - kind: HelmRelease\n    name: '*'\n  providerRef:\n    name: discord\n</code></pre>"},{"location":"FluxCD/FluxCD_demonstration_par_l_exemple/#activation-des-alertes-et-notifications","title":"Activation des alertes et notifications","text":"<p>Poussons nos modifications dans notre d\u00e9p\u00f4t GitHub :</p> <pre><code>export LOCAL_GITHUB_REPOS=\"${HOME}/code/github\"\n\ncd ${LOCAL_GITHUB_REPOS}/k8s-kind-fluxcd\n\ngit add .\ngit commit -m \"'Setting up Discord alerting for agnhost and podinfo applications.\"\ngit push\n\nflux reconcile kustomization flux-system --with-source\n</code></pre> <p>V\u00e9rifions la bonne cr\u00e9ation des alertes et notification providers :</p> codeoutput <pre><code>kubectl get providers,alerts -A\n</code></pre> <pre><code>NAMESPACE   NAME                                              AGE\nagnhost     provider.notification.toolkit.fluxcd.io/discord   27s\npodinfo     provider.notification.toolkit.fluxcd.io/discord   27s\n\nNAMESPACE   NAME                                           AGE\nagnhost     alert.notification.toolkit.fluxcd.io/discord   27s\npodinfo     alert.notification.toolkit.fluxcd.io/discord   27s\n</code></pre>"},{"location":"FluxCD/FluxCD_demonstration_par_l_exemple/#tests","title":"Tests","text":""},{"location":"FluxCD/FluxCD_demonstration_par_l_exemple/#application-agnhost","title":"Application 'agnhost'","text":""},{"location":"FluxCD/FluxCD_demonstration_par_l_exemple/#contexte","title":"Contexte","text":"<p>Pour rappel, l'application 'agnhost' est d\u00e9ploy\u00e9e sur notre cluster Kubernetes \u00e0 la version <code>2.10</code> :</p> codeoutput <pre><code>kubectl -n agnhost get deployment agnhost -o jsonpath='{.spec.template.spec.containers[].image}'\n</code></pre> <pre><code>registry.k8s.io/e2e-test-images/agnhost:2.10%\n</code></pre> <p>L'ImageRepository propose pourtant des images beaucoup r\u00e9centes de l'application :</p> codeoutput <pre><code>kubectl -n agnhost get imagerepository agnhost -o jsonpath='{.status.lastScanResult.latestTags}'  | jq -r\n</code></pre> <pre><code>[\n  \"2.9\",\n  \"2.57\",\n  \"2.56\",\n  \"2.55\",\n  \"2.54\",\n  \"2.53\",\n  \"2.52\",\n  \"2.51\",\n  \"2.50\",\n  \"2.48\"\n]\n</code></pre> <p>L'ImagePolicy que nous avons mise en place nous en donne la raison :</p> codeoutput <pre><code>kubectl -n agnhost get imagepolicy -o jsonpath='{.items[].spec.policy}'\n</code></pre> <pre><code>{\"numerical\":{\"order\":\"desc\"}}%\n</code></pre> <p>Nous avons express\u00e9ment demand\u00e9 \u00e0 Flux qu'il choisisse le tag (au format 'x.yz') le plus petit ('order: desc').</p>"},{"location":"FluxCD/FluxCD_demonstration_par_l_exemple/#test","title":"Test","text":"<p>Modifions notre ImagePolicy de sorte qu'il choisisse d\u00e9sormais l'image au tag num\u00e9riquement le plus \u00e9lev\u00e9 (pour ne pas dire l'image la plus r\u00e9cente) et for\u00e7ons la r\u00e9conciliation :</p> <pre><code>export LOCAL_GITHUB_REPOS=\"${HOME}/code/github\"\n\ncd ${LOCAL_GITHUB_REPOS}/k8s-kind-fluxcd\n\ngsed -i 's/order: desc/order: asc/' apps/foo/agnhost.imagepolicy.yaml\n\ngit add .\ngit commit -m \"Modifying 'agnhost' image policy to get the latest image.\"\ngit push\n\nflux reconcile kustomization flux-system --with-source\n```\n</code></pre> <p>Surveillons nos pods pendant la r\u00e9conciliation :</p> codeoutput <pre><code>kubectl -n agnhost get pods -w\n</code></pre> <pre><code>NAME                       READY   STATUS    RESTARTS   AGE\nagnhost-7cd476ffd6-l7v4m   1/1     Running   0          12m\nagnhost-86d684f6bf-qsccn   0/1     Pending   0          0s\nagnhost-86d684f6bf-qsccn   0/1     Pending   0          0s\nagnhost-86d684f6bf-qsccn   0/1     ContainerCreating   0          0s\nagnhost-86d684f6bf-qsccn   1/1     Running             0          1s\nagnhost-7cd476ffd6-l7v4m   1/1     Terminating         0          14m\nagnhost-7cd476ffd6-l7v4m   0/1     Completed           0          14m\nagnhost-7cd476ffd6-l7v4m   0/1     Completed           0          14m\nagnhost-7cd476ffd6-l7v4m   0/1     Completed           0          14m\n</code></pre> <p>Nous voyons le pod 'agnhost-7cd476ffd6-l7v4m' se faire remplacer par un nouveau pod 'agnhost-7cd476ffd6-l7v4m'.</p> <p>Pendant l'op\u00e9ration, le client Discord sonne et affiche des alertes :</p> <p></p>"},{"location":"FluxCD/FluxCD_demonstration_par_l_exemple/#application-podinfo","title":"Application 'podinfo'","text":""},{"location":"FluxCD/FluxCD_demonstration_par_l_exemple/#contexte_1","title":"Contexte","text":"<p>L'application 'podinfo' est d\u00e9ploy\u00e9e \u00e0 la version <code>5.2.1</code> :</p> codeoutput <pre><code>kubectl -n podinfo get helmrelease podinfo\n</code></pre> <pre><code>NAME      AGE     READY   STATUS\npodinfo   6h42m   True    Helm upgrade succeeded for release podinfo/podinfo.v3 with chart podinfo@5.2.1\n</code></pre> <p>Nous savons qu'il ne s'agit pas de la version la plus r\u00e9cente :</p> codeoutput <pre><code>export GITHUB_USER=papaFrancky\nexport GITHUB_TOKEN=&lt;my_github_personal_access_token&gt;\n\necho ${GITHUB_TOKEN} | docker login ghcr.io -u ${GITHUB_USER} --password-stdin\n\nhelm show chart oci://ghcr.io/stefanprodan/charts/podinfo\n</code></pre> <pre><code>Pulled: ghcr.io/stefanprodan/charts/podinfo:6.9.2\nDigest: sha256:971fef0d04d5b3d03d035701dad59411ea0f60e28d16190f02469ddfe5587588\napiVersion: v1\nappVersion: 6.9.2\ndescription: Podinfo Helm chart for Kubernetes\nhome: https://github.com/stefanprodan/podinfo\nkubeVersion: '&gt;=1.23.0-0'\nmaintainers:\n- email: stefanprodan@users.noreply.github.com\n  name: stefanprodan\nname: podinfo\nsources:\n- https://github.com/stefanprodan/podinfo\nversion: 6.9.2\n</code></pre> <p>La version la plus r\u00e9cente de 'podinfo' est tagu\u00e9e <code>6.9.2</code></p> <p>Nous avions pr\u00e9cis\u00e9 dans la d\u00e9finition de la HelmRelease que nous ne voulions pas de versions &gt;= 6.0.0 :</p> codeoutput <pre><code>kubectl -n podinfo get helmrelease -o jsonpath='{.items[].spec.chart.spec.version}'\n</code></pre> <pre><code>&lt;6.0.0%\n</code></pre>"},{"location":"FluxCD/FluxCD_demonstration_par_l_exemple/#test_1","title":"Test","text":"code <pre><code>export LOCAL_GITHUB_REPOS=\"${HOME}/code/github\"\n\ncd ${LOCAL_GITHUB_REPOS}/k8s-kind-apps\n\ngsed -i \"s/version: &lt;6.0.0/version: \\'\\*\\'/\" podinfo/podinfo.helmrelease.yaml\n\ngit add .\ngit commit -m 'Modified podinfo HelmRelease to get the latest app version.'\ngit push\n\nflux -n podinfo reconcile kustomization podinfo --with-source\n</code></pre> <p>Surveillons les pods de l'application pendant la r\u00e9concliation :</p> codeoutput <pre><code>kubectl -n podinfo get po -w\n</code></pre> <pre><code>NAME                     READY   STATUS    RESTARTS       AGE\npodinfo-84865fdc-hfdsp   1/1     Running   1 (122m ago)   5h28m\npodinfo-84865fdc-s9vnq   1/1     Running   1 (122m ago)   5h28m\npodinfo-db75857bd-vdl6k   0/1     Pending   0              0s\npodinfo-84865fdc-hfdsp    1/1     Terminating   1 (124m ago)   5h31m\npodinfo-db75857bd-vdl6k   0/1     Pending       0              0s\npodinfo-db75857bd-vdl6k   0/1     ContainerCreating   0              0s\npodinfo-db75857bd-9c6pq   0/1     Pending             0              0s\npodinfo-db75857bd-9c6pq   0/1     Pending             0              0s\npodinfo-db75857bd-9c6pq   0/1     ContainerCreating   0              0s\npodinfo-db75857bd-vdl6k   0/1     Running             0              2s\npodinfo-db75857bd-9c6pq   0/1     Running             0              3s\npodinfo-db75857bd-vdl6k   1/1     Running             0              3s\npodinfo-84865fdc-s9vnq    1/1     Terminating         1 (124m ago)   5h31m\npodinfo-84865fdc-hfdsp    0/1     Completed           1 (124m ago)   5h31m\npodinfo-84865fdc-hfdsp    0/1     Completed           1 (124m ago)   5h31m\npodinfo-84865fdc-hfdsp    0/1     Completed           1 (124m ago)   5h31m\npodinfo-db75857bd-9c6pq   1/1     Running             0              4s\npodinfo-84865fdc-s9vnq    0/1     Completed           1 (124m ago)   5h31m\npodinfo-84865fdc-s9vnq    0/1     Completed           1 (124m ago)   5h31m\npodinfo-84865fdc-s9vnq    0/1     Completed           1 (124m ago)   5h31m\n</code></pre> <p>Les pods <code>podinfo-84865fdc-hfdsp</code> et <code>podinfo-84865fdc-s9vnq</code> sont remplac\u00e9s par deux nouveaux pods <code>podinfo-db75857bd-vdl6k</code> et <code>podinfo-db75857bd-vdl6k</code>.</p> <p>V\u00e9rifions la version de notre application maintenant : </p> codeoutput <pre><code>kubectl -n podinfo get helmrelease podinfo\n</code></pre> <pre><code>NAME      AGE     READY   STATUS\npodinfo   7h18m   True    Helm upgrade succeeded for release podinfo/podinfo.v4 with chart podinfo@6.9.2\n</code></pre> <p>Nous sommes d\u00e9sormais bien \u00e0 la version <code>6.9.2</code> de l'application 'podinfo', soit la plus r\u00e9cente.</p> <p>Discord nous alerte dans le m\u00eame temps de changements orchestr\u00e9s sur l'application 'podinfo' : </p> <p></p> <p>Nous avons valid\u00e9 le bon fonctionnement de la remont\u00e9e d'alertes via le client Discord pour nos applications 'agnhost' et 'podinfo' </p> <p></p>"},{"location":"GKE/cluster_GKE_SSL_load-balancer/","title":"Cluster GKE en mode autopilot, load-balancer HTTPS et application 'stateless'","text":"<p>Pour me former \u00e0 Google Kubernetes Engine (GKE), j'ai suivi le tutoriel suivant mais avec beaucoup de difficult\u00e9 car il comporte des erreurs. Sur la base du tutoriel de Google, j'ai document\u00e9 la mani\u00e8re dont je m'y suis pris pour arriver \u00e0 d\u00e9ployer un cluster GKE avec un load-balancer expos\u00e9 \u00e0 internet permettant d'acc\u00e9der en HTTPS \u00e0 une petite application 'stateless'.</p> <pre><code>  # Tutoriel Google : Configurer la mise en r\u00e9seau pour un cluster de production de base\n  https://cloud.google.com/kubernetes-engine/docs/tutorials/configure-networking?hl=en.\n</code></pre>"},{"location":"GKE/cluster_GKE_SSL_load-balancer/#pre-requis","title":"Pr\u00e9-requis","text":"<ul> <li>Disposer d'un compte Google et acc\u00e9der \u00e0 la console GCP ( https://console.cloud.console.com )</li> <li>Avoir cr\u00e9\u00e9 un projet (renseigner son ID dans la variable ${PROJECT_ID} ci-apr\u00e8s)</li> <li>Lancer CloudShell (les commandes qui suivent seront ex\u00e9cut\u00e9es dans CloudShell)</li> </ul>"},{"location":"GKE/cluster_GKE_SSL_load-balancer/#variables","title":"Variables","text":"<ul> <li>Se connecter \u00e0 la console GCP</li> <li> <p>S\u00e9lectionner le projet souhait\u00e9 et dont l'ID est renseign\u00e9 dans la variable ${PROJECT_ID} ci-dessous.</p> <p>PROJECT_ID=\"project-230902\"                   # ID du projet GCP   REGION=\"europe-west4\"                         # Netherlands   CLUSTER_NAME=\"sandbox-cluster\"                # nom du cluster   RESERVED_DOMAIN_NAME=\"vanille-fraise.net\"     # Nom de domaine r\u00e9serv\u00e9 aupr\u00e8s d'un 'registrar'   APPLICATION_NAME=\"hello\"                      # Nom de l'application</p> </li> </ul>"},{"location":"GKE/cluster_GKE_SSL_load-balancer/#configurer-votre-environnement","title":"Configurer votre environnement","text":"<pre><code>  rm -rf ~/.kube                                    # nettoyage\n  rm -rf .terraform* terraform.tfstate              # nettoyage\n\n  # configuration de kubectl\n  gcloud config set project ${PROJECT_ID}\n  gcloud services enable compute.googleapis.com     # n\u00e9cessaire pour d\u00e9finir la r\u00e9gion (la commande prend du temps)\n  gcloud config set compute/region ${REGION}\n\n  gcloud config list\n</code></pre>"},{"location":"GKE/cluster_GKE_SSL_load-balancer/#activer-les-api-google-kubernetes-engine-cloud-dns","title":"Activer les API Google Kubernetes Engine, Cloud DNS.","text":"<pre><code>  gcloud services enable container.googleapis.com\n  gcloud services enable dns.googleapis.com\n  gcloud services list --enabled --project ${PROJECT_ID}\n</code></pre>"},{"location":"GKE/cluster_GKE_SSL_load-balancer/#recuperation-du-code-depuis-github","title":"R\u00e9cup\u00e9ration du code depuis GitHub","text":"<pre><code>  git clone https://github.com/papaFrancky/cluster_GKE_SSL_load-balancer\n  cd cluster_GKE_SSL_load-balancer\n</code></pre> <p>( Via la WUI GitHub : https://github.com/papaFrancky/cluster_GKE_SSL_load-balancer.git )</p>"},{"location":"GKE/cluster_GKE_SSL_load-balancer/#deploiement-du-cluster-gke","title":"D\u00e9ploiement du cluster GKE","text":"<p>( doc. utile :  HashiCorp - provider Google https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/container_cluster)</p> <p>Nous allons surcharger la valeur par d\u00e9faut de certaines variables en \u00e9crivant un fichier '.auto.tfvars' :</p> <pre><code>  cat &lt;&lt; EOF &gt; cluster.auto.tfvars\n  region               = \"${REGION}\"\n  reserved_domain_name = \"${RESERVED_DOMAIN_NAME}\"\n  cluster_name         = \"${CLUSTER_NAME}\"\n  application_name     = \"${APPLICATION_NAME}\"\n  EOF\n</code></pre> <p>Nous pouvons ensuite ex\u00e9cuter le code Terraform :</p> <pre><code>  terraform init\n  terraform plan\n  terraform apply --auto-approve\n</code></pre> <p>Exemple de sortie : </p> <pre><code>  Apply complete! Resources: 5 added, 0 changed, 0 destroyed.\n\n  Outputs:\n\n  cluster_name = \"sandbox-cluster\"\n  dns_zone_name_servers = tolist([\n    \"ns-cloud-c1.googledomains.com.\",\n    \"ns-cloud-c2.googledomains.com.\",\n    \"ns-cloud-c3.googledomains.com.\",\n    \"ns-cloud-c4.googledomains.com.\",\n  ])\n  domain = \"hello.vanille-fraise.net\"\n  region = \"europe-west4\"\n</code></pre>"},{"location":"GKE/cluster_GKE_SSL_load-balancer/#verification-du-deploiement","title":"V\u00e9rification du d\u00e9ploiement","text":""},{"location":"GKE/cluster_GKE_SSL_load-balancer/#dns","title":"DNS","text":"<p>Via la console :</p> <pre><code>  https://console.cloud.google.com/net-services/dns/zones/${APPLICATION_NAME}/details?organizationId=0&amp;project=${PROJECT_ID}\n</code></pre>"},{"location":"GKE/cluster_GKE_SSL_load-balancer/#cluster-kubernetes","title":"Cluster Kubernetes","text":"<p>Via la console :</p> <pre><code>  https://console.cloud.google.com/kubernetes/list/overview?organizationId=0&amp;project=${PROJECT_ID}\n</code></pre>"},{"location":"GKE/cluster_GKE_SSL_load-balancer/#ip-statique-publique","title":"IP statique publique","text":"<p>Via la console :</p> <pre><code>  https://console.cloud.google.com/networking/addresses/list?organizationId=0&amp;project=${PROJECT_ID}\n</code></pre>"},{"location":"GKE/cluster_GKE_SSL_load-balancer/#modification-des-name-servers-ns-du-domaine-dns-aupres-du-registrar","title":"Modification des 'Name Servers (NS) du domaine DNS aupr\u00e8s du 'registrar'","text":"<p>M\u00eame si vous avez r\u00e9serv\u00e9 votre nom de domaine chez le 'registrar' Google Domains, vous devez modifier directement aupr\u00e8s de votre 'registrar' les serveurs de noms (NS) de votre zone et les remplacer par ceux list\u00e9s en sortie du 'terraform apply'. Dans notre exemple :</p> <pre><code>  ns-cloud-c1.googledomains.com\n  ns-cloud-c2.googledomains.com\n  ns-cloud-c3.googledomains.com\n  ns-cloud-c4.googledomains.com\n</code></pre> <p>Dans mon cas, j'ai r\u00e9serv\u00e9 mon nom de domaine aupr\u00e8s du registrar Google Domains :</p> <pre><code>  https://domains.google.com/registrar/${RESERVED_DOMAIN_NAME}/dns \n      -&gt; cliquer sur 'Serveurs de noms personnalis\u00e9s'\n      -&gt; puis cliquer sur 'G\u00e9rer les serveurs de noms'\n      -&gt; Renseigner les serveurs de noms issus de la sortir de la commande 'terraform apply' (dns_zone_name_servers)\n</code></pre> <p>V\u00e9rifier la r\u00e9solution de noms :</p> <pre><code>  dig ${APPLICATION_NAME}.${RESERVED_DOMAIN_NAME}\n      ;; ANSWER SECTION:\n      hello.vanille-fraise.net. 300 IN  A       34.36.92.137\n\n  dig www.${APPLICATION_NAME}.${RESERVED_DOMAIN_NAME}\n      ;; ANSWER SECTION:\n      www.hello.vanille-fraise.net. 300 IN CNAME hello.vanille-fraise.net.\n      hello.vanille-fraise.net. 300 IN  A       34.36.92.137\n</code></pre> <p>V\u00e9rifier \u00e9galement dans CloudDNS dans la zone '${APPLICATION_NAME}' les entr\u00e9es de type NS et SOA et s'assurer qu'elles correspondent bien aux serveurs de noms attendus :</p> <pre><code>    https://console.cloud.google.com/net-services/dns/zones/${APPLICATION_NAME}/details?organizationId=0&amp;project=${PROJECT_ID}\n</code></pre>"},{"location":"GKE/cluster_GKE_SSL_load-balancer/#deploiement-du-load-balancer-applicatif-externe","title":"D\u00e9ploiement du Load-Balancer applicatif externe","text":"<p>( ManagedCertificate, FrontendConfig, Deployment, Service et Ingress)</p>"},{"location":"GKE/cluster_GKE_SSL_load-balancer/#configuration-de-kubectl-pour-se-connecter-au-cluster-recemment-cree","title":"Configuration de kubectl pour se connecter au cluster r\u00e9cemment cr\u00e9\u00e9","text":"<pre><code>  gcloud container clusters get-credentials ${CLUSTER_NAME} --region ${REGION} --project=${PROJECT_ID}\n  -&gt; g\u00e9n\u00e8re le fichier de configuration de kubectl : ~/.kube/config\n</code></pre>"},{"location":"GKE/cluster_GKE_SSL_load-balancer/#verifier-que-nous-utilisons-le-bon-contexte-kubernetes","title":"V\u00e9rifier que nous utilisons le bon contexte kubernetes :","text":"<pre><code>  kubectl config view | grep current-context \n  -&gt; current-context: gke_${PROJECT_ID}_${REGION}_${CLUSTER_NAME}\n</code></pre>"},{"location":"GKE/cluster_GKE_SSL_load-balancer/#verifier-que-nous-pouvons-bien-nous-connecter-au-cluster","title":"V\u00e9rifier que nous pouvons bien nous connecter au cluster","text":"<pre><code>  kubectl get pods -A \n  -&gt; retourne la liste de tous les pods du cluster\n</code></pre>"},{"location":"GKE/cluster_GKE_SSL_load-balancer/#creation-du-certificat-ssl-du-load-balancer-de-lingress","title":"Cr\u00e9ation du certificat SSL, du load-balancer, de l'ingress","text":"<pre><code>  kubectl apply -f kubernetes-manifests.yaml\n  ( note : la validation du certificat TLS prend 30 minutes environ )\n</code></pre>"},{"location":"GKE/cluster_GKE_SSL_load-balancer/#verification-de-la-creation-de-lingress","title":"V\u00e9rification de la cr\u00e9ation de l'ingress","text":"<pre><code>  kubectl describe ingress frontend\n</code></pre>"},{"location":"GKE/cluster_GKE_SSL_load-balancer/#verification-du-provisioning-du-certificat-tls-peut-prendre-30-minutes","title":"V\u00e9rification du provisioning du certificat TLS (peut prendre 30 minutes)","text":""},{"location":"GKE/cluster_GKE_SSL_load-balancer/#avec-kubectl","title":"avec kubectl","text":"<pre><code>  kubectl get      managedcertificates.networking.gke.io frontend-managed-cert\n  kubectl describe managedcertificates.networking.gke.io frontend-managed-cert\n</code></pre>"},{"location":"GKE/cluster_GKE_SSL_load-balancer/#via-la-console-security-certificate-manager","title":"via la console : Security / Certificate Manager","text":"<pre><code>  https://console.cloud.google.com/apis/library/certificatemanager.googleapis.com?project=${PROJECT_ID}\n  -&gt; activer Secret Manager API\n  -&gt; cliquer sur 'CLASSIC CERTIFICATES' -&gt; on voit le certificat en status : provisioning\n\n  ( note : activer l'API 'Certificate Manager' pour acc\u00e9der au certificat )\n</code></pre>"},{"location":"GKE/cluster_GKE_SSL_load-balancer/#documentation-de-troubleshooting-google-managed-ssl-certificates","title":"Documentation de troubleshooting Google-managed SSL certificates :","text":"<pre><code>  https://cloud.google.com/load-balancing/docs/ssl-certificates/troubleshooting?hl=en\n  https://cloud.google.com/load-balancing/docs/ssl-certificates/google-managed-certs?hl=en#caa\n  https://cloud.google.com/kubernetes-engine/docs/how-to/managed-certs?hl=en\n</code></pre>"},{"location":"GKE/cluster_GKE_SSL_load-balancer/#verification-de-ladresse-ip-publique-reservee","title":"V\u00e9rification de l'adresse IP publique r\u00e9serv\u00e9e","text":"<pre><code>  gcloud compute addresses describe ${APPLICATION_NAME} --global\n</code></pre>"},{"location":"GKE/cluster_GKE_SSL_load-balancer/#verification-de-lingress","title":"V\u00e9rification de l'Ingress","text":"<pre><code>  k get ingress\n\n  NAME       CLASS    HOSTS   ADDRESS          PORTS   AGE\n  frontend   &lt;none&gt;   *       34.117.157.187   80      5m41s\n</code></pre> <p>L'IP du load-balancer est ici 34.117.157.187.</p>"},{"location":"GKE/cluster_GKE_SSL_load-balancer/#test-application","title":"Test application","text":"<pre><code>  curl -Lv https://${APPLICATION_NAME}.${RESERVED_DOMAIN_NAME}\n  curl -Lv https://www.${APPLICATION_NAME}.${RESERVED_DOMAIN_NAME}\n</code></pre> <p>En acc\u00e9dant plusieurs fois \u00e0 l'URL, on constate que le 'hostname' (ie. le pod) change. </p>"},{"location":"GKE/cluster_GKE_SSL_load-balancer/#nettoyage","title":"Nettoyage","text":"<pre><code>  gcloud projects delete ${PROJECT_ID} --quiet\n  gcloud projects list\n</code></pre> <p>ou :</p> <pre><code>kubectl delete -f kubernetes-manifests.yaml\nterraform destroy --auto-approve\n</code></pre>"},{"location":"GKE/cluster_GKE_SSL_load-balancer/#divers","title":"Divers","text":"<p>https://registry.terraform.io/providers/hashicorp/google/latest/docs/guides/using_gke_with_terraform</p> <p>Debug Cloud DNS :</p> <pre><code>  gcloud dns managed-zones describe ${APPLICATION_NAME}\n  gcloud dns record-sets list --zone ${APPLICATION_NAME}\n  gcloud dns record-sets list --zone=${APPLICATION_NAME} --name ${APPLICATION_NAME}.${RESERVED_DOMAIN_NAME}\n</code></pre>"},{"location":"GKE/cluster_GKE_SSL_load-balancer/#next-step","title":"Next step","text":"<p>https://kubernetes.io/docs/tutorials/stateful-application/mysql-wordpress-persistent-volume/</p>"},{"location":"GKE/cluster_GKE_Wordpress_CloudSQL/","title":"Deploy WordPress on GKE with Persistent Disk and Cloud SQL","text":"<p>Doc de r\u00e9f\u00e9rence : https://cloud.google.com/kubernetes-engine/docs/tutorials/persistent-disk?hl=en</p>"},{"location":"GKE/cluster_GKE_Wordpress_CloudSQL/#variables","title":"Variables","text":"<pre><code>export PROJECT_ID=\"project-230903\"\nexport REGION=\"europe-west4\"\nexport CLUSTER_NAME=\"sandbox-cluster\"\nexport INSTANCE_NAME=\"mysql-wordpress-instance\"\nexport SA_NAME=\"cloudsql-proxy\"\n</code></pre>"},{"location":"GKE/cluster_GKE_Wordpress_CloudSQL/#actions-prealables","title":"Actions pr\u00e9alables","text":""},{"location":"GKE/cluster_GKE_Wordpress_CloudSQL/#nettoyage-des-traces-des-workshops-precedents","title":"Nettoyage des traces des workshops pr\u00e9c\u00e9dents","text":"<pre><code>rm -rf ~/.kube\n</code></pre>"},{"location":"GKE/cluster_GKE_Wordpress_CloudSQL/#configuration-de-lenvironnement","title":"Configuration de l'environnement","text":"<pre><code>gcloud config set project ${PROJECT_ID}\ngcloud services enable compute.googleapis.com container.googleapis.com sqladmin.googleapis.com\ngcloud config set compute/region ${REGION}\ngcloud config list\n</code></pre>"},{"location":"GKE/cluster_GKE_Wordpress_CloudSQL/#recuperation-des-manifests-kubernetes","title":"R\u00e9cup\u00e9ration des manifests Kubernetes","text":"<pre><code>git clone https://github.com/papaFrancky/cluster_GKE_Wordpress_CloudSQL\ncd cluster_GKE_Wordpress_CloudSQL\nWORKING_DIR=$(pwd)\n</code></pre>"},{"location":"GKE/cluster_GKE_Wordpress_CloudSQL/#creation-du-cluster-gke","title":"Cr\u00e9ation du cluster GKE","text":"<pre><code>gcloud container clusters create-auto ${CLUSTER_NAME}\n</code></pre> <p>Une fois le cluster cr\u00e9\u00e9, on g\u00e9n\u00e8re les informations de connexion \u00e0 ce dernier :</p> <pre><code>gcloud container clusters get-credentials ${CLUSTER_NAME} --region ${REGION}\n</code></pre> <p>-&gt; La commande cr\u00e9\u00e9 le fichier ~/.kube/config</p>"},{"location":"GKE/cluster_GKE_Wordpress_CloudSQL/#creation-dun-persistent-volume-pv-et-dun-persistent-volume-claim-pvc","title":"Cr\u00e9ation d'un Persistent Volume (PV) et d'un Persistent Volume Claim (PVC)","text":"<pre><code>kubectl apply -f ${WORKING_DIR}/wordpress.persistent_volume_claim.yaml\nkubectl get persistentvolumeclaim\n</code></pre>"},{"location":"GKE/cluster_GKE_Wordpress_CloudSQL/#creation-dune-instance-mysql-dans-cloud-sql","title":"Cr\u00e9ation d'une instance MySQL dans Cloud SQL","text":"<pre><code>gcloud sql instances create ${INSTANCE_NAME} --region ${REGION}\n</code></pre>"},{"location":"GKE/cluster_GKE_Wordpress_CloudSQL/#creation-de-la-base-de-donnees-pour-wordpress","title":"Cr\u00e9ation de la base de donn\u00e9es pour WordPress","text":"<pre><code>export INSTANCE_CONNECTION_NAME=$( gcloud sql instances describe ${INSTANCE_NAME} --format='value(connectionName)' )\ngcloud sql databases create wordpress --instance ${INSTANCE_NAME}\n\nCreating Cloud SQL database...done.                            \nCreated database [wordpress].\ninstance: mysql-wordpress-instance\nname: wordpress\nproject: project-230903\n</code></pre>"},{"location":"GKE/cluster_GKE_Wordpress_CloudSQL/#creation-du-db-account-wordpress-avec-mot-de-passe-pour-sauthentifier-a-linstance","title":"Cr\u00e9ation du DB account 'wordpress' avec mot de passe pour s'authentifier \u00e0 l'instance","text":"<pre><code>CLOUD_SQL_PASSWORD=$(openssl rand -base64 18)\necho ${CLOUD_SQL_PASSWORD}          # exemple: H3nzTp+bBmZ/3xpVswovAPG+\ngcloud sql users create wordpress --host=% --instance ${INSTANCE_NAME}  --password ${CLOUD_SQL_PASSWORD}\n</code></pre>"},{"location":"GKE/cluster_GKE_Wordpress_CloudSQL/#deploiement-de-wordpress","title":"D\u00e9ploiement de WordPress","text":""},{"location":"GKE/cluster_GKE_Wordpress_CloudSQL/#service-account-et-secrets","title":"Service-account et secrets","text":""},{"location":"GKE/cluster_GKE_Wordpress_CloudSQL/#creation-du-service-account","title":"Cr\u00e9ation du service-account","text":"<pre><code>gcloud iam service-accounts create ${SA_NAME} --display-name ${SA_NAME}\n</code></pre>"},{"location":"GKE/cluster_GKE_Wordpress_CloudSQL/#ajout-du-role-cloudsqlclient-a-uservice-account","title":"Ajout du r\u00f4le cloudsql.client a uservice account","text":"<pre><code>SA_EMAIL=$( gcloud iam service-accounts list --filter=displayName:$SA_NAME --format='value(email)' )\n# exemple: cloudsql-proxy@project-230902-2.iam.gserviceaccount.com\ngcloud projects add-iam-policy-binding ${PROJECT_ID} --role roles/cloudsql.client --member serviceAccount:${SA_EMAIL}\n</code></pre>"},{"location":"GKE/cluster_GKE_Wordpress_CloudSQL/#creation-dune-cle-pour-le-service-account","title":"Cr\u00e9ation d'une cl\u00e9 pour le service-account","text":"<pre><code>gcloud iam service-accounts keys create ${WORKING_DIR}/key.json --iam-account ${SA_EMAIL}\n\ncat key.json \n{\n  \"type\": \"service_account\",\n  \"project_id\": \"project-230903\",\n  \"private_key_id\": \"8b541bab674bf3f252b2337a6fff83fe496dc628\",\n  \"private_key\": \"-----BEGIN PRIVATE KEY-----\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCiU+nwSgzLmOA4\\nDY8AX1DtaWhrW4iqtNEOYHArdLmOZiS/Baknd4PiC/u3Y9Cn+j7vXg2rnjA4AvJM\\nc1tSf4BJMsGT2RYbYluO/s0mysPMSz7/YLvWdATHDZEl9fPFJY6O8cEBg8kWwCal\\nSo3ZqFHyPznf1E2QBcZFrLXDAxBjBDzTUOgxG1GiIh2v/oSYxU8PppLitUZ7rlV\\nDrv2gz4PXaE9RLgiEPLAZ3HLtIM4KE6jLt1j6MVDKl7vzdONLjgWE7t+dZRzWPqo\\nx9EccR4P9dYq9c4ZstUI6ZjMWg3JVD3JbnciEUuePX+WCK3usSKtDCsyKAgBHo/w\\nwOEKej2ZAgMBAAECggEAFvEyMpZcqZfRNMrhx6UxDGTl76pvreTBWT1TDSlBonkY\\nP3E+34eaOaQE7v3p+xu4sl8CIpvIZ9ouwZRaN1Yy3OWSC2HWqIcltpeXiiCFPMXz\\nwOc8lQovtKxbs9hHnDj7JYPQifTEwnTk4V6gnr8V2d2KwfJBBhZy190ZkVbJBZ1V\\nld4BVlSPDZVU/NIOLgke/ZG+d1qxR/NfRK7a/IrbZeiqyQuuif4y37o+bG+TT5Hy\\nL4okx8aoy+8dyhxe6CNRfL4nWVCJaYZam0SCEPY4rb5C9i7EUdGQj+6fWg1l82BJ\\nOa9WzPwotdDHkS7KoiZ0IzrYToWxjeTcyOGqLJvDKwKBgQDMqGHc+C6irnsIsHBu\\n6dUkAsPBsTqwKASGogjeHoC2vWwALNJk/59gB7c0TvYUc2HaugSRqAfcZGaZOMBI\\n6g4DJuFtBMyFoYXF2j8GU17VQ3APzpFQC+s4tf2TpW9srulHIBErMmgPq0Mpig6k\\nla6d+EwMlvcYXa71LI9MCmOpwwKBgQDLDP+7UxLCKA7gNHCUrc98MPP5VgAkNWdE\\n7N3T6R9QyEaK5HYKLaiCCzMMV2QFWiAo4w/WTQXcLn1prFMl781X9tkysxiSgDAY\\nVgAcg0bVsziog1erC/HqFZ1zSTHokX2zEqafGtHGMl3dbyWdCOJIvViyKDc4wNFb\\nO6JUv1xpcwKBgEdSOuCd4OqysYCpTwR40Rsbjn3AIPZPlKI71ww9xw4AQZCmIO4\\nDZuStMbW6a0Q1L4761GzZCHrH1IwU9pVLtLsXsz2SiwbsRnVR/d1YGwj10665ysl\\nLDEUQy2MDruqbQNranBKXbdwMLSuNxImU7cbi60rgysLougwQjP2vuqvAoGABe7k\\nThn4U1oOTTjbDU0i4fMgPenoaSZyVQ5C0R1fv+GKRia02ElLQjmHjVXEY2+lvuwb\\nm1x2zl9BZOQXLeWa73YUFKotDqLWRO/GYw7m8mfrzTfS+02bWuiRSsfXTdbH+9s\\nlPuYo5z3JzBHPhZzXkLCI7qPGoZv16WfcbCBx8cCgYEArU7693uPHp/V23SWLeW8\\nPU5qW9jkiFNtAF+sErLuFWtmxxz1uoEmRb8YvLPzZkyeI5v/wu7PD0a9hG4yHaGN\\nwySgdAAF3AUgBZOPLMq4LqEGjqOT3lrT7fyjgaPcQ0civ1UKbiNVwSry+68GgFY\\n9Jox8EDrq4vjW65sIer7XdY=\\n-----END PRIVATE KEY-----\\n\",\n  \"client_email\": \"cloudsql-proxy@project-230903.iam.gserviceaccount.com\",\n  \"client_id\": \"109534044814238678605\",\n  \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n  \"token_uri\": \"https://oauth2.googleapis.com/token\",\n  \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n  \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/cloudsql-proxy%40project-230903.iam.gserviceaccount.com\",\n  \"universe_domain\": \"googleapis.com\"\n}\n</code></pre>"},{"location":"GKE/cluster_GKE_Wordpress_CloudSQL/#creation-dun-secret-kubernetes-pour-les-credentials-mysql","title":"Cr\u00e9ation d'un 'secret' kubernetes pour les credentials MySQL","text":"<pre><code>kubectl create secret generic cloudsql-db-credentials --from-literal username=wordpress --from-literal password=${CLOUD_SQL_PASSWORD}\n</code></pre>"},{"location":"GKE/cluster_GKE_Wordpress_CloudSQL/#creation-dun-secret-kubernetes-pour-pour-les-credentials-du-service-account","title":"Cr\u00e9ation d'un 'secret' kubernetes pour pour les credentials du service account","text":"<pre><code>kubectl create secret generic cloudsql-sa-credentials --from-file ${WORKING_DIR}/key.json\n</code></pre>"},{"location":"GKE/cluster_GKE_Wordpress_CloudSQL/#deploiement-de-wordpress_1","title":"D\u00e9ploiement de WordPress","text":"<pre><code>cat ${WORKING_DIR}/wordpress.deployment.yaml.template | envsubst &gt; ${WORKING_DIR}/wordpress.deployment.yaml\nkubectl create -f ${WORKING_DIR}/wordpress.deployment.yaml\nkubectl get pod -l app=wordpress --watch\n</code></pre>"},{"location":"GKE/cluster_GKE_Wordpress_CloudSQL/#exposition-du-service-wordpress","title":"Exposition du service WordPress","text":"<pre><code>kubectl create -f ${WORKING_DIR}/wordpress.service.yaml\nkubectl get svc -l app=wordpress --watch\n\nNAME        TYPE           CLUSTER-IP      EXTERNAL-IP     PORT(S)        AGE\nwordpress   LoadBalancer   34.118.227.43   34.90.114.238   80:32487/TCP   46s\n</code></pre> <p>Une fois l'EXTERNAL-IP visible, on peut se connecter au service.</p>"},{"location":"GKE/cluster_GKE_Wordpress_CloudSQL/#configuration-du-blog-wordpress","title":"Configuration du blog WordPress","text":"<pre><code>http://34.91.93.136\n</code></pre> <p>-&gt; Fin du workshop !</p>"},{"location":"GKE/cluster_GKE_Wordpress_CloudSQL/#suppression-des-ressources","title":"Suppression des ressources","text":"<pre><code>kubectl delete service wordpress\nkubectl delete deployment wordpress\nkubectl delete pvc wordpress-persistent-volume-claim\ngcloud container clusters delete ${CLUSTER_NAME}\ngcloud sql instances delete ${INSTANCE_NAME}\ngcloud projects remove-iam-policy-binding ${PROJECT_ID} --role roles/cloudsql.client --member serviceAccount:${SA_EMAIL}\ngcloud iam service-accounts delete ${SA_EMAIL}\n</code></pre>"},{"location":"GKE/cluster_GKE_Wordpress_CloudSQL/#todo","title":"Todo","text":"<ul> <li>d\u00e9ployer le cluster GKE via Terraform plut\u00f4t que via la CLI</li> <li>Exposer le service en HTTPS</li> <li>Certificat wildcard cr\u00e9\u00e9 manuellement ? Peut-\u00eatre attendre Vault</li> <li>Id\u00e9alement, configurer un Ingress capable de router vers plusieurs services</li> <li>Confier les secrets \u00e0 Vault</li> <li>Tester le d\u00e9ploiement de WordPress via un Helm Chart</li> <li>Impl\u00e9menter GitOps avec FluxCD</li> </ul>"},{"location":"GKE/cluster_GKE_Wordpress_CloudSQL/#trucs-utiles","title":"Trucs utiles","text":"<ul> <li> <p>pour ouvrir un IDE : </p> <p>https://shell.cloud.google.com/?pli=1&amp;show=ide%2Cterminal</p> </li> </ul>"},{"location":"GKE/dkt_upgrade_cluster_gke/readme/","title":"Mise \u00e0 jour des clusters Kubernetes","text":"Doc opsfi https://github.com/dktunited/finance-doc-opsfi/blob/master/docs/platforms/kubernetes/exploit/upgrade/README.md <p>Ces notes visent \u00e0 montrer par l'exemple comment mettre \u00e0 jour un cluster Kubernetes dans notre contexte professionnel.</p> <p>Pour notre exemple :</p> environnement hors production version cible de GKE 1.28.6-gke.1289000"},{"location":"GKE/dkt_upgrade_cluster_gke/readme/#login-et-contexte-kubernetes","title":"Login et contexte Kubernetes","text":"<pre><code>gcloud config list\ngcloud config set project finance-6ztj\ngcloud config set compute/region europe-west4\n\ngcloud auth login\nkubecxt gke_finance-6ztj_europe-west4_hp\n</code></pre>"},{"location":"GKE/dkt_upgrade_cluster_gke/readme/#mise-a-jour-du-control-plane","title":"Mise \u00e0 jour du control-plane","text":"<p>Nous passons par la UI pour effectuer cette mise \u00e0 jour.</p>"},{"location":"GKE/dkt_upgrade_cluster_gke/readme/#mise-a-jour-des-node-pools","title":"Mise \u00e0 jour des node-pools","text":""},{"location":"GKE/dkt_upgrade_cluster_gke/readme/#procedure","title":"Proc\u00e9dure","text":"<ul> <li>cr\u00e9ation de nouveaux node pools \u00e0 la version GKE cible;</li> <li>marquer les noeuds comme non planifiables (kubectl cordon)</li> <li>d\u00e9placer les pods vers les noeuds aux versions plus r\u00e9centes (kubectl drain)</li> </ul>"},{"location":"GKE/dkt_upgrade_cluster_gke/readme/#creation-de-nouveaux-node-pools-a-la-version-gke-cible","title":"Cr\u00e9ation de nouveaux node pools \u00e0 la version GKE cible","text":"<p>Nous disposons de 2 node-pools : |NODE POOL|NUMBER OF NODES|MACHINE TYPE| |---|---|---| |petit|3 (1 par zone)|n2-standard-4| |grand|6 (2 par zone)|n2-standard-8|</p> <p>Note - nous nous posons la question de maintenir 2 node pools diff\u00e9rents puisqu'aucune de nos applications ne sont rattach\u00e9es sp\u00e9cifiquement \u00e0 un node pool plut\u00f4t qu'un autre.</p>"},{"location":"GKE/dkt_upgrade_cluster_gke/readme/#creation-du-petit-node-pool-a-la-version-de-gke-cible","title":"Cr\u00e9ation du petit node pool \u00e0 la version de GKE cible","text":"<pre><code>CLUSTER=\"hp\"                        # nom du cluster (correspond \u00e0 l'environnement servi)\nPROJECT=\"finance-6ztj\"              # projet gcp\nREGION=\"europe-west4\"               # r\u00e9gion gcp\nNEXT_VERSION=28                     # valeur incr\u00e9ment\u00e9e par nos soins \u00e0 chaque m\u00e0j\nNODE_VERSION=\"1.28.6-gke.1289000\"   # la version cible de GKE\nNODE_TYPE=\"n2-standard-4\"           # valeur correspondant au petit node-pool \nNUM_NODES=1                         # valeur correspondant au petit node-pool (nombre de noeuds par zone)\nNUM_CPUS=4                          # d\u00e9pend du type de node retenu.\nTAGS=\"\\\"rt-default-zscaler-valpha0\\\",\\\"net-main-gkenodes\\\",\\\"net-main-gkenodesfinance-europe-west4\\\"\"\nSCOPES=\"\\\"https://www.googleapis.com/auth/devstorage.read_only\\\",\\\"https://www.googleapis.com/auth/logging.write\\\",\\\"https://www.googleapis.com/auth/monitoring\\\",\\\"https://www.googleapis.com/auth/cloud-platform\\\",\\\"https://www.googleapis.com/auth/servicecontrol\\\",\\\"https://www.googleapis.com/auth/service.management.readonly\\\",\\\"https://www.googleapis.com/auth/trace.append\\\"\"\n\n    gcloud container node-pools create \"app-${NUM_CPUS}-cpu-v${NEXT_VERSION}-200go\" \\\n        --project ${PROJECT} \\\n        --region ${REGION} \\\n        --cluster ${CLUSTER} \\\n        --node-version \"${NODE_VERSION}\" \\\n        --machine-type \"${NODE_TYPE}\" \\\n        --image-type \"COS_CONTAINERD\" \\\n        --disk-type \"pd-standard\" \\\n        --disk-size \"200\" \\\n        --node-labels type=app \\\n        --metadata disable-legacy-endpoints=true \\\n        --scopes ${SCOPES} \\\n        --num-nodes \"${NUM_NODES}\" \\\n        --no-enable-autoupgrade \\\n        --shielded-integrity-monitoring \\\n        --shielded-secure-boot \\\n        --enable-autorepair \\\n        --max-surge-upgrade 1 \\\n        --max-unavailable-upgrade 0 \\\n        --tags ${TAGS}\n</code></pre> <p>V\u00e9rification</p> <pre><code>gcloud container node-pools list --region ${REGION} --cluster ${CLUSTER}\n\n    NAME                   MACHINE_TYPE   DISK_SIZE_GB  NODE_VERSION\n    app-4-cpu-v27-9-200go  n2-standard-4  200           1.27.9-gke.1092000\n    app-8-cpu-v27-9-200go  n2-standard-8  200           1.27.9-gke.1092000\n    app-4-cpu-v28-200go    n2-standard-4  200           1.28.6-gke.1289000\n</code></pre>"},{"location":"GKE/dkt_upgrade_cluster_gke/readme/#creation-du-grand-node-pool-a-la-version-de-gke-cible","title":"Cr\u00e9ation du grand node pool \u00e0 la version de GKE cible","text":"<pre><code>CLUSTER=\"hp\"                        # nom du cluster (correspond \u00e0 l'environnement servi)\nPROJECT=\"finance-6ztj\"              # projet gcp\nREGION=\"europe-west4\"               # r\u00e9gion gcp\nNEXT_VERSION=28                   # valeur incr\u00e9ment\u00e9e par nos soins \u00e0 chaque m\u00e0j\nNODE_VERSION=\"1.28.6-gke.1289000\"   # la version cible de GKE\nNODE_TYPE=\"n2-standard-8\"           # valeur correspondant au petit node-pool \nNUM_NODES=2                         # valeur correspondant au petit node-pool (nombre de noeuds par zone)\nNUM_CPUS=8                          # d\u00e9pend du type de node retenu.\nTAGS=\"\\\"rt-default-zscaler-valpha0\\\",\\\"net-main-gkenodes\\\",\\\"net-main-gkenodesfinance-europe-west4\\\"\"\nSCOPES=\"\\\"https://www.googleapis.com/auth/devstorage.read_only\\\",\\\"https://www.googleapis.com/auth/logging.write\\\",\\\"https://www.googleapis.com/auth/monitoring\\\",\\\"https://www.googleapis.com/auth/cloud-platform\\\",\\\"https://www.googleapis.com/auth/servicecontrol\\\",\\\"https://www.googleapis.com/auth/service.management.readonly\\\",\\\"https://www.googleapis.com/auth/trace.append\\\"\"\n\n    gcloud container node-pools create \"app-${NUM_CPUS}-cpu-v${NEXT_VERSION}-200go\" \\\n        --project ${PROJECT} \\\n        --region ${REGION} \\\n        --cluster ${CLUSTER} \\\n        --node-version \"${NODE_VERSION}\" \\\n        --machine-type \"${NODE_TYPE}\" \\\n        --image-type \"COS_CONTAINERD\" \\\n        --disk-type \"pd-standard\" \\\n        --disk-size \"200\" \\\n        --node-labels type=app \\\n        --metadata disable-legacy-endpoints=true \\\n        --scopes ${SCOPES} \\\n        --num-nodes \"${NUM_NODES}\" \\\n        --no-enable-autoupgrade \\\n        --shielded-integrity-monitoring \\\n        --shielded-secure-boot \\\n        --enable-autorepair \\\n        --max-surge-upgrade 1 \\\n        --max-unavailable-upgrade 0 \\\n        --tags ${TAGS}\n</code></pre> <p>V\u00e9rification</p> <pre><code>gcloud container node-pools list --region ${REGION} --cluster ${CLUSTER}\n\n    NAME                   MACHINE_TYPE   DISK_SIZE_GB  NODE_VERSION\n    app-4-cpu-v27-9-200go  n2-standard-4  200           1.27.9-gke.1092000\n    app-8-cpu-v27-9-200go  n2-standard-8  200           1.27.9-gke.1092000\n    app-4-cpu-v28-200go    n2-standard-4  200           1.28.6-gke.1289000\n    app-8-cpu-v28-200go    n2-standard-8  200           1.28.6-gke.1289000\n</code></pre> <p>Note - Si on s\u2019est tromp\u00e9 sur le nombre de noeuds par node pools, il est possible de corriger la chose comme suit :</p> <pre><code>gcloud container clusters resize ${CLUSTER} \\\n    --num-nodes ${NUM_NODES} \\\n    --node-pool app-${NUM_CPUS}-cpu-v${NEXT_VERSION}-200go \\\n    --region ${REGION}\n\nex :\ngcloud container clusters resize hp \\\n    --num-nodes 2 \\\n    --node-pool app-8-cpu-v28-200go \\\n    --region europe-west4\n</code></pre>"},{"location":"GKE/dkt_upgrade_cluster_gke/readme/#marquage-des-noeuds-comme-non-planifiables-kubectl-cordon","title":"Marquage des noeuds comme non planifiables (kubectl cordon)","text":"<p>Marquer un n\u0153ud comme non planifiable emp\u00eache la planification de nouveaux pods sur ce n\u0153ud, mais n'affecte pas les pods existants sur le n\u0153ud.</p> <pre><code>PRINT_COMMAND=true\nPREVIOUS_VERSION=\"27-9\"\n\nfor NODE in $( kubectl get nodes | grep \"v${PREVIOUS_VERSION}\" | awk '{print $1}' ); do\n  if ${PRINT_COMMAND}; then\n    echo \"kubectl cordon ${NODE}\"\n  else\n    kubectl cordon ${NODE}\n  fi\ndone\n\n    kubectl cordon  gke-hp-app-4-cpu-v27-9-200go-42347f56-j5t0\n    kubectl cordon  gke-hp-app-4-cpu-v27-9-200go-479d4396-xdwz\n    kubectl cordon  gke-hp-app-4-cpu-v27-9-200go-62b65060-lhkz\n    kubectl cordon  gke-hp-app-8-cpu-v27-9-200go-352e9832-090l\n    kubectl cordon  gke-hp-app-8-cpu-v27-9-200go-352e9832-70p2\n    kubectl cordon  gke-hp-app-8-cpu-v27-9-200go-858d2ea9-0bkq\n    kubectl cordon  gke-hp-app-8-cpu-v27-9-200go-858d2ea9-t0r2\n    kubectl cordon  gke-hp-app-8-cpu-v27-9-200go-d1ae30d5-nkbx\n    kubectl cordon  gke-hp-app-8-cpu-v27-9-200go-d1ae30d5-pwt8\n</code></pre> <p>Si la commande renvoie bien les commandes attendues, passer la variable PRINT_COMMAND \u00e0 false et rejour le script.</p> <p>Surveillance :</p> <pre><code>watch -n 1 kubectl get pods -A --field-selector=status.phase!=Running\n</code></pre>"},{"location":"GKE/dkt_upgrade_cluster_gke/readme/#deplacement-les-pods-vers-les-noeuds-aux-versions-plus-recentes-kubectl-drain","title":"D\u00e9placement les pods vers les noeuds aux versions plus r\u00e9centes (kubectl drain)","text":""},{"location":"GKE/dkt_upgrade_cluster_gke/readme/#identification-des-eventuels-problemes-lies-aux-pods-disruption-budgets-pdb","title":"Identification des \u00e9ventuels probl\u00e8mes li\u00e9s aux Pods Disruption Budgets (PDB)","text":"<pre><code>kubectl get pdb -A\n\n    NAMESPACE        NAME                                      MIN AVAILABLE   MAX UNAVAILABLE   ALLOWED DISRUPTIONS   AGE\n    istio-system     istiod                                    1               N/A               0                     432d\n    nginx-internal   nginx-internal-ingress-nginx-controller   1               N/A               1                     74d\n</code></pre> <p>-&gt; le PDB 'istiod' dans le namespace 'istio-system' a un 'allowed disruptions' positionn\u00e9 \u00e0 0, ce qui peut \u00eatre source de probl\u00e8mes.</p> <pre><code>kubectl get pods -n istio-system -o wide\n\n    NAME                      READY   STATUS    RESTARTS   AGE   IP             NODE                                         NOMINATED     NODE   READINESS GATES\n    istiod-7589db68cc-xxcpb   1/1     Running   0          11d   10.41.204.26   gke-hp-app-8-cpu-v27-9-200go-d1ae30d5-nkbx   &lt;none&gt;        &lt;none&gt;\n</code></pre> <p>-&gt; le pod auquel est rattach\u00e9 le PDB se trouve sur un noeud du 'grand' node pool (8 cpu). Nous pouvons faire le 'drain' sur le 'petit' node pool.</p>"},{"location":"GKE/dkt_upgrade_cluster_gke/readme/#drainage-le-petit-node-pool-pas-de-problemes-eventuels-avec-le-pdb-distiod","title":"Drainage le 'petit' node pool (pas de probl\u00e8mes \u00e9ventuels avec le PDB d'Istiod)","text":"<pre><code>CLUSTER=\"hp\"                        # nom du cluster (correspond \u00e0 l'environnement servi)\nNUM_CPUS=4                          # 4 pour le petit node pool; 8 pour le grand.\nPRINT_COMMAND=true\nPREVIOUS_VERSION=\"27-9\"\"\nNODE_PREFIX=gke-${CLUSTER}-app-${NUM_CPUS}-cpu-v${PREVIOUS_VERSION}-200go\n\nfor NODE in $( kubectl get nodes | grep ${NODE_PREFIX} | awk '{print $1}' ); do\n  if ${PRINT_COMMAND}; then\n    echo \"kubectl drain ${NODE} --ignore-daemonsets --delete-emptydir-data\"\n  else\n    kubectl drain ${NODE} --ignore-daemonsets --delete-emptydir-data\n  fi\ndone\n\n    kubectl drain gke-hp-app-4-cpu-v27-9-200go-42347f56-j5t0 --ignore-daemonsets --delete-emptydir-data\n    kubectl drain gke-hp-app-4-cpu-v27-9-200go-479d4396-xdwz --ignore-daemonsets --delete-emptydir-data\n    kubectl drain gke-hp-app-4-cpu-v27-9-200go-62b65060-lhkz --ignore-daemonsets --delete-emptydir-data\n</code></pre> <p>Note - Il est peut-\u00eatre pr\u00e9f\u00e9rable de faire les 'drain' un \u00e0 un manuellement pour g\u00e9rer les probl\u00e8mes \u00e9ventuels de PDB.</p> <p>V\u00e9rification du bon d\u00e9roulement de l'op\u00e9ration :</p> <pre><code>kubectl get nodes\n-&gt; avec le cordon, les anciens noeuds pasent au statut \u2018schedulingdisabled\u2019\n\n    NAME                                         STATUS                     ROLES    AGE   VERSION\n    gke-hp-app-4-cpu-v27-9-200go-42347f56-j5t0   Ready,SchedulingDisabled   &lt;none&gt;   11d   v1.27.9-gke.1092000\n    gke-hp-app-4-cpu-v27-9-200go-479d4396-xdwz   Ready,SchedulingDisabled   &lt;none&gt;   11d   v1.27.9-gke.1092000\n    gke-hp-app-4-cpu-v27-9-200go-62b65060-lhkz   Ready,SchedulingDisabled   &lt;none&gt;   11d   v1.27.9-gke.1092000\n    gke-hp-app-4-cpu-v28-200go-8d133288-qmg4     Ready                      &lt;none&gt;   33m   v1.28.6-gke.1289000\n    gke-hp-app-4-cpu-v28-200go-984857d0-3521     Ready                      &lt;none&gt;   33m   v1.28.6-gke.1289000\n    gke-hp-app-4-cpu-v28-200go-fb63cd12-0jhk     Ready                      &lt;none&gt;   33m   v1.28.6-gke.1289000\n    gke-hp-app-8-cpu-v27-9-200go-352e9832-090l   Ready,SchedulingDisabled   &lt;none&gt;   11d   v1.27.9-gke.1092000\n    gke-hp-app-8-cpu-v27-9-200go-352e9832-70p2   Ready,SchedulingDisabled   &lt;none&gt;   11d   v1.27.9-gke.1092000\n    gke-hp-app-8-cpu-v27-9-200go-858d2ea9-0bkq   Ready,SchedulingDisabled   &lt;none&gt;   11d   v1.27.9-gke.1092000\n    gke-hp-app-8-cpu-v27-9-200go-858d2ea9-t0r2   Ready,SchedulingDisabled   &lt;none&gt;   11d   v1.27.9-gke.1092000\n    gke-hp-app-8-cpu-v27-9-200go-d1ae30d5-nkbx   Ready,SchedulingDisabled   &lt;none&gt;   11d   v1.27.9-gke.1092000\n    gke-hp-app-8-cpu-v27-9-200go-d1ae30d5-pwt8   Ready,SchedulingDisabled   &lt;none&gt;   11d   v1.27.9-gke.1092000\n    gke-hp-app-8-cpu-v28-200go-28e2b668-s4z4     Ready                      &lt;none&gt;   25m   v1.28.6-gke.1289000\n    gke-hp-app-8-cpu-v28-200go-77e1408b-ccpw     Ready                      &lt;none&gt;   25m   v1.28.6-gke.1289000\n    gke-hp-app-8-cpu-v28-200go-c6a37d33-ws6h     Ready                      &lt;none&gt;   25m   v1.28.6-gke.1289000\n</code></pre>"},{"location":"GKE/dkt_upgrade_cluster_gke/readme/#bascule-distiod-sur-le-nouveau-node-pool-qui-nest-pas-en-cordon","title":"Bascule d'Istiod sur le nouveau node-pool (qui n'est pas en 'cordon')","text":"<p>Pour g\u00e9rer sereinement le probl\u00e8me de PDB, nous allons forcer le re-d\u00e9ploiement d'Istiod.</p> <p>Ce dernier ne pourra aller ailleurs que sur le nouveau node pool provisionn\u00e9 car les autres sont toujours en 'cordon'.</p> <pre><code>kubectl -n istio-system rollout restart deploy istiod\n\nkubectl -n istio-system get pod -o wide\n\nkubectl -n istio-system get po\n\n    NAME                      READY   STATUS    RESTARTS   AGE\n    istiod-6f4d65d7f8-6t5bz   1/1     Running   0          87s\n</code></pre> <p>-&gt; Maintenant qu'Istiod a bascul\u00e9 sur le nouveau node-pool, nous pouvons 'drainer' le node pool restant.</p>"},{"location":"GKE/dkt_upgrade_cluster_gke/readme/#drainage-le-petit-node-pool-pas-de-problemes-eventuels-avec-le-pdb-distiod_1","title":"Drainage le 'petit' node pool (pas de probl\u00e8mes \u00e9ventuels avec le PDB d'Istiod)","text":"<p>Pour identifier les labels rattach\u00e9s aux noeuds du cluster :</p> <pre><code>kubectl get nodes --show-labels\n</code></pre> <p>-&gt; Le label qui nous int\u00e9resse est le suivant : </p> <pre><code>cloud.google.com/gke-nodepool=app-8-cpu-v27-9-200go\n</code></pre> <p>Identifions les noeuds des clusters \u00e0 'drainer' :</p> <pre><code>kubectl get nodes -l cloud.google.com/gke-nodepool=app-8-cpu-v27-9-200go\n\n    NAME                                         STATUS                     ROLES    AGE   VERSION\n    gke-hp-app-8-cpu-v27-200go-352e9832-090l   Ready,SchedulingDisabled   &lt;none&gt;   11d   v1.27.9-gke.1092000\n    gke-hp-app-8-cpu-v27-200go-352e9832-70p2   Ready,SchedulingDisabled   &lt;none&gt;   11d   v1.27.9-gke.1092000\n    gke-hp-app-8-cpu-v27-200go-858d2ea9-0bkq   Ready,SchedulingDisabled   &lt;none&gt;   11d   v1.27.9-gke.1092000\n    gke-hp-app-8-cpu-v27-200go-858d2ea9-t0r2   Ready,SchedulingDisabled   &lt;none&gt;   11d   v1.27.9-gke.1092000\n    gke-hp-app-8-cpu-v27-200go-d1ae30d5-nkbx   Ready,SchedulingDisabled   &lt;none&gt;   11d   v1.27.9-gke.1092000\n    gke-hp-app-8-cpu-v27-200go-d1ae30d5-pwt8   Ready,SchedulingDisabled   &lt;none&gt;   11d   v1.27.9-gke.1092000\n</code></pre> <p>Lan\u00e7ons le drainage :</p> <pre><code>PRINT_COMMAND=true\n\nfor NODE in $( kubectl get nodes -l cloud.google.com/gke-nodepool=app-8-cpu-v27-9-200go --no-headers | awk '{print $1}' ); do\n  if ${PRINT_COMMAND}; then\n    echo kubectl drain ${NODE}} --ignore-daemonsets --delete-emptydir-data\n  else\n    kubectl drain ${NODE}} --ignore-daemonsets --delete-emptydir-data\ndone\n\n    kubectl drain gke-hp-app-8-cpu-v27-9-200go-352e9832-090l --ignore-daemonsets --delete-emptydir-data\n    kubectl drain gke-hp-app-8-cpu-v27-9-200go-352e9832-70p2 --ignore-daemonsets --delete-emptydir-data\n    kubectl drain gke-hp-app-8-cpu-v27-9-200go-858d2ea9-0bkq --ignore-daemonsets --delete-emptydir-data\n    kubectl drain gke-hp-app-8-cpu-v27-9-200go-858d2ea9-t0r2 --ignore-daemonsets --delete-emptydir-data\n    kubectl drain gke-hp-app-8-cpu-v27-9-200go-d1ae30d5-nkbx --ignore-daemonsets --delete-emptydir-data\n    kubectl drain gke-hp-app-8-cpu-v27-9-200go-d1ae30d5-pwt8 --ignore-daemonsets --delete-emptydir-data\n</code></pre> <p>Note - le 'drain' ne g\u00e8re que les pods qui ont \u00e9t\u00e9 g\u00e9n\u00e9r\u00e9s par un autre objet : deployment, replicaset, daemonset. Si un pod qui a \u00e9t\u00e9 g\u00e9n\u00e9r\u00e9 directement avec la commande 'kubectl create pod', le drain tombera en erreur.</p> <p>Voici un exemple :</p> <pre><code>kubectl drain gke-hp-app-4-cpu-v27-9-200go-479d4396-xdwz --ignore-daemonsets --delete-emptydir-data\n\n    node/gke-hp-app-4-cpu-v27-9-200go-479d4396-xdwz already cordoned\n    error: unable to drain node \"gke-hp-app-4-cpu-v27-9-200go-479d4396-xdwz\" due to error:cannot delete Pods declare no controller (use --force to override): ns-autodiag/pgclient, ns-masterfi-preprod/pgclient, continuing command...\n    There are pending nodes to be drained:\n     gke-hp-app-4-cpu-v27-9-200go-479d4396-xdwz\n    cannot delete Pods declare no controller (use --force to override): ns-autodiag/pgclient, ns-masterfi-preprod/pgclient\n</code></pre> <p>-&gt; il faut forcer l\u2019op\u00e9ration </p> <pre><code>kubectl drain gke-hp-app-4-cpu-v27-9-200go-479d4396-xdwz --ignore-daemonsets --delete-emptydir-data \u2014force\n</code></pre> <p>V\u00e9rification :</p> <pre><code>kubectl get pod -o=custom-columns=NAME:.metadata.name,STATUS:.status.phase,NODE:.spec.nodeName --all-namespaces | grep v27-9\n</code></pre> <p>-&gt; on ne voir plus que des daemonsets.</p>"},{"location":"GKE/dkt_upgrade_cluster_gke/readme/#suppression-des-anciens-node-pools","title":"Suppression des anciens node pools","text":""},{"location":"GKE/dkt_upgrade_cluster_gke/readme/#verification-des-node-pools","title":"V\u00e9rification des node pools","text":"<pre><code>gcloud container node-pools list --region europe-west4 --cluster hp\n\n    NAME                   MACHINE_TYPE   DISK_SIZE_GB  NODE_VERSION\n    app-4-cpu-v27-9-200go  n2-standard-4  200           1.27.9-gke.1092000\n    app-8-cpu-v27-9-200go  n2-standard-8  200           1.27.9-gke.1092000\n    app-4-cpu-v28-200go    n2-standard-4  200           1.28.6-gke.1289000\n    app-8-cpu-v28-200go    n2-standard-8  200           1.28.6-gke.1289000\n</code></pre>"},{"location":"GKE/dkt_upgrade_cluster_gke/readme/#suppression-des-anciens-node-pools_1","title":"Suppression des anciens node pools","text":"<pre><code>gcloud container node-pools delete app-4-cpu-v27-9-200go --region europe-west4 --cluster hp\ngcloud container node-pools delete app-8-cpu-v27-9-200go --region europe-west4 --cluster hp\n</code></pre> <p>Fin</p>"},{"location":"GKE/terraform_GKE_cluster/","title":"Index","text":"<p>Docs utiles : - https://registry.terraform.io/modules/terraform-google-modules/kubernetes-engine/google/latest - https://github.com/terraform-google-modules/terraform-google-kubernetes-engine - https://cloud.google.com/kubernetes-engine/docs/how-to/creating-a-zonal-cluster?hl=en#terraform - https://registry.terraform.io/modules/terraform-google-modules/kubernetes-engine/google/latest/submodules/workload-identity - https://cloud.google.com/kubernetes-engine/docs/concepts/alias-ips?hl=en - https://learnk8s.io/terraform-gke</p> <p>resource \"google_container_cluster\" \"default\" {   name               = \"gke-standard-zonal-single-zone\"   location           = \"us-central1-a\"   initial_node_count = 1</p> <p># Set <code>deletion_protection</code> to <code>true</code> will ensure that one cannot   # accidentally delete this instance by use of Terraform.   deletion_protection = false }</p>"},{"location":"Minikube/minikube_nginx_controller_macos/","title":"Minikube et l'ingress Nginx sur macOS","text":"<p>D\u00e9ploiement d'un cluster Minikube sur MacOS avec un Ingress Nginx et exposition d'un application de d\u00e9mo.</p>"},{"location":"Minikube/minikube_nginx_controller_macos/#docs","title":"Docs","text":"Liens utiles le tuto d'origine (kubernetes.io) all GKE code samples hello-app source code hello-app image on Google Container Registry (GCR) cURL : provide a custom IP adress for a name Docker &amp; Kubernetes : Setting up Ingress with NGINX Controller on Minikube (Mac) Minikube - accessing apps minikube tunnel Kubernetes documentation - networking - ingress"},{"location":"Minikube/minikube_nginx_controller_macos/#pre-requis","title":"Pr\u00e9-requis","text":""},{"location":"Minikube/minikube_nginx_controller_macos/#installation-de-kubectl-kubens-kubectx","title":"installation de kubectl, kubens, kubectx, ...","text":"<p>Se reporter aux autres docs, ok ? ^^</p>"},{"location":"Minikube/minikube_nginx_controller_macos/#installation-et-demarrage-de-minikube-avec-nginx-comme-ingress","title":"Installation et d\u00e9marrage de minikube avec Nginx comme ingress","text":"Minikube install with nginx ingress controller<pre><code>brew update &amp;&amp; brew install minikube\nminikube start\nminikube status\nminikube addons list\nminikube addons enable ingress\nk get po -n ingress-nginx\n</code></pre>"},{"location":"Minikube/minikube_nginx_controller_macos/#demo","title":"D\u00e9mo","text":""},{"location":"Minikube/minikube_nginx_controller_macos/#deploiement-et-exposition-dune-dummy-app","title":"D\u00e9ploiement et exposition d'une 'dummy app'","text":"<p>L'application 'hello-app' affiche dans une page web le nom de l'application, sa version ainsi que le pod dans lequel elle tourne.</p> D\u00e9ploiement d'une 'dummy app'<pre><code>    # D\u00e9ploiement de l'application \u00e0 la version 1 :\n    kubectl create deployment web --image=gcr.io/google-samples/hello-app:1.0 --dry-run=client -o yaml &gt; web.deployment.yaml\n    kubectl apply -f web.deployment.yaml\n\n    # V\u00e9rification de son bon fonctionnement et de son port d'\u00e9coute :\n    podName=$( kubectl -n default get pod -l app=web -o json | jq -r '.items[].metadata.name' )\n    kubectl -n default logs ${podName}\n\n        # 2024/03/30 14:46:46 Server listening on port 8080\n\n\n    # Exposition de l'application :\n    kubectl expose deployment web --type=NodePort --port=8080 --dry-run=client -o yaml &gt; web.service.yaml\n\n    kubectl -n default get service web\n\n        # NAME   TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE\n        # web    NodePort   10.98.252.182   &lt;none&gt;        8080:32002/TCP   109s\n\n\n    # Acc\u00e8s \u00e0 l'application :\n\n      - via le port-forwarding :\n\n        kubetl port-forward service/web 8080:8080 &amp;\n        curl http://localhost:8080\n\n      - via le NodePort :\n\n        minikube service web    # Renvoie une URL pour se connecter \u00e0 un service (de type NodePort)\n\n          # |-----------|------|-------------|---------------------------|\n          # | NAMESPACE | NAME | TARGET PORT |            URL            |\n          # |-----------|------|-------------|---------------------------|\n          # | default   | web  |        8080 | http://192.168.49.2:32002 |\n          # |-----------|------|-------------|---------------------------|\n          # \ud83c\udfc3  Tunnel de d\u00e9marrage pour le service web.\n          # |-----------|------|-------------|------------------------|\n          # | NAMESPACE | NAME | TARGET PORT |          URL           |\n          # |-----------|------|-------------|------------------------|\n          # | default   | web  |             | http://127.0.0.1:54247 |\n          # |-----------|------|-------------|------------------------|\n          # \ud83c\udf89  Ouverture du service default/web dans le navigateur par d\u00e9faut...\n          # \u2757  Comme vous utilisez un pilote Docker sur darwin, le terminal doit \u00eatre ouvert pour l'ex\u00e9cuter.\n</code></pre> <p>C'est tr\u00e8s bien, \u00e7a fonctionne, mais ce n'est pas ce qu'on veut faire : on veut passer par un service de type Load-Balancer en renseignant un FQDN et nom une IP et un port TCP.</p>"},{"location":"Minikube/minikube_nginx_controller_macos/#definition-dun-ingress-controller-pour-notre-ummy-app","title":"D\u00e9finition d'un Ingress Controller pour notre 'ummy app'","text":"<pre><code>cat &lt;&lt; EOF &gt;&gt; web.ingress.yaml\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: web\n  annotations:\n    nginx.ingress.kubernetes.io/rewrite-target: /\nspec:\n  rules:\n    - host: hello-world.info\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: web\n                port:\n                  number: 8080\nEOF\n\nkubectl -n default apply -f web.ingress.yaml\nkubectl -n default get ingress web\n\n    # NAME   CLASS   HOSTS              ADDRESS        PORTS   AGE\n    # web    nginx   hello-world.info   192.168.49.2   80      10s\n</code></pre> <p>Note : l'adresse IP renvoy\u00e9e correspond ici \u00e0 celle de minikube (qu'on peut obtenir avec la commande: \"minikube ip\")</p> <p>Sur un Mac, il n'est pas posible d'utiliser un service NodePort directement \u00e0 cause de la fa\u00e7on dont la couche r\u00e9seau de Docker est impl\u00e9ment\u00e9e. Nous devons utiliser un moyen de contournement :  minikube tunnel.</p> <p>La commande 'minikube tunnel' cr\u00e9e une route vers les services d\u00e9ploy\u00e9s avec le type LoadBalancer et d\u00e9finit leur Ingress \u00e0 leur ClusterIP. Dans un nouveau terminal, ex\u00e9cuter la commande suivante sans l'interrompre :</p> <pre><code>sudo minikube tunnel    # sudo est n\u00e9cessaire car on vise des ports r\u00e9serv\u00e9s (&lt;1024)\n</code></pre>"},{"location":"Minikube/minikube_nginx_controller_macos/#acces-a-lapplication-avec-curl","title":"Acc\u00e8s \u00e0 l'application avec cURL","text":"<pre><code>curl --Header \"Host: hello-world.info\" http://localhost/\n</code></pre>"},{"location":"Minikube/minikube_nginx_controller_macos/#acces-a-lapplication-avec-un-navigateur","title":"Acc\u00e8s \u00e0 l'application avec un navigateur","text":"<p>Il faudra modifier l'entr\u00e9e localhost dans /etc/hosts : 127.0.0.1   localhost hello-world.info Ensuite il suffira d'acc\u00e9der \u00e0 l'URL : </p> <pre><code>http://hello-world.info/\n</code></pre>"},{"location":"Minikube/minikube_nginx_controller_macos/#deploiement-dune-seconde-application","title":"D\u00e9ploiement d'une seconde application","text":"<p>Pour aller plus loin, nous allons cr\u00e9r un second deployment de la m\u00eame 'dummy app', mais \u00e0 la version 2 et l'exposer ensuite :</p> <pre><code>kubectl create deployment web2 --image=gcr.io/google-samples/hello-app:2.0 --dry-run=client -o yaml &gt; web2.deployment.yaml\nkubectl apply -f web2.deployment.yaml\n\nkubectl expose deployment web2 --port=8080 --type=NodePort --dry-run=client -o yaml &gt; web2.service.yaml\nkubectl apply -f web2.service.yaml\n\nkubectl -n default get pods,deployments,services\n</code></pre> <p>Nous allons compl\u00e9ter le manifest de notre Ingress controller :</p> <pre><code>cat &lt;&lt; EOF &gt;&gt; web.ingress.yaml\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: web\n  annotations:\n    nginx.ingress.kubernetes.io/rewrite-target: /\nspec:\n  rules:\n    - host: hello-world.info\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: web\n                port:\n                  number: 8080\n          - path: /v2\n            pathType: Prefix\n            backend:\n              service:\n                name: web2\n                port:\n                  number: 8080\nEOF\n\nkubectl -n default apply -f web.ingress.yaml\nkubectl -n default get ingress web\n</code></pre>"},{"location":"Minikube/minikube_nginx_controller_macos/#acces-aux-2-applications","title":"Acc\u00e8s aux 2 applications","text":"<p>Pour tester, il suffit d'acc\u00e9der \u00e0 partir d'un navigateur ou de cURL aux URLs suivantes :</p> <pre><code>* http://hello-world.info/      # avec cURL : curl --Header \"Host: hello-world.info\" http://localhost/\n* http://hello-world.info/v2    # avec cURL : curl --Header \"Host: hello-world.info\" http://localhost/v2\n</code></pre> <p>Et voil\u00e0 ^^</p>"},{"location":"Minikube/minikube_nginx_controller_macos/#modification-du-routage-chaque-application-aura-son-propre-fqdn","title":"Modification du routage : chaque application aura son propre FQDN","text":"<p>Nous souhaitons d\u00e9sormais rendre accessible : * web  via l'URL hello-world.info; * web2 via l'URL hello-world-2.info.</p>"},{"location":"Minikube/minikube_nginx_controller_macos/#modification-de-lingress-resource","title":"Modification de l'Ingress Resource","text":"<pre><code>kubectl delete ingress web\n\ncat &lt;&lt; EOF &gt;&gt; web2.ingress.yaml \napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: web2\n  annotations:\n    nginx.ingress.kubernetes.io/rewrite-target: /\nspec:\n  rules:\n    - host: hello-world.info\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: web\n                port:\n                  number: 8080\n    - host: hello-world-2.info\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: web2\n                port:\n                  number: 8080\nEOF\n\nkubectl apply -f web2.ingress.yaml\nkubect ldescribe ingress web2\n</code></pre>"},{"location":"Minikube/minikube_nginx_controller_macos/#tests","title":"Tests","text":"<pre><code>curl --header \"Host: hello-world.info\" http://127.0.0.1/\n\n# Hello, world!\n# Version: 1.0.0\n# Hostname: web-57f46db77f-r2rtc\n\n\ncurl --header \"Host: hello-world-2.info\" http://127.0.0.1/\n\n  # Hello, world!\n  # Version: 2.0.0\n  # Hostname: web2-866dc4bcc8-52xjp\n</code></pre> <p>Via le navigateur, il faut avant tout modifier l'entr\u00e9e du fichier /etc/hosts pour que 127.0.0.1 renvoie vers hello-world.info et hello-world-2.info.</p>"},{"location":"Minikube/minikube_nginx_controller_macos/#nettoyage","title":"Nettoyage","text":"<ul> <li> <p>Arr\u00eater minikube tunnel et fermer son terminal;</p> </li> <li> <p>Supprimer l'Ingress controller, les services et les deployments :</p> <p>kubectl -n default delete ingress web   kubectl -n default delete services web web2   kubectl -n default delete deployments web web2</p> </li> <li> <p>V\u00e9rifier que nous n'avons rien oubli\u00e9:</p> <p>kubectl -n default get pods,deployments,services,ingress</p> </li> <li> <p>Arr\u00eater le cluster minikube :</p> <p>minikube stop   minikube status</p> </li> </ul>"},{"location":"Minikube/prometheus_on_minikube_from_scratch/","title":"Prometheus on MiniKube from scratch","text":"<p>Creating and publishing an Alpine+Prometheus image on Docker Hub and deploying the Prometheus service on MiniKube </p> <p>Prerequisite : MiniKube already installed, up and running</p> USEFUL PAGES URL RBAC Prometheus https://github.com/prometheus-operator/prometheus-operator/tree/main/example/rbac/prometheus RBAC ClusterRole https://kubernetes.io/docs/reference/access-authn-authz/rbac/#kubectl-create-clusterrole How to Install Prometheus on Kubernetes and Use It for Monitoring https://phoenixnap.com/kb/prometheus-kubernetes MiniKube - persistent volumes https://minikube.sigs.k8s.io/docs/handbook/persistent_volumes/ Kubernetes persistent volumes https://spacelift.io/blog/kubernetes-persistent-volumes"},{"location":"Minikube/prometheus_on_minikube_from_scratch/#retrieving-the-prometheus-helm-chart-locally","title":"Retrieving the Prometheus Helm Chart locally","text":"<pre><code>helm pull prometheus-community/prometheus --untar\n</code></pre>"},{"location":"Minikube/prometheus_on_minikube_from_scratch/#docker-image","title":"Docker image","text":"<pre><code>docker build -t &lt;my_docker_username&gt;/prometheus:2.48.1 -t &lt;my_docker_username&gt;/prometheus:latest .\ndocker login --username &lt;my_docker_username&gt; --password &lt;my_docker_password&gt;\ndocker push zigouigoui/prometheus:2.48.1\ndocker push zigouigoui/prometheus:latest\n</code></pre>"},{"location":"Minikube/prometheus_on_minikube_from_scratch/#prometheus-service-on-minikube","title":"Prometheus service on MiniKube","text":"<pre><code>manifestsList=( monitoring.namespace.yaml\n                prometheus.service-account.yaml\n                prometheus.cluster-role.yaml\n                prometheus.cluster-role-binding.yaml\n                prometheus.configmap.yaml\n                prometheus.persistent-volume.data.yaml\n                prometheus.persistent-volume.logs.yaml\n                prometheus.persistent-volume-claim.data.yaml\n                prometheus.persistent-volume-claim.logs.yaml\n                prometheus.deployment.yaml\n                prometheus.service.yaml )\n\nfor manifest in ${manifestsList[@]}; do \n    echo \"kubectl apply -f ${manifest}\"\ndone\n</code></pre>"},{"location":"Minikube/prometheus_on_minikube_from_scratch/#accessing-the-wui","title":"Accessing the WUI","text":"<pre><code>kubectl port-forward service/prometheus 8080:9090\n</code></pre> <p>-&gt; browser : http://localhost:8080 -&gt; Prometheus WUI</p>"},{"location":"Prometheus_and_Grafana/kube-prometheus-stack_managed_with_fluxcd/","title":"'kube-prometheus-stack' managed with FluxCD","text":""},{"location":"Prometheus_and_Grafana/kube-prometheus-stack_managed_with_fluxcd/#abstract","title":"Abstract","text":"<p>Ce howto fait suite au hoxto 'FluxCD / FluxCD - D\u00e9monstration par l'exemple'.</p> <p>Il d\u00e9crit comment d\u00e9ployer via FluxCD le Helm Chart 'kube-prometheus-stack' qui vise \u00e0 installer un monitoring de notre cluster Kubernetes reposant sur Prometheus / Alert manager et Grafana.</p> Doc URL artifacthub.io https://artifacthub.io/packages/helm/prometheus-community/kube-prometheus-stack GitHub https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack"},{"location":"Prometheus_and_Grafana/kube-prometheus-stack_managed_with_fluxcd/#preparatifs","title":"Pr\u00e9paratifs","text":"<p>Pr\u00e9parons notre environnement local :</p> <pre><code># R\u00e9pertoire accueillant nos d\u00e9p\u00f4ts Git en local\nexport LOCAL_GITHUB_REPOS=\"${HOME}/code/github\"\n\n# Mise \u00e0 jour des copies locales des d\u00e9p\u00f4ts d\u00e9di\u00e9s \u00e0 FluxCD et aux applications qu'il g\u00e8re\ncd ${LOCAL_GITHUB_REPOS}/k8s-kind-apps   &amp;&amp; git pull\ncd ${LOCAL_GITHUB_REPOS}/k8s-kind-fluxcd &amp;&amp; git pull\n\n# Cr\u00e9ation d'un r\u00e9pertoire d\u00e9di\u00e9 au monitoring\nmkdir -p ${LOCAL_GITHUB_REPOS}/k8s-kind-fluxcd/apps/monitoring\n</code></pre> <p>Cr\u00e9ons ensuite un namesapce d\u00e9di\u00e9 au monitoring :</p> <pre><code>kubectl create ns monitoring --dry-run=client -o yaml &gt; ${LOCAL_GITHUB_REPOS}/k8s-kind-fluxcd/apps/monitoring/namespace.yaml\nkubectl apply -f ${LOCAL_GITHUB_REPOS}/k8s-kind-fluxcd/apps/monitoring/namespace.yaml\n</code></pre>"},{"location":"Prometheus_and_Grafana/kube-prometheus-stack_managed_with_fluxcd/#helm-repository","title":"Helm Repository","text":"<p>Nous pouvons d\u00e9sormais d\u00e9finir aupr\u00e8s de FluxCD le HelmRepository qui nous int\u00e9resse :</p> codeoutput <pre><code>flux create source helm prometheus-community \\\n  --url=https://prometheus-community.github.io/helm-charts \\\n  --namespace=monitoring \\\n  --interval=1m \\\n  --export &gt; ${LOCAL_GITHUB_REPOS}/k8s-kind-fluxcd/apps/monitoring/helm-repository.yaml\n</code></pre> <pre><code>---\napiVersion: source.toolkit.fluxcd.io/v1beta2\nkind: HelmRepository\nmetadata:\n  name: prometheus-community\n  namespace: monitoring\nspec:\n  interval: 1m0s\n  url: https://prometheus-community.github.io/helm-charts\n</code></pre> <p>Poussons nos modifications sur notre d\u00e9p\u00f4t GitHub pour que FluxCD les prenne en compte :</p> codeoutput <pre><code>export LOCAL_GITHUB_REPOS=\"${HOME}/code/github\"\n\ncd ${LOCAL_GITHUB_REPOS}/k8s-kind-fluxcd\n\ngit add .\ngit commit -m \"feat: Defining a namespace + a Helm repository for Prometheus.\"\ngit push\n\nflux reconcile kustomization flux-system --with-source\n\nkubectl -n monitoring get helmrepository\n</code></pre> <pre><code>NAME                   URL                                                  AGE   READY   STATUS\nprometheus-community   https://prometheus-community.github.io/helm-charts   37s   True    stored artifact: revision 'sha256:8d880a1010d4ba3df22364b59881e235590c184266f0c9fb894eeedb23442b12'\n</code></pre>"},{"location":"Prometheus_and_Grafana/kube-prometheus-stack_managed_with_fluxcd/#helm-release","title":"Helm Release","text":"<p>Tentons l'installation de la stack de monitoring avec les valeurs par d\u00e9faut (ie. sans devoir les surchager avec le flag '--values') :</p> codeoutput <pre><code>export LOCAL_GITHUB_REPOS=\"${HOME}/code/github\"\n\nflux create helmrelease kube-prometheus-stack \\\n  --source=HelmRepository/prometheus-community \\\n  --chart=kube-prometheus-stack \\\n  --namespace=monitoring \\\n  --export &gt; ${LOCAL_GITHUB_REPOS}/k8s-kind-fluxcd/apps/monitoring/helm-release.yaml\n</code></pre> <pre><code>---\napiVersion: helm.toolkit.fluxcd.io/v2beta1\nkind: HelmRelease\nmetadata:\n  name: kube-prometheus-stack\n  namespace: monitoring\nspec:\n  chart:\n    spec:\n      chart: kube-prometheus-stack\n      reconcileStrategy: ChartVersion\n      sourceRef:\n        kind: HelmRepository\n        name: prometheus-community\n  interval: 1m0s\n</code></pre> <p>Poussons nos modifications sur notre d\u00e9p\u00f4t GitHub pour que FluxCD les prenne en compte :</p> codeoutput <pre><code>export LOCAL_GITHUB_REPOS=\"${HOME}/code/github\"\n\ncd ${LOCAL_GITHUB_REPOS}/k8s-kind-fluxcd\n\ngit add .\ngit commit -m \"feat: Defining a kube-prometheus-stack Helm release.\"\ngit push\n\nflux reconcile kustomization flux-system --with-source\n\nkubectl -n monitoring get helmrelease\n</code></pre> <pre><code>NAME                    AGE   READY   STATUS\nkube-prometheus-stack   93s   True    Release reconciliation succeeded\n</code></pre> <p>V\u00e9rifions ce qui a \u00e9t\u00e9 d\u00e9ploy\u00e9 sur le cluster :</p> codeoutput <pre><code>kubectl -n monitoring get all\n</code></pre> <pre><code>NAME                                                            READY   STATUS    RESTARTS   AGE\npod/alertmanager-kube-prometheus-stack-alertmanager-0           2/2     Running   0          118s\npod/kube-prometheus-stack-grafana-7cf5785ff8-qp5xf              3/3     Running   0          2m10s\npod/kube-prometheus-stack-kube-state-metrics-65594f9476-8tpcv   1/1     Running   0          2m10s\npod/kube-prometheus-stack-operator-6459f9c556-67dvk             1/1     Running   0          2m10s\npod/kube-prometheus-stack-prometheus-node-exporter-qkjzz        1/1     Running   0          2m10s\npod/prometheus-kube-prometheus-stack-prometheus-0               2/2     Running   0          118s\n\nNAME                                                     TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                      AGE\nservice/alertmanager-operated                            ClusterIP   None            &lt;none&gt;        9093/TCP,9094/TCP,9094/UDP   118s\nservice/kube-prometheus-stack-alertmanager               ClusterIP   10.96.133.65    &lt;none&gt;        9093/TCP,8080/TCP            2m10s\nservice/kube-prometheus-stack-grafana                    ClusterIP   10.96.203.186   &lt;none&gt;        80/TCP                       2m10s\nservice/kube-prometheus-stack-kube-state-metrics         ClusterIP   10.96.22.99     &lt;none&gt;        8080/TCP                     2m10s\nservice/kube-prometheus-stack-operator                   ClusterIP   10.96.18.86     &lt;none&gt;        443/TCP                      2m10s\nservice/kube-prometheus-stack-prometheus                 ClusterIP   10.96.60.161    &lt;none&gt;        9090/TCP,8080/TCP            2m10s\nservice/kube-prometheus-stack-prometheus-node-exporter   ClusterIP   10.96.92.181    &lt;none&gt;        9100/TCP                     2m10s\nservice/prometheus-operated                              ClusterIP   None            &lt;none&gt;        9090/TCP                     118s\n\nNAME                                                            DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE\ndaemonset.apps/kube-prometheus-stack-prometheus-node-exporter   1         1         1       1            1           kubernetes.io/os=linux   2m10s\n\nNAME                                                       READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/kube-prometheus-stack-grafana              1/1     1            1           2m10s\ndeployment.apps/kube-prometheus-stack-kube-state-metrics   1/1     1            1           2m10s\ndeployment.apps/kube-prometheus-stack-operator             1/1     1            1           2m10s\n\nNAME                                                                  DESIRED   CURRENT   READY   AGE\nreplicaset.apps/kube-prometheus-stack-grafana-7cf5785ff8              1         1         1       2m10s\nreplicaset.apps/kube-prometheus-stack-kube-state-metrics-65594f9476   1         1         1       2m10s\nreplicaset.apps/kube-prometheus-stack-operator-6459f9c556             1         1         1       2m10s\n\nNAME                                                               READY   AGE\nstatefulset.apps/alertmanager-kube-prometheus-stack-alertmanager   1/1     118s\nstatefulset.apps/prometheus-kube-prometheus-stack-prometheus       1/1     118s\n</code></pre>"},{"location":"Prometheus_and_Grafana/kube-prometheus-stack_managed_with_fluxcd/#mise-en-place-des-alertes-avec-discord","title":"Mise en place des alertes avec Discord","text":""},{"location":"Prometheus_and_Grafana/kube-prometheus-stack_managed_with_fluxcd/#creation-dun-salon-discord","title":"Cr\u00e9ation d'un salon Discord","text":"<p>Nous n'allons pas r\u00e9inventer la roue et r\u00e9utiliserons le serveur Discord que nous avons provisionn\u00e9 pr\u00e9alablement et dont l'installation a \u00e9t\u00e9 d\u00e9taill\u00e9e dans ce howto.</p> <p>Nous suivrons \u00e9galement la proc\u00e9dure de cr\u00e9ation d'un salon de ce howto pour cr\u00e9er un salon priv\u00e9 d\u00e9di\u00e9 aux alertes li\u00e9es \u00e0 'kube-prometheus-stack' que nous nommerons 'monitoring' et pour lequel nous cr\u00e9erons un webhook.</p> <p></p>"},{"location":"Prometheus_and_Grafana/kube-prometheus-stack_managed_with_fluxcd/#creation-du-secret-correspondant-au-webhook","title":"Cr\u00e9ation du secret correspondant au webhook","text":"codeoutput <pre><code>export WEBHOOK_FOO=\"https://discord.com/api/webhooks/1242845059800633425/zyTYEpNZGf6vpd6C1sRLqeW_TGyFEMP2EM8BXAzockt20eeennkSHDKoO2-UxEG0K4ah\"\nexport LOCAL_GITHUB_REPOS=\"${HOME}/code/github\"\n\ncd ${LOCAL_GITHUB_REPOS}/k8s-kind-fluxcd\n\nkubectl -n monitoring create secret generic discord-webhook --from-literal=address=${WEBHOOK_FOO} --dry-run=client -o yaml &gt; apps/monitoring/discord-webhook.secret.yaml\nkubectl apply -f apps/monitoring/discord-webhook.secret.yaml\n</code></pre> <pre><code>apiVersion: v1\ndata:\n  address: aHR0cHM6Ly9kaXNjb3JkLmNvbS9hcGkvd2ViaG9va3MvMTI0Mjg0NTA1OTgwMDYzMzQyNS96eVRZRXBOWkdmNnZwZDZDMXNSTHFlV19UR3lGRU1QMkVNOEJYQXpvY2t0MjBlZWVubmtTSERLb08yLVV4RUcwSzRhaA==\nkind: Secret\nmetadata:\n  creationTimestamp: null\n  name: discord-webhook\n  namespace: monitoring\n</code></pre>"},{"location":"Prometheus_and_Grafana/kube-prometheus-stack_managed_with_fluxcd/#creation-du-notification-provider","title":"Cr\u00e9ation du 'notification provider'","text":"code'monitoring' notification provider <pre><code>export LOCAL_GITHUB_REPOS=\"${HOME}/code/github\"\n\ncd ${LOCAL_GITHUB_REPOS}/k8s-kind-fluxcd\n\nflux create alert-provider discord \\\n  --type=discord \\\n  --secret-ref=discord-webhook \\\n  --channel=monitoring \\\n  --username=FluxCD \\\n  --namespace=monitoring \\\n  --export &gt; apps/monitoring/notification-provider.yaml\n</code></pre> <pre><code>---\napiVersion: notification.toolkit.fluxcd.io/v1beta2\nkind: Provider\nmetadata:\n  name: discord\n  namespace: monitoring\nspec:\n  channel: monitoring\n  secretRef:\n    name: discord-webhook\n  type: discord\n  username: FluxCD\n</code></pre>"},{"location":"Prometheus_and_Grafana/kube-prometheus-stack_managed_with_fluxcd/#configuration-de-lalerte-discord-dans-fluxcd","title":"Configuration de l'alerte Discord dans FluxCD","text":"codeoutput <pre><code>export LOCAL_GITHUB_REPOS=\"${HOME}/code/github\"\n\ncd ${LOCAL_GITHUB_REPOS}/k8s-kind-fluxcd\n\nflux create alert discord \\\n  --event-severity=info \\\n  --event-source='GitRepository/*,Kustomization/*,ImageRepository/*,ImagePolicy/*,HelmRepository/*,HelmRelease/*' \\\n  --provider-ref=discord \\\n  --namespace=monitoring \\\n  --export &gt; apps/monitoring/notification-alert.yaml\n</code></pre> <pre><code>---\napiVersion: notification.toolkit.fluxcd.io/v1beta2\nkind: Alert\nmetadata:\n  name: discord\n  namespace: monitoring\nspec:\n  eventSeverity: info\n  eventSources:\n  - kind: GitRepository\n    name: '*'\n  - kind: Kustomization\n    name: '*'\n  - kind: ImageRepository\n    name: '*'\n  - kind: ImagePolicy\n    name: '*'\n  - kind: HelmRepository\n    name: '*'\n  - kind: HelmRelease\n    name: '*'\n  providerRef:\n    name: discord\n</code></pre>"},{"location":"Prometheus_and_Grafana/kube-prometheus-stack_managed_with_fluxcd/#activation-des-alertes-et-notifications","title":"Activation des alertes et notifications","text":""},{"location":"Prometheus_and_Grafana/kube-prometheus-stack_managed_with_fluxcd/#activation-au-niveau-de-fluxcd","title":"Activation au niveau de FluxCD","text":"<p>Poussons nos modifications dans notre d\u00e9p\u00f4t GitHub :</p> <pre><code>export LOCAL_GITHUB_REPOS=\"${HOME}/code/github\"\n\ncd ${LOCAL_GITHUB_REPOS}/k8s-kind-fluxcd\n\ngit add .\ngit commit -m \"feat: setting up 'monitoring' Discord alerting.\"\ngit push\n\nflux reconcile kustomization flux-system --with-source\n</code></pre>"},{"location":"Prometheus_and_Grafana/kube-prometheus-stack_managed_with_fluxcd/#verification-de-la-creation-des-alertes-et-notifications","title":"V\u00e9rification de la cr\u00e9ation des alertes et notifications","text":"codeoutput <pre><code>kubectl -n monitoring get providers,alerts\n</code></pre> <pre><code>NAME                                              AGE   READY   STATUS\nprovider.notification.toolkit.fluxcd.io/discord   9s    True    Initialized\n\nNAME                                              AGE   READY   STATUS\nalert.notification.toolkit.fluxcd.io/discord      9s    True    Initialized\n</code></pre>"},{"location":"Prometheus_and_Grafana/kube-prometheus-stack_managed_with_fluxcd/#test-de-lalerting","title":"Test de l'alerting","text":"<p>Nous allons supprimer la Helm Release et la r\u00e9installer pour nous assurer que nous sommes bien notifi\u00e9s dans le salon 'monitoring' de Discord :</p> <pre><code>export LOCAL_GITHUB_REPOS=\"${HOME}/code/github\"\n\ncd ${LOCAL_GITHUB_REPOS}/k8s-kind-fluxcd\n\n\n# suppression de la helm release :\n\nmv apps/monitoring/helm-release.yaml apps/monitoring/helm-release.yaml.BKP\n\ngit add .\ngit commit -m 'test: removing the prometheus stack.' \ngit push\n\nflux reconcile kustomization flux-system --with-source\n\n\n# r\u00e9-installation de la helm release :\n\nmv apps/monitoring/helm-release.yaml.BKP apps/monitoring/helm-release.yaml\n\ngit add .\ngit commit -m 'test: re-installing the prometheus stack.' \ngit push\n\nflux reconcile kustomization flux-system --with-source\n</code></pre> <p>L'op\u00e9ration a bien g\u00e9n\u00e9r\u00e9 une notification comme attendu :</p> <p></p> <p>Warning</p> <p>Discord nous a alert\u00e9 du d\u00e9ploiement de la Helm Release mais pas de sa suppression.</p>"},{"location":"Prometheus_and_Grafana/kube-prometheus-stack_managed_with_fluxcd/#acces-a-la-stack-de-monitoring-via-le-port-forwarding","title":"Acc\u00e8s \u00e0 la stack de monitoring via le 'port-forwarding'","text":""},{"location":"Prometheus_and_Grafana/kube-prometheus-stack_managed_with_fluxcd/#activation-du-port-forwarding-pour-tous-les-composants","title":"Activation du port-forwarding pour tous les composants","text":""},{"location":"Prometheus_and_Grafana/kube-prometheus-stack_managed_with_fluxcd/#determination-des-composants-de-la-stack-de-monitoring","title":"D\u00e9termination des composants de la stack de monitoring","text":"codeoutput <pre><code>kubectl -n monitoring get services\n</code></pre> <pre><code>NAME                                             TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                      AGE\nalertmanager-operated                            ClusterIP   None            &lt;none&gt;        9093/TCP,9094/TCP,9094/UDP   12h\nkube-prometheus-stack-alertmanager               ClusterIP   10.96.61.95     &lt;none&gt;        9093/TCP,8080/TCP            12h\nkube-prometheus-stack-grafana                    ClusterIP   10.96.211.70    &lt;none&gt;        80/TCP                       12h\nkube-prometheus-stack-kube-state-metrics         ClusterIP   10.96.216.205   &lt;none&gt;        8080/TCP                     12h\nkube-prometheus-stack-operator                   ClusterIP   10.96.97.78     &lt;none&gt;        443/TCP                      12h\nkube-prometheus-stack-prometheus                 ClusterIP   10.96.20.214    &lt;none&gt;        9090/TCP,8080/TCP            12h\nkube-prometheus-stack-prometheus-node-exporter   ClusterIP   10.96.19.61     &lt;none&gt;        9100/TCP                     12h\nprometheus-operated                              ClusterIP   None            &lt;none&gt;        9090/TCP                     12h  \n</code></pre> Doc URL prometheus operator beginner's guide https://blog.container-solutions.com/prometheus-operator-beginners-guide <p>Je ne suis pas encore tr\u00e8s familier avec cette stack, mais en lisant la doc dans le tableau pr\u00e9c\u00e9dent, je comprends qu'il faut privil\u00e9gier 'prometheus-operated' et 'alertmanager-operated' pour acc\u00e9der aux 2 services \u00e9ponymes.</p>"},{"location":"Prometheus_and_Grafana/kube-prometheus-stack_managed_with_fluxcd/#port-forwarding-de-prometheus-alertmanager-et-grafana","title":"Port-forwarding de prometheus, alertmanager et grafana","text":"<pre><code>kubectl -n monitoring port-forward service/prometheus-operated           9090:9090 &amp;\nkubectl -n monitoring port-forward service/alertmanager-operated         9093:9093 &amp;\nkubectl -n monitoring port-forward service/kube-prometheus-stack-grafana 8080:80   &amp;\n</code></pre>"},{"location":"Prometheus_and_Grafana/kube-prometheus-stack_managed_with_fluxcd/#acces-a-prometheus","title":"Acc\u00e8s \u00e0 Prometheus","text":"<p>Tip</p> <p>URL d'acc\u00e8s : http://localhost:9090</p> <p>Assurons-nous que nous acc\u00e9dons bien \u00e0 Prometheus \u00e0 l'aide de notre navigateur :</p> <p></p> <p>V\u00e9rifions ensuite que toutes les 'targets' sont bien accessibles :</p> <p></p> <p>Elles apparaissent toutes en 'healthy' :</p> <p></p> <p>Si tel n'est pas le cas, reportez-vous au howto suivant pour patcher correctement votre cluster KinD : https://papafrancky.github.io/000_setup/Kubernetes_en_local/</p>"},{"location":"Prometheus_and_Grafana/kube-prometheus-stack_managed_with_fluxcd/#acces-a-alertmanager","title":"Acc\u00e8s \u00e0 Alertmanager","text":"<p>Tip</p> <p>URL d'acc\u00e8s : http://localhost:9093</p> <p></p>"},{"location":"Prometheus_and_Grafana/kube-prometheus-stack_managed_with_fluxcd/#acces-a-grafana","title":"Acc\u00e8s \u00e0 Grafana","text":"<p>Tip</p> <p>URL d'acc\u00e8s : http://localhost:8080</p> <p></p> <p>Grafana n\u00e9cessite un compte (login, mot de passe) pour se connecter. Ce compte est stock\u00e9 dans le namespace sous la forme d'un 'secret' : </p> codesecrets list'kube-prometheus-stack-grafana' secretadmin credentials, base64 hashedadmin loginadmin password <pre><code># liste des secrets dans le namespace 'monitoring' :\nkubectl -n monitoring get secrets\n\n# d\u00e9tails du secret 'kube-prometheus-stack-grafana' :\nkubectl -n monitoring get secret kube-prometheus-stack-grafana -o yaml\n\n# r\u00e9cup\u00e9ration des secrets en base 64 :\nkubectl -n monitoring get secret kube-prometheus-stack-grafana -o json | jq -r '.data'\n\n# obtention du login admin :\nkubectl -n monitoring get secret kube-prometheus-stack-grafana -o jsonpath='{.data.admin-user}' | base64 -d\n\n# obtention du mot de passe admin :\nkubectl -n monitoring get secret kube-prometheus-stack-grafana -o jsonpath='{.data.admin-password}' | base64 -d\n</code></pre> <pre><code>NAME                                                           TYPE                 DATA   AGE\nalertmanager-kube-prometheus-stack-alertmanager                Opaque               1      13h\nalertmanager-kube-prometheus-stack-alertmanager-generated      Opaque               1      13h\nalertmanager-kube-prometheus-stack-alertmanager-tls-assets-0   Opaque               0      13h\nalertmanager-kube-prometheus-stack-alertmanager-web-config     Opaque               1      13h\ndiscord-webhook                                                Opaque               1      13h\nkube-prometheus-stack-admission                                Opaque               3      16h\nkube-prometheus-stack-grafana                                  Opaque               3      13h\nprometheus-kube-prometheus-stack-prometheus                    Opaque               1      13h\nprometheus-kube-prometheus-stack-prometheus-tls-assets-0       Opaque               1      13h\nprometheus-kube-prometheus-stack-prometheus-web-config         Opaque               1      13h\nsh.helm.release.v1.kube-prometheus-stack.v1                    helm.sh/release.v1   1      13h  \n</code></pre> <pre><code>apiVersion: v1\ndata:\n  admin-password: cHJvbS1vcGVyYXRvcg==\n  admin-user: YWRtaW4=\n  ldap-toml: \"\"\nkind: Secret\nmetadata:\n  annotations:\n    meta.helm.sh/release-name: kube-prometheus-stack\n    meta.helm.sh/release-namespace: monitoring\n  creationTimestamp: \"2024-05-24T20:35:45Z\"\n  labels:\n    app.kubernetes.io/instance: kube-prometheus-stack\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/version: 10.4.1\n    helm.sh/chart: grafana-7.3.11\n    helm.toolkit.fluxcd.io/name: kube-prometheus-stack\n    helm.toolkit.fluxcd.io/namespace: monitoring\n  name: kube-prometheus-stack-grafana\n  namespace: monitoring\n  resourceVersion: \"4610\"\n  uid: 69b6c201-f329-44d2-86dc-f588851b3d8f\ntype: Opaque\n</code></pre> <pre><code>{\n  \"admin-password\": \"cHJvbS1vcGVyYXRvcg==\",\n  \"admin-user\": \"YWRtaW4=\",\n  \"ldap-toml\": \"\"\n}\n</code></pre> <pre><code>admin%\n</code></pre> <pre><code>prom-operator%\n</code></pre> <p>Nous pouvons d\u00e9sormais ouvrir une sessions :</p> <p></p> <p>Par d\u00e9faut, la stack 'kube-prometheus-stack' propose un grand nombre de dashboards :</p> <p></p> <p>Nous ne passerons pas en revue tous les dashboards, mais voici 2 exemples de dashboards fournis par d\u00e9faut avec la stack :</p> <p>Kubernetes / Compute Resources / Cluster</p> <p></p> <p>Node Exporter / Nodes</p> <p></p> <p>----- TODO -----</p> <ul> <li>Se poser la question de prot\u00e9ger les secrets avec Vault</li> <li>une fois les secrets dans Vault, d\u00e9truire le cluster et le re-cr\u00e9er avec les d\u00e9p\u00f4ts 'k8s-kind-fluxcd' et 'k8s-kind-apps' existants -&gt; montrer l'avantage de FluxCD dans le cadre d'un PRA.</li> </ul>"},{"location":"Prometheus_and_Grafana/prometheus_grafana_on_k8s/","title":"Prometheus &amp; Grafana on Kubernetes","text":"<p>Ce howto explique comment d\u00e9ployer Prometheus et Grafana avec Helm.</p>"},{"location":"Prometheus_and_Grafana/prometheus_grafana_on_k8s/#ajout-des-helm-repositories","title":"Ajout des Helm repositories","text":"<pre><code>helm repo add prometheus-community https://prometheus-community.github.io/helm-charts\nhelm repo add grafana https://grafana.github.io/helm-charts\n\nhelm repo list\n\n  # NAME                    URL\n  # prometheus-community    https://prometheus-community.github.io/helm-charts\n  # grafana                 https://grafana.github.io/helm-charts\n\n\nhelm repo update\n\n  # Hang tight while we grab the latest from your chart repositories...\n  # ...Successfully got an update from the \"grafana\" chart repository\n  # ...Successfully got an update from the \"prometheus-community\" chart repository\n  # Update Complete. \u2388Happy Helming!\u2388\n</code></pre>"},{"location":"Prometheus_and_Grafana/prometheus_grafana_on_k8s/#prometheus","title":"Prometheus","text":""},{"location":"Prometheus_and_Grafana/prometheus_grafana_on_k8s/#installation-du-serveur","title":"Installation du serveur","text":"<pre><code># Installation de Prometheus\nhelm install prometheus prometheus-community/prometheus\n\n  NAME: prometheus\n  LAST DEPLOYED: Sat Dec  9 13:26:23 2023\n  NAMESPACE: default\n  STATUS: deployed\n  REVISION: 1\n  TEST SUITE: None\n  NOTES:\n  The Prometheus server can be accessed via port 80 on the following DNS name from within your cluster:\n  prometheus-server.default.svc.cluster.local\n\n\n# Obtention de l'URL du serveur Prometheus\nexport POD_NAME=$(kubectl get pods --namespace default -l \"app.kubernetes.io/name=prometheus,app.kubernetes. instance=prometheus\" -o jsonpath=\"{.items[0].metadata.name}\")\nkubectl --namespace default port-forward ${POD_NAME} 9090\n\n\n# Obtention de l'URL de l'Alert Manager  \nexport POD_NAME=$(kubectl get pods --namespace default -l \"app.kubernetes.io/name=alertmanager,app.kubernetes. instance=prometheus\" -o jsonpath=\"{.items[0].metadata.name}\")\nkubectl --namespace default port-forward ${POD_NAME} 9093\n\n\n# Obtention de l'URL de la Push Gateway  \nexport POD_NAME=$(kubectl get pods --namespace default -l \"app=prometheus-pushgateway,component=pushgateway\"-ojsonpath=\"{.items[0].metadata.name}\")\nkubectl --namespace default port-forward $POD_NAME 9091\n</code></pre>"},{"location":"Prometheus_and_Grafana/prometheus_grafana_on_k8s/#verification-de-linstallation","title":"V\u00e9rification de l'installation","text":"<pre><code>kubectl get all\n\n  NAME                                                     READY   STATUS    RESTARTS   AGE\n  pod/prometheus-alertmanager-0                            1/1     Running   0          9m44s\n  pod/prometheus-kube-state-metrics-85596bfdb6-6r4pp       1/1     Running   0          9m44s\n  pod/prometheus-prometheus-node-exporter-w5skp            1/1     Running   0          9m44s\n  pod/prometheus-prometheus-pushgateway-79745d4495-dh8cv   1/1     Running   0          9m44s\n  pod/prometheus-server-fd677cd4c-5sc5x                    2/2     Running   0          9m44s\n\n  NAME                                          TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE\n  service/kubernetes                            ClusterIP   10.96.0.1       &lt;none&gt;        443/TCP    44m\n  service/prometheus-alertmanager               ClusterIP   10.96.135.67    &lt;none&gt;        9093/TCP   9m44s\n  service/prometheus-alertmanager-headless      ClusterIP   None            &lt;none&gt;        9093/TCP   9m44s\n  service/prometheus-kube-state-metrics         ClusterIP   10.96.78.11     &lt;none&gt;        8080/TCP   9m44s\n  service/prometheus-prometheus-node-exporter   ClusterIP   10.96.170.181   &lt;none&gt;        9100/TCP   9m44s\n  service/prometheus-prometheus-pushgateway     ClusterIP   10.96.148.56    &lt;none&gt;        9091/TCP   9m44s\n  service/prometheus-server                     ClusterIP   10.96.118.59    &lt;none&gt;        80/TCP     9m44s  \n  NAME                                                 DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE\n  daemonset.apps/prometheus-prometheus-node-exporter   1         1         1       1            1           kubernetes.io/os=linux   9m44s  \n  NAME                                                READY   UP-TO-DATE   AVAILABLE   AGE\n  deployment.apps/prometheus-kube-state-metrics       1/1     1            1           9m44s\n  deployment.apps/prometheus-prometheus-pushgateway   1/1     1            1           9m44s\n  deployment.apps/prometheus-server                   1/1     1            1           9m44s  \n  NAME                                                           DESIRED   CURRENT   READY   AGE\n  replicaset.apps/prometheus-kube-state-metrics-85596bfdb6       1         1         1       9m44s\n  replicaset.apps/prometheus-prometheus-pushgateway-79745d4495   1         1         1       9m44s\n  replicaset.apps/prometheus-server-fd677cd4c                    1         1         1       9m44s  \n  NAME                                       READY   AGE\n  statefulset.apps/prometheus-alertmanager   1/1     9m44s\n</code></pre>"},{"location":"Prometheus_and_Grafana/prometheus_grafana_on_k8s/#port-forwarding-tcp9090","title":"Port-forwarding (tcp/9090)","text":"<pre><code>Prometheus port-forwarding on port 9090\nexport POD_NAME=$(kubectl get pods --namespace default -l \"app.kubernetes.io/name=prometheus,app.kubernetes.io/instance=prometheus\" -o jsonpath=\"{.items[0].metadata.name}\")\nkubectl --namespace default port-forward $POD_NAME 9090 &amp;\n</code></pre> <p>-&gt; Prometheus est accessible \u00e0 l'adresse suivante : http://localhost:9090 !</p>"},{"location":"Prometheus_and_Grafana/prometheus_grafana_on_k8s/#grafana","title":"Grafana","text":""},{"location":"Prometheus_and_Grafana/prometheus_grafana_on_k8s/#installation","title":"Installation","text":"<pre><code>helm install grafana grafana/grafana\n\n  NAME: grafana\n  LAST DEPLOYED: Sat Dec  9 14:46:26 2023\n  NAMESPACE: default\n  STATUS: deployed\n  REVISION: 1\n  NOTES:\n  1. Get your 'admin' user password by running:  \n     kubectl get secret --namespace default grafana -o jsonpath=\"{.data.admin-password}\" | base64 --decode ; echo\n\n  2. The Grafana server can be accessed via port 80 on the following DNS name from within your cluster:\n     grafana.default.svc.cluster.local\n\n     Get the Grafana URL to visit by running these commands in the same shell:\n       export POD_NAME=$(kubectl get pods --namespace default -l \"app.kubernetes.io/name=grafana,app.kubernetes.io/instance=grafana\" -o jsonpath=\"{.items[0].metadataname}\")\n       kubectl --namespace default port-forward $POD_NAME 3000\n\n  3. Login with the password from step 1 and the username: admin\n  #################################################################################\n  ######   WARNING: Persistence is disabled!!! You will lose your data when   #####\n  ######            the Grafana pod is terminated.                            #####\n  #################################################################################\n</code></pre>"},{"location":"Prometheus_and_Grafana/prometheus_grafana_on_k8s/#port-forwarding-tcp3000","title":"Port-forwarding (tcp/3000)","text":"<pre><code>export POD_NAME=$(kubectl get pods --namespace default -l \"app.kubernetes.io/name=grafana,app.kubernetes.io/instance=grafana\" -o jsonpath=\"{.items[0].metadata.name}\")\nkubectl --namespace default port-forward $POD_NAME 3000 &amp;\n</code></pre> <p>-&gt; Grafana est accessible \u00e0 l'URL suivante : http://localhost:3000</p> <p>Tip</p> <p>Admin password : kubectl get secret --namespace default grafana -o jsonpath=\"{.data.admin-password}\" | base64 --decode ; echo </p>"},{"location":"Prometheus_and_Grafana/prometheus_grafana_on_k8s/#work-in-progress","title":"WORK IN PROGRESS","text":"<pre><code>helm show chart prometheus-community/prometheus\n\n# annotations:\n#   artifacthub.io/license: Apache-2.0\n#   artifacthub.io/links: |\n#     - name: Chart Source\n#       url: https://github.com/prometheus-community/helm-charts\n#     - name: Upstream Project\n#       url: https://github.com/prometheus/prometheus\n# apiVersion: v2\n# appVersion: v2.48.0\n# dependencies:\n# - condition: alertmanager.enabled\n#   name: alertmanager\n#   repository: https://prometheus-community.github.io/helm-charts\n#   version: 1.7.*\n# - condition: kube-state-metrics.enabled\n#   name: kube-state-metrics\n#   repository: https://prometheus-community.github.io/helm-charts\n#   version: 5.15.*\n# - condition: prometheus-node-exporter.enabled\n#   name: prometheus-node-exporter\n#   repository: https://prometheus-community.github.io/helm-charts\n#   version: 4.24.*\n# - condition: prometheus-pushgateway.enabled\n#   name: prometheus-pushgateway\n#   repository: https://prometheus-community.github.io/helm-charts\n#   version: 2.4.*\n# description: Prometheus is a monitoring system and time series database.\n# home: https://prometheus.io/\n# icon: https://raw.githubusercontent.com/prometheus/prometheus.github.io/master/assets/prometheus_logo-cb55bb5c346.png\n# keywords:\n# - monitoring\n# - prometheus\n# kubeVersion: '&gt;=1.19.0-0'\n# maintainers:\n# - email: gianrubio@gmail.com\n#   name: gianrubio\n# - email: zanhsieh@gmail.com\n#   name: zanhsieh\n# - email: miroslav.hadzhiev@gmail.com\n#   name: Xtigyro\n# - email: naseem@transit.app\n#   name: naseemkullah\n# - email: rootsandtrees@posteo.de\n#   name: zeritti\n# name: prometheus\n# sources:\n# - https://github.com/prometheus/alertmanager\n# - https://github.com/prometheus/prometheus\n# - https://github.com/prometheus/pushgateway\n# - https://github.com/prometheus/node_exporter\n# - https://github.com/kubernetes/kube-state-metrics\n# type: application\n# version: 25.8.1\n\n# To unsinstall Prometheus\nhelm uninstall prometheus\n\n# To get the default helm configuration :\nhelm show values prometheus-community/prometheus &gt; helm_values.prometheus.yaml.ORIG\ncp helm_values.prometheus.yaml.ORIG helm_values.prometheus.yaml\n\n# To install Prometheus with the default configuration :\nhelm install prometheus prometheus-community/prometheus\n\n# To install Prometheus with a custom configuration :\nhelm install prometheus prometheus-community/prometheus -f helm_values.prometheus.yaml\n\n# To upgrade an already installed Prometheus with a custom configuration :\nhelm upgrade prometheus prometheus-community/prometheus -f helm_values.prometheus.yaml\n\n\nhelm history prometheus                                                                                              \ue0b2 1 \u2718 \ue0b3 \uf013 \ue0b3 kind-sandbox \u2388 \ue0b3 16:47:23 \uf017\n# REVISION  UPDATED                     STATUS      CHART               APP VERSION DESCRIPTION\n# 1         Sat Dec  9 16:34:59 2023    superseded  prometheus-25.8.1   v2.48.0     Install complete\n# 2         Sat Dec  9 16:46:39 2023    deployed    prometheus-25.8.1   v2.48.0     Upgrade complete\n\nhelm rollback prometheus 1\n</code></pre>"},{"location":"Prometheus_and_Grafana/prometheus_grafana_on_k8s/#customiser-un-helm-chart-a-partir-des-fichiers-source","title":"Customiser un Helm Chart \u00e0 partir des fichiers source","text":""},{"location":"Prometheus_and_Grafana/prometheus_grafana_on_k8s/#chart-download-locally","title":"Chart download locally","text":"<pre><code>helm pull prometheus-community/prometheus --untar\n</code></pre>"},{"location":"Prometheus_and_Grafana/prometheus_grafana_on_k8s/#install-a-chart-from-local-files","title":"Install a Chart from local files","text":"<pre><code>helm install prometheus --dry-run ./prometheus -f helm_values.prometheus.yaml\n</code></pre>"},{"location":"Prometheus_and_Grafana/prometheus_grafana_on_k8s/#blah-blah-blah","title":"blah blah blah","text":"<p>k exec -it prometheus-server-fd677cd4c-7t8b8 -- sh ps /bin/prometheus-config-reloader --watched-dir=/etc/config --reload-url=http://127.0.0.1:9090/-/reload</p>"},{"location":"Prometheus_and_Grafana/prometheus_grafana_on_k8s/#cours-helm-sur-pluralsight","title":"Cours Helm sur Pluralsight","text":""},{"location":"Prometheus_and_Grafana/prometheus_grafana_on_k8s/#pour-tester-les-helms","title":"Pour tester les helms :","text":"<pre><code>helm template [chart] (works 'offline', without kubernetes)\nhelm install [release] [chart] --dry-run --debug 2&gt;&amp;1       (real helm install but without commit)\nhelm get all [release] -&gt; compiles all the values\n</code></pre>"},{"location":"Vault/cluster_minikube_vault_with_raft_storage/","title":"Cluster [kind|minikube] - HashiCorp Vault with Raft integrated storage","text":"Description URL Vault installation to minikube via Helm with Integrated Storage https://developer.hashicorp.com/vault/tutorials/kubernetes/kubernetes-minikube-raft Vault : Kubernetes auth method https://developer.hashicorp.com/vault/docs/auth/kubernetes Configure Hashicorp's Vault for Kubernetes Auth https://docs.armory.io/continuous-deployment/armory-admin/secrets/vault-k8s-configuration/"},{"location":"Vault/cluster_minikube_vault_with_raft_storage/#installation-de-vault-avec-helm","title":"Installation de Vault avec Helm","text":""},{"location":"Vault/cluster_minikube_vault_with_raft_storage/#deploiement-de-la-helm-release","title":"D\u00e9ploiement de la Helm release","text":"<pre><code>helm repo add hashicorp https://helm.releases.hashicorp.com\nhelm repo update\nhelm search repo hashicorp/vault\nhelm show values hashicorp/vault # affiche toutes les 'values' surchargeables.\n\ncat &gt; helm-vault-raft-values.yml &lt;&lt;EOF\nserver:\n  affinity: \"\"\n  ha:\n    enabled: true\n    raft: \n      enabled: true\nEOF\n\nhelm install vault hashicorp/vault --values helm-vault-raft-values.yml\n</code></pre>"},{"location":"Vault/cluster_minikube_vault_with_raft_storage/#verification-du-bon-fonctionnement-des-pods-vault","title":"V\u00e9rification du bon fonctionnement des pods Vault","text":"<pre><code>kubectl get po -l app.kubernetes.io/name=vault\n\nNAME      READY   STATUS    RESTARTS   AGE\nvault-0   0/1     Running   0          4m8s\nvault-1   0/1     Running   0          4m6s\nvault-2   0/1     Running   0          4m5s\n</code></pre>"},{"location":"Vault/cluster_minikube_vault_with_raft_storage/#vault-init-vault-unseal","title":"Vault init | Vault unseal","text":""},{"location":"Vault/cluster_minikube_vault_with_raft_storage/#initialisation-de-vault-0-avec-1-key-share-et-1-key-threshold","title":"Initialisation de vault-0 avec 1 'key share' et 1 'key threshold'","text":"<p>note : cette approche n'est pas compatible avec de la production '</p> <pre><code>kubectl exec vault-0 -- vault operator init -key-shares=1 -key-threshold=1 -format=json &gt; cluster-keys.json\n\ncat cluster-keys.json \n{\n  \"unseal_keys_b64\": [\n    \"4HrwZMZahnLrsdpxI8WHyNDRwc/S7kHZyILIL5HT5Mw=\"\n  ],\n  \"unseal_keys_hex\": [\n    \"e07af064c65a8672ebb1da7123c587c8d0d1c1cfd2ee41d9c882c82f91d3e4cc\"\n  ],\n  \"unseal_shares\": 1,\n  \"unseal_threshold\": 1,\n  \"recovery_keys_b64\": [],\n  \"recovery_keys_hex\": [],\n  \"recovery_keys_shares\": 0,\n  \"recovery_keys_threshold\": 0,\n  \"root_token\": \"hvs.O8Lr0M0YtUG2T8WUJYDNerLs\"\n}\n\nVAULT_UNSEAL_KEY=$(jq -r \".unseal_keys_b64[]\" cluster-keys.json)        # -&gt; 4HrwZMZahnLrsdpxI8WHyNDRwc/S7kHZyILIL5HT5Mw=\n\nkubectl exec vault-0 -- vault operator unseal $VAULT_UNSEAL_KEY\nkubectl exec -ti vault-1 -- vault operator raft join http://vault-0.vault-internal:8200\nkubectl exec -ti vault-2 -- vault operator raft join http://vault-0.vault-internal:8200\nkubectl exec -ti vault-1 -- vault operator unseal $VAULT_UNSEAL_KEY\nkubectl exec -ti vault-2 -- vault operator unseal $VAULT_UNSEAL_KEY\n</code></pre>"},{"location":"Vault/cluster_minikube_vault_with_raft_storage/#creation-dun-secret","title":"Cr\u00e9ation d'un Secret","text":"<pre><code>ROOT_TOKEN=$( jq -r \".root_token\" cluster-keys.json )       # hvs.O8Lr0M0YtUG2T8WUJYDNerLs\nkubectl exec --stdin=true --tty=true vault-0 -- /bin/sh\n\n  # Dans le pod 'vault-0' :\n\n  vault login ${ROOT_TOKEN}                                   # entrer le token root \n  vault secrets enable -path=secret kv-v2\n  vault kv put secret/wordpress/mysql username=\"mysql-account\" password=\"mysql-account-password\"\n\n  ======= Secret Path =======\n  secret/data/wordpress/mysql\n\n  ======= Metadata =======\n  Key                Value\n  ---                -----\n  created_time       2023-09-05T13:35:01.042933683Z\n  custom_metadata    &lt;nil&gt;\n  deletion_time      n/a\n  destroyed          false\n  version            1\n\n\n  vault kv get secret/wordpress/mysql\n\n  ======= Secret Path =======\n  secret/data/wordpress/mysql\n\n  ======= Metadata =======\n  Key                Value\n  ---                -----\n  created_time       2023-09-05T13:35:01.042933683Z\n  custom_metadata    &lt;nil&gt;\n  deletion_time      n/a\n  destroyed          false\n  version            1\n\n  ====== Data ======\n  Key         Value\n  ---         -----\n  password    mysql-account-password\n  username    mysql-account\n\n  exit                    # exit session from pod vault-0\n</code></pre>"},{"location":"Vault/cluster_minikube_vault_with_raft_storage/#configuration-de-lauthentification-kubernetes","title":"Configuration de l'authentification Kubernetes","text":"<pre><code>kubectl exec --stdin=true --tty=true vault-0 -- /bin/sh\n\n  # Dans le pos 'vault-0' :\n\n  vault auth enable kubernetes\n  vault write auth/kubernetes/config kubernetes_host=\"https://${KUBERNETES_PORT_443_TCP_ADDR}:443\"\n  vault read auth/kubernetes/config\n\n  Key                       Value\n  ---                       -----\n  disable_iss_validation    true\n  disable_local_ca_jwt      false\n  issuer                    n/a\n  kubernetes_ca_cert        n/a\n  kubernetes_host           https://10.96.0.1:443\n  pem_keys                  []\n\n\n  vault policy write wordpress - &lt;&lt;EOF\n  path \"secret/*\" {\n      capabilities = [\"list\"]\n  }\n  path \"secret/data/wordpress/*\" {\n    capabilities = [\"read\",\"list\"]\n  }\n  EOF\n\n\nvault policy read wordpress\npath \"secret/data/wordpress/mysql\" {\n capabilities = [\"read\"]\n}\n\n\nvault write auth/kubernetes/role/wordpress \\\n  bound_service_account_names=vault \\\n  bound_service_account_namespaces=default \\\n  policies=wordpress ttl=24h\n</code></pre>"},{"location":"Vault/cluster_minikube_vault_with_raft_storage/#deploiement-dun-pod-avec-le-service-account-vault","title":"D\u00e9ploiement d'un pod avec le service-account 'vault'","text":"<pre><code>echo &gt; fedora.deployment.yml &lt;&lt; EOF\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: fedora\n  name: fedora\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: fedora\n  template:\n    metadata:\n      creationTimestamp: null\n      labels:\n        app: fedora\n    spec:\n      serviceAccountName: vault\n      containers:\n      - image: fedora\n        name: fedora\n        command: ['sleep', '10000']\nEOF\n\n# V\u00e9rification :\nkubectl get deployments,pods -l app=fedora\n\nNAME                     READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/fedora   1/1     1            1           168m\n\nNAME                         READY   STATUS    RESTARTS       AGE\npod/fedora-5c4dc7445-697pm   1/1     Running   1 (106s ago)   168m\n</code></pre>"},{"location":"Vault/cluster_minikube_vault_with_raft_storage/#sassurer-que-le-pod-peut-bien-acceder-au-secret-wordpressmysql-dans-vault","title":"S'assurer que le pod peut bien acc\u00e9der au secret 'wordpress/mysql' dans Vault","text":"<pre><code># Connexion au pod :\nkubectl exec --tty --stdin fedora-5c4dc7445-697pm -- /bin/bash\n\n# Installation de Vault : \ndnf install -y dnf-plugins-core\ndnf config-manager --add-repo https://rpm.releases.hashicorp.com/fedora/hashicorp.repo\ndnf -y install vault\n\n# Test du bon acc\u00e8s au secret 'wordpress/mysql'\n\nexport VAULT_ADDR=\"http://${VAULT_SERVICE_HOST}:${VAULT_SERVICE_PORT}\"\nSA_TOKEN=$( cat /var/run/secrets/kubernetes.io/serviceaccount/token )\nVAULT_TOKEN=$( vault write auth/kubernetes/login role=wordpress jwt=${SA_TOKEN} | grep -w ^token | awk '{print $2}' )\n\nvault login ${VAULT_TOKEN}\nvault kv list -mount=secret wordpress\nvault kv get -mount=secret wordpress/mysql      # m\u00eame chose que \"vault kv get /secret/wordpress/mysql\"\n</code></pre>"},{"location":"Vault/helm_vault_auto-unseal/","title":"Auto-unsealed Vault Helm deployment managed with FluxCD","text":""},{"location":"Vault/helm_vault_auto-unseal/#abstract","title":"Abstract","text":"<p>Dans un premier temps, nous tenterons de d\u00e9ployer Vault \u00e0 partir du Helm chart officiel en mode auto-unseal. Nous choisirons GCP KMS pour cela. Lorsque nous arriverons \u00e0 nos fins, nous configurerons FluxCD pour qu'il g\u00e8re son d\u00e9ploiement tout seul.</p>"},{"location":"Vault/helm_vault_auto-unseal/#docs-de-reference","title":"Docs de r\u00e9f\u00e9rence","text":"<ul> <li>https://developer.hashicorp.com/vault/docs/platform/k8s/helm</li> <li>https://developer.hashicorp.com/vault/tutorials/kubernetes/kubernetes-minikube-consul</li> <li>https://developer.hashicorp.com/vault/tutorials/auto-unseal/autounseal-gcp-kms</li> <li>https://developer.hashicorp.com/vault/tutorials/kubernetes/kubernetes-raft-deployment-guide</li> <li>https://gist.github.com/sdeoras/96e78780561b1e941e8d5c4d3a78b7e9</li> <li>https://developer.hashicorp.com/vault/docs/configuration/seal/gcpckms</li> </ul>"},{"location":"Vault/helm_vault_auto-unseal/#pre-requis-kubernetes-cluster-local","title":"Pr\u00e9-requis Kubernetes (cluster local)","text":"<ul> <li>un cluster Kind op\u00e9rationnel, </li> <li>FluxCD d\u00e9ploy\u00e9,</li> <li>Helm install\u00e9,</li> <li>kubectl install\u00e9.</li> </ul>"},{"location":"Vault/helm_vault_auto-unseal/#pre-requis-gcp","title":"Pr\u00e9-requis GCP","text":"<p>Acc\u00e8s \u00e0 la console GCP :  https://console.cloud.google.com</p> <ul> <li>project</li> <li>project name : vault</li> <li>project ID : vault-415918</li> <li> <p>project number : 226383909329</p> </li> <li> <p>service account</p> </li> <li>service account name : sa-vault</li> <li>service account ID : 114299537044679868050</li> <li>email address : sa-vault@vault-415918.iam.gserviceaccount.com</li> <li>private key (json file) : /Users/franck/Downloads/credentials.json </li> </ul> <pre><code>cat /Users/franck/Downloads/credentials.json\n\n {\n  \"type\": \"service_account\",\n  \"project_id\": \"vault-415918\",\n  \"private_key_id\": \"0f05bb392e8450a34f96b0fe813137ca06210a8b\",\n  \"private_key\": \"-----BEGIN PRIVATE KEY-----\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQDWvrEgapjonPkm\\n32zmC6Dyo/PyXSnyfaiNQ4TYL0HWvExXkal9P8rsvyK+I+tyEEZuCMzGjlzS9v1e\\nlAwwE3gcTQ1wuS    #  +eaMVOTy4FD4cdjsJMenRTolfRVLFCO5McTAsHwwLhirOQbeNb\\nLehLagxEN3q0rzG0eFHpVFL1p9T29hcnFmCLPUWBGyliAUY1d8DThyr7KoxT3ShG\\n3vhOnsNVPMw+30EC    #  +JITZbqx3XBuiIAjLDctWOsWr34yHLkmdDQ6tepU7DW5Vv26\\ngNjEoseXfs59gya2g15CYwMqex+1iE8v6lwFbP2y/iydr6jbK9xR9j5EQtQjjkr4\\nwfD6Rg9BAgMBAAECggEAGzlQ5+TQ/mmquxSHZWFDWzS7ysBS1Ay9dIMtxou0fg5L\\nDgLyXhAwn/    #  OElQmlYfAm7ZuB/Qiz7dl6uiXB/HT2Eihr3sbV/vAWALJ7CWXe4Y66\\nmnV1D9vnOYDSJAJnc6aUSLyekzdBQmXGn/A29cXmSN5RA7JTdnyWbc0kje4On+wh\\nWedadJMWzNFq9y1K6pLfWQINUzRYqlmexqfSYbEzoUgYHCk0PhHbk5+fTV8JU+Id\\n1E    #  +jtxnnvowi6b9002Zeoxrb2u9kKm/vaxGug40LiWNLguR6UWAkGR377LDXOe2S\\nxLJBz1IW9uKO5b7Mcn2xJD7+05UwRefe9JhqupqmLQKBgQDsLGF6gOW/60wXJ4Oe\\nsBhjc6XYcdjKUtXfHTnHE5wMcUVJo+88dEmF0df+6Kr5GAYEsqn    #  +0CzgdeM1zQDY\\nbkuogaN79A7h0tm+jzcIu4FuJG6OCqCekLlZ/b49DvQJWF0uqKXID8Q6Ai5zU4+2\\nFL0oKzDT/OFKLNsjVFIy1A8KuwKBgQDoxcpNW1IbmoGVp/Bdv/a3xYnjEdQedgEU\\nDdglCJK    #  +C2zvQJX4r6ZmhYzx6c6KWgd3naetUL7rPKFjUX2Tnu1f31tIhChiO8Eh\\niaSpGiRdx9quq1qpvNkurbto0Ublt92siCPe0OcxIIAsOytqNzuD2YfJvQ2hfb8N\\nf795sZQEMwKBgQDSp+NquX40aVQ9culbqgaW7piHL0UHckuB7zeR8lPGZWJABRFn\\nAvJxgnL    #  +09lsxZjYp+QpfNYKgBxh6LFQW1DwxHFmJpL/qmq+JlAYYedYrvZNi/0o\\ncj5hnosJO0VA8Khs7dCxWh7U/w0foPEWn/j4002CSJVK7Ceqo5ON8shX8QKBgGve\\n9VCCCHv4TyMuj4qyokAp0CuloHp5Tyie/dKztWVS4CnD8Xws0l1ieJ3HL0sYS6uY\\nKRN9fux    #  +zX+8TQizNugeFyx06k4TyP2kzuT6022OZ35YtIxCkxc5tcbubP+aBKWm\\n9ZCVmP5ARIW66fSwIemJTo8kCIQVRQuZbv+TVrfXAoGAA46OjdCsPVbQOI/agbsK\\nC3EGrBYz2g76i4t9wUoHMUjbZ1Jul/X7YXWBfKqAiimZuBBxkLKirKAfGLKGJovg\\n/HWop    #  +llvqJxwZtQkMzkyVG/J9R2N41wBf5exT/aDSdsoZXqkqW2oiFlO5nBt+03\\nQjcnzsHFcLrp3qzBco4aL04=\\n-----END PRIVATE KEY-----\\n\",\n  \"client_email\": \"sa-vault@vault-415918.iam.gserviceaccount.com\",\n  \"client_id\": \"114299537044679868050\",\n  \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n  \"token_uri\": \"https://oauth2.googleapis.com/token\",\n  \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n  \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/sa-vault%40vault-415918.iam.gserviceaccount.com\",\n  \"universe_domain\": \"googleapis.com\"\n}\n</code></pre> <ul> <li>cr\u00e9ation du key ring : vault-helm-unseal-kr</li> <li> <p>cr\u00e9ation de la cl\u00e9 : vault-helm-unseal-key   -&gt; grant access sur la cl\u00e9 vault-helm-unseal-key au principal 'sa-vault' avec les r\u00f4les ' Cloud KMS Viewer' et 'Cloud KMS CryptoKey Encrypter/Decrypter'</p> </li> <li> <p>activation des APIs :</p> </li> <li>Cloud Key Management Service (KMS) API : enabled</li> <li> <p>Compute Engine API : enabled (pour d\u00e9finir la r\u00e9gion/zone)</p> </li> <li> <p>d\u00e9finition de la r\u00e9gion et de la zone :</p> </li> <li>via la console :<ul> <li>https://console.cloud.google.com/compute/settings?hl=fr&amp;project=vault-415918</li> <li>r\u00e9gion : europe-west9 (Paris)</li> <li>zone: europe-west9-a</li> </ul> </li> <li>via la CLI :</li> </ul> <pre><code>gcloud auth login\ngcloud compute project-info describe --project vault-415918\ngcloud config set project vault-415918\ngcloud services enable compute.googleapis.com cloudkms.googleapis.com\ngcloud config set compute/region europe-west9\ngcloud config set compute/zone europe-west9-a\ngcloud config list\n</code></pre> <ul> <li>cr\u00e9ation d'un secret Kubernetes pour la cl\u00e9 priv\u00e9e du service account 'sa-vault' <pre><code>kubectl -n vault create secret generic kms-creds --from-file=/Users/franck/Downloads/credentials.json\nkubectl get secret kms-creds -o yaml | yq -r '.data' | awk '{print $2}' | base64 -d | jq -r '.'\n\n-&gt; m\u00eame sortie que : cat /Users/franck/Downloads/credentials.json \n</code></pre></li> </ul>"},{"location":"Vault/helm_vault_auto-unseal/#deploiement-manuel-de-vault-en-mode-auto-unseal-depuis-le-helm-chart-officiel","title":"D\u00e9ploiement manuel de Vault en mode auto-unseal depuis le Helm Chart officiel","text":""},{"location":"Vault/helm_vault_auto-unseal/#helm-repository-hashicorp","title":"Helm repository HashiCorp","text":"<pre><code>helm repo add hashicorp https://helm.releases.hashicorp.com\n\nhelm search repo hashicorp\nhelm show values hashicorp/vault\n\nkubectl create namespace vault\nkubens vault\n\nhelm search repo hashicorp/vault --versions\nhelm install vault hashicorp/vault --namespace vault --dry-run\n</code></pre>"},{"location":"Vault/helm_vault_auto-unseal/#ecriture-des-custom-values-pour-activer-lauto-unseal","title":"Ecriture des custom values pour activer l'auto-unseal","text":"<pre><code>cat custom-values.txt\n\nglobal:\n  enabled: false\n  namespace: \"vault\"\ninjector:\n  enabled: false\nserver:\n  enabled: true\n  # Used to define commands to run after the pod is ready.\n  # This can be used to automate processes such as initialization\n  # or boostrapping auth methods.\n  postStart: []\n  # - /bin/sh\n  # - -c\n  # - /vault/userconfig/myscript/run.sh\n  extraEnvironmentVars:\n    GOOGLE_REGION: europe-west9\n    GOOGLE_PROJECT: vault-415918\n    GOOGLE_APPLICATION_CREDENTIALS: /vault/userconfig/kms-creds/credentials.json\n  extraVolumes:\n    - type: secret\n      name: kms-creds\n      path: /vault/userconfig # default is `/vault/userconfig`\n  standalone:\n    config: |\n      ui = true\n\n      listener \"tcp\" {\n        tls_disable = 1\n        address = \"[::]:8200\"\n        cluster_address = \"[::]:8201\"\n        # Enable unauthenticated metrics access (necessary for Prometheus Operator)\n        #telemetry {\n        #  unauthenticated_metrics_access = \"true\"\n        #}\n      }\n      storage \"file\" {\n        path = \"/vault/data\"\n      }\n      seal \"gcpckms\" {\n         project     = \"vault-415918\"\n         region      = \"europe-west9\"\n         key_ring    = \"vault-helm-unseal-kr\"\n         crypto_key  = \"vault-helm-unseal-key\"\n      }\n  #ha:\n  #  enabled: true\n  #  replicas: 1\n  #  raft:\n  #    enabled: true\n  serviceAccount:\n    create: true\n    name: \"vault\"\nui:\n  # True if you want to create a Service entry for the Vault UI.\n  #\n  # serviceType can be used to control the type of service created. For\n  # example, setting this to \"LoadBalancer\" will create an external load\n  # balancer (for supported K8S installations) to access the UI.\n  enabled: false\n</code></pre>"},{"location":"Vault/helm_vault_auto-unseal/#deploiement-manuel-de-vault-en-mode-auto-unseal","title":"D\u00e9ploiement manuel de Vault en mode auto-unseal","text":"<pre><code>helm instal vault hashicorp/vault -f values.yml --dry-run\nkubens vault\nkubectl get all\n\nkubectl logs vault-0\n\n  # 2024-03-02T13:42:57.055Z [INFO]  proxy environment: http_proxy=\"\" https_proxy=\"\" no_proxy=\"\"\n  # 2024-03-02T13:42:57.245Z [INFO]  incrementing seal generation: generation=1\n  # 2024-03-02T13:42:57.246Z [INFO]  core: Initializing version history cache for core\n  # 2024-03-02T13:42:57.246Z [INFO]  events: Starting event system\n  # 2024-03-02T13:42:57.247Z [INFO]  core: stored unseal keys supported, attempting fetch\n  # 2024-03-02T13:42:57.247Z [WARN]  failed to unseal core: error=\"stored unseal keys are supported, but none were found\"\n  # 2024-03-02T13:43:01.821Z [INFO]  core: security barrier not initialized\n  # 2024-03-02T13:43:01.821Z [INFO]  core.autoseal: recovery seal configuration missing, but cannot check old path as core is sealed\n\nkubectl exec -it vault-0 -- vault status\n\n  # Key                      Value\n  # ---                      -----\n  # Recovery Seal Type       gcpckms\n  # Initialized              false\n  # Sealed                   true\n  # Total Recovery Shares    0\n  # Threshold                0\n  # Unseal Progress          0/0\n  # Unseal Nonce             n/a\n  # Version                  1.15.2\n  # Build Date               2023-11-06T11:33:28Z\n  # Storage Type             file\n  # HA Enabled               false\n  # command terminated with exit code 2\n\nkubectl exec vault-0 -- vault operator init\n\n  # Recovery Key 1: BE9yVRP/GNAbb2cIOccb+0e9S8hF9QTOYxqfDq14JdsU\n  # Recovery Key 2: LDbM7aYBpEWsW28Ul+aLiaSzzTqMplk8KviKI9IJNE5V\n  # Recovery Key 3: 3c6lgD82bct7/maaS5HJ+Z/Q3y5IAmeAU+UcNW3eoDOy\n  # Recovery Key 4: XuG7btTetf/ZAIaDxQoM8+qn79GDFA0uXArBq5OBM+kx\n  # Recovery Key 5: 7xRR+XEYZRNwfhrQiaflUVj+6BPLzUlqHuwG4aqxZMOT\n  # \n  # Initial Root Token: hvs.G145zNl012ApNOap3sn2zhIG\n  # \n  # Success! Vault is initialized\n  # \n  # Recovery key initialized with 5 key shares and a key threshold of 3. Please\n  # securely distribute the key shares printed above.\n\n\nkubectl exec -it vault-0 -- vault status\n\n  # Key                      Value\n  # ---                      -----\n  # Recovery Seal Type       shamir\n  # Initialized              true\n  # Sealed                   false\n  # Total Recovery Shares    5\n  # Threshold                3\n  # Version                  1.15.2\n  # Build Date               2023-11-06T11:33:28Z\n  # Storage Type             file\n  # Cluster Name             vault-cluster-6fa0df73\n  # Cluster ID               b26de2f1-d9e7-8225-ad55-f114b37eeffb\n  # HA Enabled               false\n\nkubectl delete pod vault-0\nkubectl exec -it vault-0 -- vault status\n</code></pre> <p>-&gt; l'auto-unseal a fonctionn\u00e9 !</p>"},{"location":"Vault/helm_vault_auto-unseal/#conclusion","title":"Conclusion","text":"<p>Nous savons d\u00e9sormais comment installer Vault via Helm sur notre cluster Kubernetes en mode auto-unseal. Voyons comment confier sa gestion \u00e0 FluxCD maintenant.</p>"},{"location":"Vault/helm_vault_auto-unseal/#deploiement-de-vault-en-mode-auto-unseal-depuis-le-helm-chart-officiel-et-pilote-par-fluxcd","title":"D\u00e9ploiement de Vault en mode auto-unseal depuis le Helm Chart officiel et pilot\u00e9 par FluxCD","text":""},{"location":"Vault/helm_vault_auto-unseal/#clonage-en-local-du-repository-git-de-fluxcd","title":"Clonage en local du repository Git de FluxCD","text":"<p>Nous devons ajouter les manifests dans le repo Git pilot\u00e9 par FluxCD. Pour identifier ce dernier : kubectl get gitrepository -n flux-system -&gt; https://github.com/papaFrancky/kubernetes-development.git</p> <p>Mettons \u00e0 jour la copie locale de ce repo :  <pre><code>    cd ~/code/github\n    git clone git@github.com:papafrancky/kubernetes-development.git\n    cd kubernetes-development\n    mkdir -p products/vault\n</code></pre></p>"},{"location":"Vault/helm_vault_auto-unseal/#alerting-discord","title":"alerting Discord","text":""},{"location":"Vault/helm_vault_auto-unseal/#creation-du-salon-prive-sur-le-client-discord","title":"cr\u00e9ation du salon priv\u00e9 sur le client Discord","text":"<p>Cr\u00e9ation d'un nouveau salon (priv\u00e9) :    - nom : vault-development   - webhook :     - nom : FluxCD     - URL : https://discord.com/api/webhooks/1213494413511237642/7gRzmfYCwDqWwI2D-1jfLZCNvDBotoe_rY2sson57G1Ya40-EtEMWAZy9FsxmjCZTJ4C</p>"},{"location":"Vault/helm_vault_auto-unseal/#on-place-le-webhook-du-salon-discord-dans-un-secret-kubernetes","title":"on place le webhook du salon discord dans un secret kubernetes","text":"<pre><code>DISCORD_WEBHOOK=\"https://discord.com/api/webhooks/1213494413511237642/7gRzmfYCwDqWwI2D-1jfLZCNvDBotoe_rY2sson57G1Ya40-EtEMWAZy9FsxmjCZTJ4C\"\nkubectl -n vault create secret generic discord-vault-development-webhook --from-literal=address=${DISCORD_WEBHOOK} --dry-run=client -o yaml &gt; products/vault/discord-vault-development-webhook.secret.yaml\n</code></pre>"},{"location":"Vault/helm_vault_auto-unseal/#definition-de-lalert-provider-discord","title":"d\u00e9finition de l'alert-provider Discord","text":"<pre><code>flux create alert-provider discord \\\n  --type=discord \\\n  --secret-ref=discord-vault-development-webhook \\\n  --channel=vault-development \\\n  --username=FluxCD \\\n  --namespace=vault \\\n  --export &gt; products/vault/notification-provider.yaml\n</code></pre>"},{"location":"Vault/helm_vault_auto-unseal/#configuration-des-alertes-discord","title":"configuration des alertes Discord","text":"<pre><code>flux create alert discord \\\n  --event-severity=info \\\n  --event-source='GitRepository/*,Kustomization/*,ImageRepository/*,ImagePolicy/*,HelmRepository/*,HelmRelease/*' \\\n  --provider-ref=discord \\\n  --namespace=vault \\\n  --export &gt; products/vault/notification-alert.yaml\n</code></pre>"},{"location":"Vault/helm_vault_auto-unseal/#poussons-les-manifests-sur-le-repo-central-pour-que-fluxcd-les-gere","title":"Poussons les manifests sur le repo central pour que FluxCD les g\u00e8re :","text":"<pre><code>git add .\ngit commit -m 'feat: configuring discord alerting for vault.'\ngit push\n\nflux reconcile kustomization flux-system --with-source\nflux events -w\n</code></pre>"},{"location":"Vault/helm_vault_auto-unseal/#gestion-du-repo-helm","title":"Gestion du repo Helm","text":"<pre><code>flux create source helm hashicorp \\\n  --url=https://helm.releases.hashicorp.com \\\n  --namespace=vault \\\n  --interval=1m \\\n  --export &gt; products/vault/helm-repository.yml\n</code></pre>"},{"location":"Vault/helm_vault_auto-unseal/#deploiement-de-vault-helm-release","title":"D\u00e9ploiement de Vault (helm release)","text":"<p>Recopier le fichier 'values.yaml' en 'custom-values.txt' (FluxCD ne g\u00e8re que les manifests en YAML)</p> <pre><code>flux create helmrelease vault \\\n  --source=HelmRepository/hashicorp \\\n  --chart=vault \\\n  --values=products/vault/custom-values.txt \\\n  --namespace=vault \\\n  --export &gt; products/vault/helm-release.yaml\n\ngit status\n  # Sur la branche main\n  # Votre branche est \u00e0 jour avec 'origin/main'.\n  # \n  # Fichiers non suivis:\n  #   (utilisez \"git add &lt;fichier&gt;...\" pour inclure dans ce qui sera valid\u00e9)\n  #     products/vault/custom-values.txt\n  #     products/vault/helm-release.yaml\n  #     products/vault/helm-repository.yml\n\ngit add .\ngit commit -m 'feat: managing vault helm deployment.'\ngit push\n\nkubectl get all\n  # NAME          READY   STATUS    RESTARTS   AGE\n  # pod/vault-0   1/1     Running   0          17s\n  # \n  # NAME                     TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)             AGE\n  # service/vault            ClusterIP   10.96.102.150   &lt;none&gt;        8200/TCP,8201/TCP   17s\n  # service/vault-internal   ClusterIP   None            &lt;none&gt;        8200/TCP,8201/TCP   17s\n  # \n  # NAME                     READY   AGE\n  # statefulset.apps/vault   1/1     17s\n\nkubectl exec -it vault-0 -- vault status\n  # Key                      Value\n  # ---                      -----\n  # Recovery Seal Type       shamir\n  # Initialized              true\n  # Sealed                   false\n  # Total Recovery Shares    5\n  # Threshold                3\n  # Version                  1.15.2\n  # Build Date               2023-11-06T11:33:28Z\n  # Storage Type             file\n  # Cluster Name             vault-cluster-6fa0df73\n  # Cluster ID               b26de2f1-d9e7-8225-ad55-f114b37eeffb\n  # HA Enabled               false\n</code></pre> <p>-&gt; C'est GOOD !!! Note : pas besoin d'initialiser notre Vault car la config a \u00e9t\u00e9 r\u00e9cup\u00e9r\u00e9e depuis le volume du statefulset cr\u00e9\u00e9 pr\u00e9alablement.</p>"},{"location":"Vault/kind_vault_external_secrets_operator/","title":"Kind - Vault - External-secrets operator","text":"Description URL Kind install https://kind.sigs.k8s.io/docs/user/quick-start/#installing-from-release-binaries Helm installation https://helm.sh/docs/intro/install/ External-secrets operator installation https://external-secrets.io/latest/introduction/getting-started/ Vault helm installation https://developer.hashicorp.com/vault/tutorials/kubernetes/kubernetes-minikube-raft#install-the-vault-helm-chart Kubectl installation https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/ kubens &amp; kubectx installation https://github.com/ahmetb/kubectx#manual-installation-macos-and-linux External-secrets - Vault provider https://external-secrets.io/latest/provider/hashicorp-vault/ Tutorial: How to Set External-Secrets with Hashicorp Vault https://blog.container-solutions.com/tutorialexternal-secrets-with-hashicorp-vault Vault - Kubernetes authentication https://external-secrets.io/latest/provider/hashicorp-vault/#kubernetes-authentication"},{"location":"Vault/kind_vault_external_secrets_operator/#prerequisites","title":"Prerequisites","text":""},{"location":"Vault/kind_vault_external_secrets_operator/#kind-installation-and-cluster-creation","title":"'Kind' installation and cluster creation","text":"<pre><code>curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.20.0/kind-linux-amd64\nchmod +x ./kind\nsudo mv ./kind /usr/local/bin/kind\nkind --version\nkind create cluster --name sandbox\nkind get clusters\n</code></pre>"},{"location":"Vault/kind_vault_external_secrets_operator/#helm-installation","title":"Helm installation","text":"<pre><code>curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3\nchmod 700 get_helm.sh\n./get_helm.sh\nhelm version\n</code></pre>"},{"location":"Vault/kind_vault_external_secrets_operator/#kubectl-installation","title":"kubectl installation","text":"<pre><code>curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\"\nchmod +x kubectl\nsudo mv kubectl /usr/local/bin/kubectl\nkubectl version\nprintf \"\\nalias k=kubectl\\n\" &gt;&gt; ~/.bashrc &amp;&amp; source ~/.bashrc\nk get po -A\n</code></pre>"},{"location":"Vault/kind_vault_external_secrets_operator/#kubens-and-kubectx-installation","title":"kubens and kubectx installation","text":"<pre><code>sudo git clone https://github.com/ahmetb/kubectx /opt/kubectx\nsudo ln -s /opt/kubectx/kubectx /usr/local/bin/kubectx\nsudo ln -s /opt/kubectx/kubens /usr/local/bin/kubens\n</code></pre>"},{"location":"Vault/kind_vault_external_secrets_operator/#external-secrets-operator-installation","title":"External-secrets operator installation","text":"<pre><code>helm repo add external-secrets https://charts.external-secrets.io\nhelm install external-secrets external-secrets/external-secrets -n external-secrets --create-namespace\n\n# NAME: external-secrets\n# LAST DEPLOYED: Tue Sep 19 16:14:00 2023\n# NAMESPACE: external-secrets\n# STATUS: deployed\n# REVISION: 1\n# TEST SUITE: None\n# NOTES:\n# external-secrets has been deployed successfully!\n# \n# In order to begin using ExternalSecrets, you will need to set up a SecretStore\n# or ClusterSecretStore resource (for example, by creating a 'vault' SecretStore).\n# \n# More information on the different types of SecretStores and how to configure them\n# can be found in our Github: https://github.com/external-secrets/external-secrets\n</code></pre>"},{"location":"Vault/kind_vault_external_secrets_operator/#vault-installation","title":"Vault installation","text":"<pre><code>helm repo add hashicorp https://helm.releases.hashicorp.com\nhelm repo update\nhelm search repo hashicorp/vault\nhelm show values hashicorp/vault\ncat &lt;&lt; EOF &gt; custom-values.yaml\nserver:\n  affinity: \"\"\n  ha:\n    enabled: true\n    replicas: 1\n    raft:\n      enabled: true\nEOF\n\nhelm install vault hashicorp/vault --values custom-values.yaml -n vault --create-namespace\n\n# NAME: vault\n# LAST DEPLOYED: Mon Sep 18 20:23:23 2023\n# NAMESPACE: vault\n# STATUS: deployed\n# REVISION: 1\n# NOTES:\n# Thank you for installing HashiCorp Vault!\n# \n# Now that you have deployed Vault, you should look over the docs on using\n# Vault with Kubernetes available here:\n# \n# https://www.vaultproject.io/docs/\n# \n# \n# Your release is named vault. To learn more about the release, try:\n#\n# helm status vault\n# helm get manifest vault\n\n\nhelm list -A\n\n# NAME                    NAMESPACE               REVISION        UPDATED                                 STATUS          CHART                   APP VERSION\n# external-secrets        external-secrets        1               2023-09-26 19:45:58.063107607 +0000 UTC deployed        external-secrets-0.9.5  v0.9.5     \n# vault                   vault                   1               2023-09-26 19:46:44.752525469 +0000 UTC deployed        vault-0.25.0            1.14.0\n\n\n\nk -n vault get po\n\n# NAME                                    READY   STATUS    RESTARTS   AGE\n# vault-0                                 0/1     Running   0          78s\n# vault-agent-injector-67c48f8f4c-psznb   1/1     Running   0          79s\n</code></pre> <p>export VAULT_ADDR=http://127.0.0.1:8200 k -n vault exec --tty --stdin vault-0 -- /bin/sh </p> <p>vault status</p>"},{"location":"Vault/kind_vault_external_secrets_operator/#vault-initialization-and-unsealing-operations","title":"Vault initialization and unsealing operations","text":""},{"location":"Vault/kind_vault_external_secrets_operator/#loging-into-the-vault-pod","title":"Loging into the Vault pod","text":"<p>In namespace 'vault', the pod 'vault-0' is running but not ready. One must initialize Vault and unseal it. To do so, one must connect to the pod and interact with Vault using the provided CLI :</p> <pre><code>k -n vault --tty --stdin exec vault-0 -- /bin/sh\n</code></pre> <p>All the following operations will be executed in this pod :</p> <pre><code>vault status\n\n# Key                Value\n# ---                -----\n# Seal Type          shamir\n# Initialized        false\n# Sealed             true\n# Total Shares       0\n# Threshold          0\n# Unseal Progress    0/0\n# Unseal Nonce       n/a\n# Version            1.14.0\n# Build Date         2023-06-19T11:40:23Z\n# Storage Type       raft\n# HA Enabled         true\n</code></pre>"},{"location":"Vault/kind_vault_external_secrets_operator/#vault-initialization","title":"Vault initialization","text":"<pre><code>vault operator init -key-shares=1 -key-threshold=1 -format=json\n\n# {\n#   \"unseal_keys_b64\": [\n#     \"/yNqVmr967kJfM2vCjiRC+XFI304XApJDy8b2RVQaLs=\"\n#   ],\n#   \"unseal_keys_hex\": [\n#     \"ff236a566afdebb9097ccdaf0a38910be5c5237d385c0a490f2f1bd9155068bb\"\n#   ],\n#   \"unseal_shares\": 1,\n#   \"unseal_threshold\": 1,\n#   \"recovery_keys_b64\": [],\n#   \"recovery_keys_hex\": [],\n#   \"recovery_keys_shares\": 0,\n#   \"recovery_keys_threshold\": 0,\n#   \"root_token\": \"hvs.fJuVStEaM1h2G66LnefKsDlI\"\n# }\n</code></pre>"},{"location":"Vault/kind_vault_external_secrets_operator/#vault-unsealing","title":"Vault unsealing","text":"<pre><code>vault operator unseal /yNqVmr967kJfM2vCjiRC+XFI304XApJDy8b2RVQaLs=\n\n# Key                     Value\n# ---                     -----\n# Seal Type               shamir\n# Initialized             true\n# Sealed                  false\n# Total Shares            1\n# Threshold               1\n# Version                 1.14.0\n# Build Date              2023-06-19T11:40:23Z\n# Storage Type            raft\n# Cluster Name            vault-cluster-58870b24\n# Cluster ID              39288dfb-5e4e-93f6-a085-83472f00350d\n# HA Enabled              true\n# HA Cluster              https://vault-0.vault-internal:8201\n# HA Mode                 active\n# Active Since            2023-09-26T19:59:18.358850261Z\n# Raft Committed Index    36\n# Raft Applied Index      36\n</code></pre> <p>Vault is fully operational from now. Let's quit the Vault-0 pod :</p> <pre><code>exit\n</code></pre>"},{"location":"Vault/kind_vault_external_secrets_operator/#workshop","title":"Workshop","text":""},{"location":"Vault/kind_vault_external_secrets_operator/#vault-read-all-policy","title":"Vault 'read all' policy","text":"<pre><code># Login into the Vault pod :\nk -n vault --tty --stdin exec vault-0 -- /bin/sh\n\n# Connect to Vault using the root token :\nvault login hvs.fJuVStEaM1h2G66LnefKsDlI\n\n# Write a policy allowing every paths to be read :\nvault policy write read-all - &lt;&lt; EOF     \npath \"*\"                                                  \n{  capabilities = [\"read\"]                \n}                         \nEOF\n\n# Quit the pod :\nexit\n</code></pre>"},{"location":"Vault/kind_vault_external_secrets_operator/#enable-kubernetes-authentication-on-vault","title":"Enable Kubernetes authentication on Vault","text":"<pre><code># Login into the Vault pod :\nk -n vault --tty --stdin exec vault-0 -- /bin/sh\n\n# Enable Kubernetes authentication :\nvault auth enable kubernetes\n\nvault auth list\n\n# Path           Type          Accessor                    Description                Version\n# ----           ----          --------                    -----------                -------\n# kubernetes/    kubernetes    auth_kubernetes_9a149aa4    n/a                        n/a\n# token/         token         auth_token_b498fd96         token based credentials    n/a\n</code></pre>"},{"location":"Vault/kind_vault_external_secrets_operator/#configuration-of-the-kubernetes-authentication","title":"Configuration of the Kubernetes authentication","text":"<pre><code># source: https://developer.hashicorp.com/vault/docs/auth/kubernetes#kubernetes-auth-method\n#\n#   Use local service account token as the reviewer JWT\n#   \n#   When running Vault in a Kubernetes pod the recommended option is to use the pod's local service account token.\n#   Vault will periodically re-read the file to support short-lived tokens. To use the local token and CA certificate,\n#   omit token_reviewer_jwt and kubernetes_ca_cert when configuring the auth method. Vault will attempt to load them\n#   from token and ca.crt respectively inside the default mount folder /var/run/secrets/kubernetes.io/serviceaccount/.\n\n\n# Login into the Vault pod :\nk -n vault --tty --stdin exec vault-0 -- /bin/sh\n\n# Configuration of the Kubernetes authentication :\nvault write auth/kubernetes/config kubernetes_host=https://${KUBERNETES_SERVICE_HOST}:${KUBERNETES_SERVICE_PORT}\n\n# Success! Data written to: auth/kubernetes/config\n# This role authorizes the \"vault\" service account in the vault namespace and it gives it the read-all policy.\n\n# Leave the pod :\nexit\n</code></pre>"},{"location":"Vault/kind_vault_external_secrets_operator/#role-definition-for-the-kubernetes-authentication-role-binding","title":"Role definition for the Kubernetes authentication + role binding","text":"<pre><code># Kubernetes namespace dedicated to our application \"app1\" :\nkubectl create namespace ns-app1\n\n# Kubernetes service-account dedicated to \"app1\" :\nkubectl create sa sa-app1 -n ns-app1\n\n# Role binding \nkubectl create clusterrolebinding sa-app1-tokenreview-access \\\n    --clusterrole=system:auth-delegator \\\n    --serviceaccount=ns-app1:sa-app1\n\n  # clusterrolebinding.rbac.authorization.k8s.io/sa-app1-tokenreview-access created\n\n\n# Login to the Vault pod :\nk -n vault --tty --stdin exec vault-0 -- /bin/sh\n\n# Role definition allowing our service-account from namespace 'ns-app1' to read secrets from Vault :  \nvault write auth/kubernetes/role/vault-read \\\n    bound_service_account_names=sa-app1 \\\n    bound_service_account_namespaces=ns-app1 \\\n    policies=read-all \\\n    ttl=1h\n\n  # Success! Data written to: auth/kubernetes/role/vault-read\n\n\n# Let's check our role 'vault-read' :\nvault read auth/kubernetes/role/vault-read\n\n# Key                                 Value\n# ---                                 -----\n# alias_name_source                   serviceaccount_uid\n# bound_service_account_names         [sa-app1]\n# bound_service_account_namespaces    [ns-app1]\n# policies                            [read-all]\n# token_bound_cidrs                   []\n# token_explicit_max_ttl              0s\n# token_max_ttl                       0s\n# token_no_default_policy             false\n# token_num_uses                      0\n# token_period                        0s\n# token_policies                      [read-all]\n# token_ttl                           1h\n# token_type                          default\n# ttl                                 1h\n</code></pre>"},{"location":"Vault/kind_vault_external_secrets_operator/#vault-secret-provisioning","title":"Vault Secret provisioning","text":"<pre><code># Login into the Vault pod :\nk -n vault --tty --stdin exec vault-0 -- /bin/sh\n\n# KV version 2 secrets engine activation :\nvault secrets enable -version=2 kv\n\n# Secret provisioning :\nvault kv put kv/path/to/my/secret password=secretpassword\n\n    # ====== Secret Path ======\n    # kv/data/path/to/my/secret\n    # \n    # ======= Metadata =======\n    # Key                Value\n    # ---                -----\n    # created_time       2023-09-26T20:34:22.588811864Z\n    # custom_metadata    &lt;nil&gt;\n    # deletion_time      n/a\n    # destroyed          false\n    # version            1\n    # Let's leave the pod 'vault-0'\n\n\n# Secrets retrieval :\nvault kv get kv/path/to/my/secret\n\n# Let's leave the pod :\nexit\n</code></pre>"},{"location":"Vault/kind_vault_external_secrets_operator/#configuration-of-the-external-secrets-operator-eso","title":"Configuration of the External Secrets Operator (ESO)","text":"<pre><code>cat &lt;&lt; EOF | kubectl apply -f -\napiVersion: external-secrets.io/v1beta1\nkind: SecretStore\nmetadata:\n  name: ss-app1\n  namespace: ns-app1\nspec:\n  provider:\n    vault:\n      server: \"http://vault.vault:8200\"\n      path: \"kv\"\n      version: \"v2\"\n      auth:\n        # Authenticate against Vault using a Kubernetes ServiceAccount\n        # token stored in a Secret.\n        # https://www.vaultproject.io/docs/auth/kubernetes\n        kubernetes:\n          # Path where the Kubernetes authentication backend is mounted in Vault\n          mountPath: \"kubernetes\"\n          # A required field containing the Vault Role to assume.\n          role: \"vault-read\"\n          # Optional service account field containing the name\n          # of a kubernetes ServiceAccount\n          serviceAccountRef:\n            name: \"sa-app1\"\n          # Optional secret field containing a Kubernetes ServiceAccount JWT\n          #  used for authenticating with Vault\n          #secretRef:\n          #  name: \"my-secret\"\n          #  key: \"vault\"\nEOF\n\n    # secretstore.external-secrets.io/ss-app1 created\n\n\n# Check the status of our newly created secret store :\nk -n ns-app1 get ss ss-app1\n\n    # NAME      AGE   STATUS   CAPABILITIES   READY\n    # ss-app1   38m   Valid    ReadWrite      True\n</code></pre>"},{"location":"Vault/kind_vault_external_secrets_operator/#testing-the-good-accessibility-to-secrets-from-a-pod-using-the-relevant-service-account","title":"Testing the good accessibility to secrets from a pod using the relevant service-account","text":""},{"location":"Vault/kind_vault_external_secrets_operator/#executing-a-pod-in-the-namespace-and-with-the-service-account-both-dedicated-to-our-application-app1","title":"Executing a pod in the namespace and with the service-account both dedicated to our application 'app1'","text":"<pre><code># Execute an Alpine pod using the service-account dedicated to our application :\nkubectl -n ns-app1 run --tty --stdin app1 --image=alpine --overrides='{ \"spec\": { \"serviceAccount\": \"sa-app1\" }  }' -- /bin/sh\n\n# Install cURL\napk update &amp;&amp; apk add curl\n\n# Find the service-account token on ths pod :\nSA_JWT_TOKEN=$( cat /var/run/secrets/kubernetes.io/serviceaccount/token )\n    # -&gt; One can check the JWT token copying and pasting it in https://jwt.io/ website.\n\n# Login to Vault using the JWT token :\nCLIENT_TOKEN=$( curl --silent --request POST --data '{\"jwt\": \"'\"${SA_JWT_TOKEN}\"'\", \"role\": \"vault-read\"}' http://vault.vault:8200/v1/auth/kubernetes/login\n</code></pre> <p>| jq -r .auth.client_token )</p> <pre><code>    # {\n    #   \"request_id\": \"34b9adcd-2c2a-ec3f-c634-a011cbb49327\",\n    #   \"lease_id\": \"\",\n    #   \"renewable\": false,\n    #   \"lease_duration\": 0,\n    #   \"data\": null,\n    #   \"wrap_info\": null,\n    #   \"warnings\": null,\n    #   \"auth\": {\n    #     \"client_token\": \"hvs.CAESINcxRZaOtzL_ULx0qHhYFRJ0AaesHUPuu7w4eCQXk-SaGh4KHGh2cy5jTXkwd0FwU2U1SEI0bnhhY3ZPSHFLbGg\",\n    #     \"accessor\": \"oTMgsFmcPUx2E0ziAPUkT4tM\",\n    #     \"policies\": [\n    #       \"default\",\n    #       \"read-all\"\n    #     ],\n    #     \"token_policies\": [\n    #       \"default\",\n    #       \"read-all\"\n    #     ],\n    #     \"metadata\": {\n    #       \"role\": \"vault-read\",\n    #       \"service_account_name\": \"sa-app1\",\n    #       \"service_account_namespace\": \"ns-app1\",\n    #       \"service_account_secret_name\": \"\",\n    #       \"service_account_uid\": \"3c78185b-bd8b-445c-a0b7-445d2fd28a42\"\n    #     },\n    #     \"lease_duration\": 3600,\n    #     \"renewable\": true,\n    #     \"entity_id\": \"b13136aa-8355-1bbd-6972-32033cb5fab3\",\n    #     \"token_type\": \"service\",\n    #     \"orphan\": true,\n    #     \"mfa_requirement\": null,\n    #     \"num_uses\": 0\n    #   }\n    # }\n\necho ${CLIENT_TOKEN}\n    # -&gt; hvs.CAESIFXYSgDelsWHSEPm_RNsbl-Oxd3tVA4D0hC8rnGaKC42Gh4KHGh2cy5kcUVRamgwOWFTTFZSaDE1dXRiMUxxZEg\n</code></pre>"},{"location":"Vault/kind_vault_external_secrets_operator/#retrieving-the-secret-from-vault","title":"Retrieving the secret from Vault","text":"<pre><code>curl --silent --header \"X-Vault-Token:${CLIENT_TOKEN}\"  http://vault.vault:8200/v1/kv/data/path/to/my/secret | jq\n\n    # {\n    #   \"request_id\": \"b3fefd63-0005-0a36-7721-e33eeff6de9c\",\n    #   \"lease_id\": \"\",\n    #   \"renewable\": false,\n    #   \"lease_duration\": 0,\n    #   \"data\": {\n    #     \"data\": {\n    #       \"password\": \"secretpassword\"\n    #     },\n    #     \"metadata\": {\n    #       \"created_time\": \"2023-09-26T20:34:22.588811864Z\",\n    #       \"custom_metadata\": null,\n    #       \"deletion_time\": \"\",\n    #       \"destroyed\": false,\n    #       \"version\": 1\n    #     }\n    #   },\n    #   \"wrap_info\": null,\n    #   \"warnings\": null,\n    #   \"auth\": null\n    # }\n\n    # -&gt; password : secretpassword ^^\n</code></pre>"},{"location":"Vault/kind_vault_external_secrets_operator/README.2/","title":"Kind - Vault - External-secrets operator","text":"Description URL Kind install https://kind.sigs.k8s.io/docs/user/quick-start/#installing-from-release-binaries Helm installation https://helm.sh/docs/intro/install/ External-secrets operator installation https://external-secrets.io/latest/introduction/getting-started/ Vault helm installation https://developer.hashicorp.com/vault/tutorials/kubernetes/kubernetes-minikube-raft#install-the-vault-helm-chart Kubectl installation https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/ kubens &amp; kubectx installation https://github.com/ahmetb/kubectx#manual-installation-macos-and-linux External-secrets - Vault provider https://external-secrets.io/latest/provider/hashicorp-vault/ Tutorial: How to Set External-Secrets with Hashicorp Vault https://blog.container-solutions.com/tutorialexternal-secrets-with-hashicorp-vault Vault - Kubernetes authentication https://external-secrets.io/latest/provider/hashicorp-vault/#kubernetes-authentication"},{"location":"Vault/kind_vault_external_secrets_operator/README.2/#prerequisites","title":"Prerequisites","text":""},{"location":"Vault/kind_vault_external_secrets_operator/README.2/#kind-installation-and-cluster-creation","title":"'Kind' installation and cluster creation","text":"<pre><code>curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.20.0/kind-linux-amd64\nchmod +x ./kind\nsudo mv ./kind /usr/local/bin/kind\nkind --version\nkind create cluster --name sandbox\nkind get clusters\n</code></pre>"},{"location":"Vault/kind_vault_external_secrets_operator/README.2/#helm-installation","title":"Helm installation","text":"<pre><code>curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3\nchmod 700 get_helm.sh\n./get_helm.sh\nhelm version\n</code></pre>"},{"location":"Vault/kind_vault_external_secrets_operator/README.2/#kubectl-installation","title":"kubectl installation","text":"<pre><code>curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\"\nchmod +x kubectl\nsudo mv kubectl /usr/local/bin/kubectl\nkubectl version\nprintf \"\\nalias k=kubectl\\n\" &gt;&gt; ~/.bashrc &amp;&amp; source ~/.bashrc\nk get po -A\n</code></pre>"},{"location":"Vault/kind_vault_external_secrets_operator/README.2/#kubens-and-kubectx-installation","title":"kubens and kubectx installation","text":"<pre><code>sudo git clone https://github.com/ahmetb/kubectx /opt/kubectx\nsudo ln -s /opt/kubectx/kubectx /usr/local/bin/kubectx\nsudo ln -s /opt/kubectx/kubens /usr/local/bin/kubens\n</code></pre>"},{"location":"Vault/kind_vault_external_secrets_operator/README.2/#external-secrets-operator-installation","title":"External-secrets operator installation","text":"<pre><code>helm repo add external-secrets https://charts.external-secrets.io\nhelm install external-secrets external-secrets/external-secrets -n external-secrets --create-namespace\n\n    # NAME: external-secrets\n    # LAST DEPLOYED: Tue Sep 19 16:14:00 2023\n    # NAMESPACE: external-secrets\n    # STATUS: deployed\n    # REVISION: 1\n    # TEST SUITE: None\n    # NOTES:\n    # external-secrets has been deployed successfully!\n    # \n    # In order to begin using ExternalSecrets, you will need to set up a SecretStore\n    # or ClusterSecretStore resource (for example, by creating a 'vault' SecretStore).\n    # \n    # More information on the different types of SecretStores and how to configure them\n    # can be found in our Github: https://github.com/external-secrets/external-secrets\n</code></pre>"},{"location":"Vault/kind_vault_external_secrets_operator/README.2/#vault-installation","title":"Vault installation","text":"<pre><code>helm repo add hashicorp https://helm.releases.hashicorp.com\nhelm repo update\nhelm search repo hashicorp/vault\nhelm show values hashicorp/vault\ncat &lt;&lt; EOF &gt; custom-values.yaml\nserver:\n  affinity: \"\"\n  ha:\n    enabled: true\n    replicas: 1\n    raft:\n      enabled: true\nEOF\n\nhelm install vault hashicorp/vault --values custom-values.yaml -n vault --create-namespace\n\n    # NAME: vault\n    # LAST DEPLOYED: Mon Sep 18 20:23:23 2023\n    # NAMESPACE: vault\n    # STATUS: deployed\n    # REVISION: 1\n    # NOTES:\n    # Thank you for installing HashiCorp Vault!\n    # \n    # Now that you have deployed Vault, you should look over the docs on using\n    # Vault with Kubernetes available here:\n    # \n    # https://www.vaultproject.io/docs/\n    # \n    # \n    # Your release is named vault. To learn more about the release, try:\n    #\n    # helm status vault\n    # helm get manifest vault\n\n\nhelm list -A\n\n    # NAME                    NAMESPACE               REVISION        UPDATED                                 STATUS          CHART                   APP VERSION\n    # external-secrets        external-secrets        1               2023-09-26 19:45:58.063107607 +0000 UTC deployed        external-secrets-0.9.5  v0.9.5     \n    # vault                   vault                   1               2023-09-26 19:46:44.752525469 +0000 UTC deployed        vault-0.25.0            1.14.0\n\n\n\nk -n vault get po\n\n    # NAME                                    READY   STATUS    RESTARTS   AGE\n    # vault-0                                 0/1     Running   0          78s\n    # vault-agent-injector-67c48f8f4c-psznb   1/1     Running   0          79s\n\nexport VAULT_ADDR=http://127.0.0.1:8200\nk -n vault exec --tty --stdin vault-0 -- /bin/sh\n\nvault status\n</code></pre>"},{"location":"Vault/kind_vault_external_secrets_operator/README.2/#vault-initialization-and-unsealing-operations","title":"Vault initialization and unsealing operations","text":""},{"location":"Vault/kind_vault_external_secrets_operator/README.2/#loging-into-the-vault-pod","title":"Loging into the Vault pod","text":"<p>In namespace 'vault', the pod 'vault-0' is running but not ready. One must initialize Vault and unseal it. To do so, one must connect to the pod and interact with Vault using the provided CLI :</p> <pre><code>k -n vault --tty --stdin exec vault-0 -- /bin/sh\n</code></pre> <p>All the following operations will be executed in this pod :</p> <pre><code>vault status\n\n    # Key                Value\n    # ---                -----\n    # Seal Type          shamir\n    # Initialized        false\n    # Sealed             true\n    # Total Shares       0\n    # Threshold          0\n    # Unseal Progress    0/0\n    # Unseal Nonce       n/a\n    # Version            1.14.0\n    # Build Date         2023-06-19T11:40:23Z\n    # Storage Type       raft\n    # HA Enabled         true\n</code></pre>"},{"location":"Vault/kind_vault_external_secrets_operator/README.2/#vault-initialization","title":"Vault initialization","text":"<pre><code>vault operator init -key-shares=1 -key-threshold=1 -format=json\n\n    # {\n    #   \"unseal_keys_b64\": [\n    #     \"/yNqVmr967kJfM2vCjiRC+XFI304XApJDy8b2RVQaLs=\"\n    #   ],\n    #   \"unseal_keys_hex\": [\n    #     \"ff236a566afdebb9097ccdaf0a38910be5c5237d385c0a490f2f1bd9155068bb\"\n    #   ],\n    #   \"unseal_shares\": 1,\n    #   \"unseal_threshold\": 1,\n    #   \"recovery_keys_b64\": [],\n    #   \"recovery_keys_hex\": [],\n    #   \"recovery_keys_shares\": 0,\n    #   \"recovery_keys_threshold\": 0,\n    #   \"root_token\": \"hvs.fJuVStEaM1h2G66LnefKsDlI\"\n    # }\n</code></pre>"},{"location":"Vault/kind_vault_external_secrets_operator/README.2/#vault-unsealing","title":"Vault unsealing","text":"<pre><code>vault operator unseal /yNqVmr967kJfM2vCjiRC+XFI304XApJDy8b2RVQaLs=\n\n    # Key                     Value\n    # ---                     -----\n    # Seal Type               shamir\n    # Initialized             true\n    # Sealed                  false\n    # Total Shares            1\n    # Threshold               1\n    # Version                 1.14.0\n    # Build Date              2023-06-19T11:40:23Z\n    # Storage Type            raft\n    # Cluster Name            vault-cluster-58870b24\n    # Cluster ID              39288dfb-5e4e-93f6-a085-83472f00350d\n    # HA Enabled              true\n    # HA Cluster              https://vault-0.vault-internal:8201\n    # HA Mode                 active\n    # Active Since            2023-09-26T19:59:18.358850261Z\n    # Raft Committed Index    36\n    # Raft Applied Index      36\n</code></pre> <p>Vault is fully operational from now. Let's quit the Vault-0 pod :</p> <pre><code>exit\n</code></pre>"},{"location":"Vault/kind_vault_external_secrets_operator/README.2/#workshop","title":"Workshop","text":""},{"location":"Vault/kind_vault_external_secrets_operator/README.2/#vault-read-all-policy","title":"Vault 'read all' policy","text":"<pre><code># Login into the Vault pod :\nk -n vault --tty --stdin exec vault-0 -- /bin/sh\n\n# Connect to Vault using the root token :\nvault login hvs.fJuVStEaM1h2G66LnefKsDlI\n\n# Write a policy allowing every paths to be read :\nvault policy write read-all - &lt;&lt; EOF     \npath \"*\"                                                  \n{  capabilities = [\"read\"]                \n}                         \nEOF\n\n# Quit the pod :\nexit\n</code></pre>"},{"location":"Vault/kind_vault_external_secrets_operator/README.2/#enable-kubernetes-authentication-on-vault","title":"Enable Kubernetes authentication on Vault","text":"<pre><code># Login into the Vault pod :\nk -n vault --tty --stdin exec vault-0 -- /bin/sh\n\n# Enable Kubernetes authentication :\nvault auth enable kubernetes\n\nvault auth list\n\n    # Path           Type          Accessor                    Description                Version\n    # ----           ----          --------                    -----------                -------\n    # kubernetes/    kubernetes    auth_kubernetes_9a149aa4    n/a                        n/a\n    # token/         token         auth_token_b498fd96         token based credentials    n/a\n\n# Let's quit the pod again :\nexit\n</code></pre>"},{"location":"Vault/kind_vault_external_secrets_operator/README.2/#configuration-of-the-kubernetes-authentication","title":"Configuration of the Kubernetes authentication","text":"<pre><code># source: https://developer.hashicorp.com/vault/docs/auth/kubernetes#kubernetes-auth-method\n#\n#   Use local service account token as the reviewer JWT\n#   \n#   When running Vault in a Kubernetes pod the recommended option is to use the pod's local service account token.\n#   Vault will periodically re-read the file to support short-lived tokens. To use the local token and CA certificate,\n#   omit token_reviewer_jwt and kubernetes_ca_cert when configuring the auth method. Vault will attempt to load them\n#   from token and ca.crt respectively inside the default mount folder /var/run/secrets/kubernetes.io/serviceaccount/.\n\n\n# Login into the Vault pod :\nk -n vault --tty --stdin exec vault-0 -- /bin/sh\n\n# Configuration of the Kubernetes authentication :\nvault write auth/kubernetes/config kubernetes_host=https://${KUBERNETES_SERVICE_HOST}:${KUBERNETES_SERVICE_PORT}\n\n    # Success! Data written to: auth/kubernetes/config\n\n\n# Leave the pod :\nexit\n</code></pre>"},{"location":"Vault/kind_vault_external_secrets_operator/README.2/#role-definition-for-the-kubernetes-authentication-role-binding","title":"Role definition for the Kubernetes authentication + role binding","text":"<pre><code># Kubernetes namespace dedicated to our application \"app1\" :\nkubectl create namespace ns-app1\n\n# Kubernetes service-account dedicated to \"app1\" :\nkubectl create sa sa-app1 -n ns-app1\n\n# Role binding \nkubectl create clusterrolebinding sa-app1-tokenreview-access \\\n    --clusterrole=system:auth-delegator \\\n    --serviceaccount=ns-app1:sa-app1\n\n    # clusterrolebinding.rbac.authorization.k8s.io/sa-app1-tokenreview-access created\n\n\n# Login to the Vault pod :\nk -n vault --tty --stdin exec vault-0 -- /bin/sh\n\n# Role definition allowing our service-account from namespace 'ns-app1' to read secrets from Vault :  \nvault write auth/kubernetes/role/vault-read \\\n    bound_service_account_names=sa-app1 \\\n    bound_service_account_namespaces=ns-app1 \\\n    policies=read-all \\\n    ttl=1h\n\n    # Success! Data written to: auth/kubernetes/role/vault-read\n\n\n# Let's check our role 'vault-read' :\nvault read auth/kubernetes/role/vault-read\n\n# Key                                 Value\n# ---                                 -----\n# alias_name_source                   serviceaccount_uid\n# bound_service_account_names         [sa-app1]\n# bound_service_account_namespaces    [ns-app1]\n# policies                            [read-all]\n# token_bound_cidrs                   []\n# token_explicit_max_ttl              0s\n# token_max_ttl                       0s\n# token_no_default_policy             false\n# token_num_uses                      0\n# token_period                        0s\n# token_policies                      [read-all]\n# token_ttl                           1h\n# token_type                          default\n# ttl                                 1h\n\n# Let's leave the Vault pod :\nexit\n</code></pre>"},{"location":"Vault/kind_vault_external_secrets_operator/README.2/#vault-secret-provisioning","title":"Vault Secret provisioning","text":"<pre><code># Login into the Vault pod :\nk -n vault --tty --stdin exec vault-0 -- /bin/sh\n\n# KV version 2 secrets engine activation :\nvault secrets enable -version=2 kv\n\n# Secret provisioning :\nvault kv put kv/path/to/my/secret password=secretpassword\n\n    # ====== Secret Path ======\n    # kv/data/path/to/my/secret\n    # \n    # ======= Metadata =======\n    # Key                Value\n    # ---                -----\n    # created_time       2023-09-26T20:34:22.588811864Z\n    # custom_metadata    &lt;nil&gt;\n    # deletion_time      n/a\n    # destroyed          false\n    # version            1\n    # Let's leave the pod 'vault-0'\n\n\n# Secrets retrieval :\nvault kv get kv/path/to/my/secret\n\n# Let's leave the pod :\nexit\n</code></pre>"},{"location":"Vault/kind_vault_external_secrets_operator/README.2/#configuration-of-the-external-secrets-operator-eso","title":"Configuration of the External Secrets Operator (ESO)","text":"<pre><code>cat &lt;&lt; EOF | kubectl apply -f -\napiVersion: external-secrets.io/v1beta1\nkind: SecretStore\nmetadata:\n  name: ss-app1\n  namespace: ns-app1\nspec:\n  provider:\n    vault:\n      server: \"http://vault.vault:8200\"\n      path: \"kv\"\n      version: \"v2\"\n      auth:\n        # Authenticate against Vault using a Kubernetes ServiceAccount\n        # token stored in a Secret.\n        # https://www.vaultproject.io/docs/auth/kubernetes\n        kubernetes:\n          # Path where the Kubernetes authentication backend is mounted in Vault\n          mountPath: \"kubernetes\"\n          # A required field containing the Vault Role to assume.\n          role: \"vault-read\"\n          # Optional service account field containing the name\n          # of a kubernetes ServiceAccount\n          serviceAccountRef:\n            name: \"sa-app1\"\n          # Optional secret field containing a Kubernetes ServiceAccount JWT\n          #  used for authenticating with Vault\n          #secretRef:\n          #  name: \"my-secret\"\n          #  key: \"vault\"\nEOF\n\n    # secretstore.external-secrets.io/ss-app1 created\n\n\n# Check the status of our newly created secret store :\nk -n ns-app1 get ss ss-app1\n\n    # NAME      AGE   STATUS   CAPABILITIES   READY\n    # ss-app1   38m   Valid    ReadWrite      True\n</code></pre>"},{"location":"Vault/kind_vault_external_secrets_operator/README.2/#testing-the-good-accessibility-to-secrets-from-a-pod-using-the-relevant-service-account","title":"Testing the good accessibility to secrets from a pod using the relevant service-account","text":""},{"location":"Vault/kind_vault_external_secrets_operator/README.2/#definition-of-a-vault-external-secret","title":"Definition of a Vault external secret","text":"<pre><code>cat &lt;&lt; EOF &gt; vault-secrets.externalsecret.yaml\napiVersion: external-secrets.io/v1beta1\nkind: ExternalSecret\nmetadata:\n  name: vault-secrets\nspec:\n  refreshInterval: \"15s\"\n  secretStoreRef:\n    name: ss-app1\n    kind: SecretStore\n  target:\n    name: my-secret\n  data:\n  - secretKey: my_secret_password\n    remoteRef:\n      key: kv/path/to/my/secret\n      property: password\nEOF\nkubectl apply -f vault-secrets.externalsecret.yaml\n</code></pre>"},{"location":"Vault/kind_vault_external_secrets_operator/README.2/#executing-a-pod-in-the-namespace-and-with-the-service-account-both-dedicated-to-our-application-app1","title":"Executing a pod in the namespace and with the service-account both dedicated to our application 'app1'","text":"<pre><code># Create an Alpine pod using the service-account dedicated to our application and printing a Vault external-secret :\ncat &lt;&lt; EOF &gt; app1.pod.yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  labels:\n    run: app1\n  name: app1\n  namespace: ns-app1\nspec:\n  containers:\n  - name: app1\n    image: alpine\n    command: [\"printenv\"]\n    args: [\"MY_SECRET\"]\n    env:\n    - name: MY_SECRET\n      valueFrom:\n        secretKeyRef:\n          name: my-secret\n          key: my_secret_password\n  restartPolicy: Never\n  serviceAccount: sa-app1\nEOF\n\nkubectl -n ns-app1 --tty --stdin exec app1\n\nk -n ns-app1 logs app1\n# -&gt; secretpassword\n</code></pre>"},{"location":"Vault/protect_secrets_using_Vault_and_External_Secrets_Operator/readme/","title":"Protect the secrets of your application using HashiCorp Vault and the External Secrets Operator (ESO)","text":""},{"location":"Vault/protect_secrets_using_Vault_and_External_Secrets_Operator/readme/#abstract","title":"Abstract","text":"<p>Notre cluster Kubernetes est op\u00e9rationnel, avec FluxCD qui g\u00e8re notamment des repos et des charts Helm. Nous avons \u00e9galement un service Vault initialis\u00e9 et descell\u00e9.</p> <p>Nous allons d\u00e9ployer via Flux et \u00e0 partir de son Helm Chart l'op\u00e9rateur External Secrets.</p> <p>Ensuite nous appliquerons la protection des secrets avec Vault via ESO sur une application (MySQL).</p>"},{"location":"Vault/protect_secrets_using_Vault_and_External_Secrets_Operator/readme/#docs-de-reference","title":"Docs de r\u00e9f\u00e9rence","text":"<ul> <li>https://external-secrets.io/latest/introduction/getting-started/</li> <li>https://external-secrets.io/latest/provider/hashicorp-vault/</li> <li>https://github.com/jeffsanicola/vault-policy-guide?tab=readme-ov-file#kv-policies</li> <li>https://external-secrets.io/latest/guides/common-k8s-secret-types/</li> <li>https://external-secrets.io/latest/guides/templating/</li> <li>https://external-secrets.io/latest/provider/hashicorp-vault/#kubernetes-authentication</li> <li>https://support.hashicorp.com/hc/en-us/articles/4404389946387-Kubernetes-auth-method-Permission-Denied-error</li> <li> <p>https://external-secrets.io/main/guides/templating/#helm</p> </li> <li> <p>https://medium.com/@stefanprodan/automate-helm-chart-repository-publishing-with-github-actions-and-pages-8a374ce24cf4</p> </li> <li>https://github.com/stefanprodan/helm-gh-pages</li> </ul>"},{"location":"Vault/protect_secrets_using_Vault_and_External_Secrets_Operator/readme/#pre-requis-kubernetes-cluster-local","title":"Pr\u00e9-requis Kubernetes (cluster local)","text":"<p>Avoir suivi la partie : 'Auto-unsealed Vault Helm deployment managed with FluxCD'. Nous devrions donc d\u00e9j\u00e0 avoir \u00e0 notre disposition : - un cluster Kind op\u00e9rationnel,  - FluxCD d\u00e9ploy\u00e9, - Helm install\u00e9, - kubectl install\u00e9, - HashiCorp Vault d\u00e9ploy\u00e9 depuis le Helm Chart officiel en mode auto-unseal via FluxCD.</p> <p>Commen\u00e7ons par d\u00e9ployer l'External Secrets Operator (ESO)</p>"},{"location":"Vault/protect_secrets_using_Vault_and_External_Secrets_Operator/readme/#vault","title":"Vault","text":""},{"location":"Vault/protect_secrets_using_Vault_and_External_Secrets_Operator/readme/#deploiement-manuel-de-vault-en-mode-auto-unseal","title":"D\u00e9ploiement manuel de Vault en mode auto-unseal","text":"<pre><code>helm install vault hashicorp/vault -f values.yml --dry-run\nkubens vault\nkubectl get all\n\nkubectl logs vault-0\n\n\nkubectl exec -it vault-0 -- vault status\n</code></pre>"},{"location":"Vault/protect_secrets_using_Vault_and_External_Secrets_Operator/readme/#deploiement-de-vault-en-mode-auto-unseal-depuis-le-helm-chart-officiel-et-pilote-par-fluxcd","title":"D\u00e9ploiement de Vault en mode auto-unseal depuis le Helm Chart officiel et pilot\u00e9 par FluxCD","text":""},{"location":"Vault/protect_secrets_using_Vault_and_External_Secrets_Operator/readme/#alerting-discord","title":"alerting Discord","text":""},{"location":"Vault/protect_secrets_using_Vault_and_External_Secrets_Operator/readme/#creation-du-salon-prive-sur-le-client-discord","title":"cr\u00e9ation du salon priv\u00e9 sur le client Discord","text":"<p>Cr\u00e9ation d'un nouveau salon (priv\u00e9) :    - nom : vault-development   - webhook :     - nom : FluxCD     - URL : https://discord.com/api/webhooks/1213494413511237642/7gRzmfYCwDqWwI2D-1jfLZCNvDBotoe_rY2sson57G1Ya40-EtEMWAZy9FsxmjCZTJ4C</p>"},{"location":"Vault/protect_secrets_using_Vault_and_External_Secrets_Operator/readme/#on-place-le-webhook-du-salon-discord-dans-un-secret-kubernetes","title":"on place le webhook du salon discord dans un secret kubernetes","text":"<pre><code>DISCORD_WEBHOOK=\"https://discord.com/api/webhooks/1213494413511237642/7gRzmfYCwDqWwI2D-1jfLZCNvDBotoe_rY2sson57G1Ya40-EtEMWAZy9FsxmjCZTJ4C\"\nkubectl -n vault create secret generic discord-vault-development-webhook --from-literal=address=${DISCORD_WEBHOOK} --dry-run=client -o yaml &gt; products/vault/discord-vault-development-webhook.secret.yaml\n</code></pre>"},{"location":"Vault/protect_secrets_using_Vault_and_External_Secrets_Operator/readme/#definition-de-lalert-provider-discord","title":"d\u00e9finition de l'alert-provider Discord","text":"<pre><code>flux create alert-provider discord \\\n  --type=discord \\\n  --secret-ref=discord-webhook \\\n  --channel=vault-development \\\n  --username=FluxCD \\\n  --namespace=vault \\\n  --export &gt; products/vault/notification-provider.yaml\n</code></pre>"},{"location":"Vault/protect_secrets_using_Vault_and_External_Secrets_Operator/readme/#configuration-des-alertes-discord","title":"configuration des alertes Discord","text":"<pre><code>flux create alert discord \\\n  --event-severity=info \\\n  --event-source='GitRepository/*,Kustomization/*,ImageRepository/*,ImagePolicy/*,HelmRepository/*,HelmRelease/*' \\\n  --provider-ref=discord \\\n  --namespace=vault \\\n  --export &gt; products/vault/notification-alert.yaml\n</code></pre>"},{"location":"Vault/protect_secrets_using_Vault_and_External_Secrets_Operator/readme/#poussons-les-manifests-sur-le-repo-central-pour-que-fluxcd-les-gere","title":"Poussons les manifests sur le repo central pour que FluxCD les g\u00e8re :","text":"<pre><code>git add .\ngit commit -m 'feat: configuring discord alerting for vault.'\ngit push\n\nflux reconcile kustomization flux-system --with-source\nflux events -w\n</code></pre>"},{"location":"Vault/protect_secrets_using_Vault_and_External_Secrets_Operator/readme/#gestion-du-repo-helm","title":"Gestion du repo Helm","text":"<pre><code>flux create source helm hashicorp \\\n  --url=https://helm.releases.hashicorp.com \\\n  --namespace=vault \\\n  --interval=1m \\\n  --export &gt; products/vault/helm-repository.yml\n</code></pre>"},{"location":"Vault/protect_secrets_using_Vault_and_External_Secrets_Operator/readme/#deploiement-de-vault-helm-release","title":"D\u00e9ploiement de Vault (helm release)","text":"<p>Recopier le fichier 'values.yaml' en 'custom-values.txt' (FluxCD ne g\u00e8re que les manifests en YAML)</p> <pre><code>flux create helmrelease vault \\\n  --source=HelmRepository/hashicorp \\\n  --chart=vault \\\n  --values=products/vault/custom-values.txt \\\n  --namespace=vault \\\n  --export &gt; products/vault/helm-release.yaml\n\ngit status\n  # Sur la branche main\n  # Votre branche est \u00e0 jour avec 'origin/main'.\n  # \n  # Fichiers non suivis:\n  #   (utilisez \"git add &lt;fichier&gt;...\" pour inclure dans ce qui sera valid\u00e9)\n  #     products/vault/custom-values.txt\n  #     products/vault/helm-release.yaml\n  #     products/vault/helm-repository.yml\n\ngit add .\ngit commit -m 'feat: managing vault helm deployment.'\ngit push\n\nkubectl get all\n  # NAME          READY   STATUS    RESTARTS   AGE\n  # pod/vault-0   1/1     Running   0          17s\n  # \n  # NAME                     TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)             AGE\n  # service/vault            ClusterIP   10.96.102.150   &lt;none&gt;        8200/TCP,8201/TCP   17s\n  # service/vault-internal   ClusterIP   None            &lt;none&gt;        8200/TCP,8201/TCP   17s\n  # \n  # NAME                     READY   AGE\n  # statefulset.apps/vault   1/1     17s\n\nkubectl exec -it vault-0 -- vault status\n  # Key                      Value\n  # ---                      -----\n  # Recovery Seal Type       shamir\n  # Initialized              true\n  # Sealed                   false\n  # Total Recovery Shares    5\n  # Threshold                3\n  # Version                  1.15.2\n  # Build Date               2023-11-06T11:33:28Z\n  # Storage Type             file\n  # Cluster Name             vault-cluster-6fa0df73\n  # Cluster ID               b26de2f1-d9e7-8225-ad55-f114b37eeffb\n  # HA Enabled               false\n</code></pre> <p>-&gt; C'est GOOD !!! Note : pas besoin d'initialiser notre Vault car la config a \u00e9t\u00e9 r\u00e9cup\u00e9r\u00e9e depuis le volume du statefulset cr\u00e9\u00e9 pr\u00e9alablement.</p>"},{"location":"Vault/protect_secrets_using_Vault_and_External_Secrets_Operator/readme/#deploiement-de-lexternal-secrets-operator-eso","title":"D\u00e9ploiement de l'External Secrets Operator (ESO)","text":""},{"location":"Vault/protect_secrets_using_Vault_and_External_Secrets_Operator/readme/#clonage-en-local-du-repository-git-de-fluxcd","title":"Clonage en local du repository Git de FluxCD","text":"<p>Nous devons ajouter les manifests dans le repo Git pilot\u00e9 par FluxCD. Pour identifier ce dernier : </p> <pre><code>kubectl get gitrepository -n flux-system -&gt; https://github.com/papaFrancky/kubernetes-development.git\n</code></pre> <p>Mettons \u00e0 jour la copie locale de ce repo : </p> <pre><code>cd ~/code/github/kubernetes-development\ngit pull\nmkdir -p products/external-secrets\n</code></pre>"},{"location":"Vault/protect_secrets_using_Vault_and_External_Secrets_Operator/readme/#commencons-par-creer-le-namespace-dedie-a-eso","title":"Commen\u00e7ons par cr\u00e9er le namespace d\u00e9di\u00e9 \u00e0 ESO","text":"<pre><code>kubectl create ns external-secrets --dry-run=client -o yaml | \\\n  grep -vE \"creationTimestamp:|^spec:|^status:\" &gt; products/external-secrets/namespace.yaml\nkubectl apply -f products/external-secrets/namespace.yaml\n</code></pre>"},{"location":"Vault/protect_secrets_using_Vault_and_External_Secrets_Operator/readme/#definissons-ensuite-le-helm-repository-pour-fluxcd","title":"D\u00e9finissons ensuite le Helm repository pour FluxCD","text":"<pre><code>flux create source helm external-secrets \\\n  --url=https://charts.external-secrets.io \\\n  --namespace=external-secrets \\\n  --interval=1m \\\n  --export &gt; products/external-secrets/helm-repository.yml\n</code></pre>"},{"location":"Vault/protect_secrets_using_Vault_and_External_Secrets_Operator/readme/#deploiement-de-loperateur-external-secrets","title":"D\u00e9ploiement de l'op\u00e9rateur External Secrets","text":"<pre><code>flux create helmrelease vault \\\n  --source=HelmRepository/external-secrets \\\n  --chart=external-secrets \\\n  --namespace=external-secrets \\\n  --export &gt; products/external-secrets/helm-release.yaml\n</code></pre>"},{"location":"Vault/protect_secrets_using_Vault_and_External_Secrets_Operator/readme/#mise-a-jour-du-repo-git-distant-pour-prise-en-compte-par-fluxcd","title":"Mise \u00e0 jour du repo Git distant pour prise en compte par FluxCD","text":"<pre><code>git add .\ngit commit -m 'feat: installing External Secrets Operator (ESO.)'\ngit push\n\nflux reconcile kustomization flux-system --with-source\nflux events -w\n\nkubectl -n external-secrets get helmrepo,helmrelease\n\n  # NAME                                                       URL                                  AGE     READY   STATUS\n  # helmrepository.source.toolkit.fluxcd.io/external-secrets   https://charts.external-secrets.io   9m27s   True    stored artifact:       # revision 'sha256:35986103ade32186cf3d151ce19bae8939cfcbf7f64011cf5c5678ad2c8df860'\n  # \n  # NAME                                       AGE     READY   STATUS\n  # helmrelease.helm.toolkit.fluxcd.io/vault   9m27s   True    Helm install succeeded for release external-secrets/vault.v1 with chart external-secrets@0.9.13\n</code></pre>"},{"location":"Vault/protect_secrets_using_Vault_and_External_Secrets_Operator/readme/#configuration-de-vault","title":"Configuration de Vault","text":"<p>Dans la partie 'helm_vault_auto-unseal', nous avons d\u00e9crit l'installation de Vault en mode descell\u00e9 \u00e0 partir d'un Helm Chart et pilot\u00e9 par FluxCD.</p> <p>Lors de l'initialisation de Vault, nous avions r\u00e9cup\u00e9r\u00e9 le 'root token' dont nous aurons besoin pour la suite des op\u00e9rations :</p> <pre><code>Initial Root Token: hvs.G145zNl012ApNOap3sn2zhIG\n</code></pre>"},{"location":"Vault/protect_secrets_using_Vault_and_External_Secrets_Operator/readme/#activation-de-lauthentification-kubernetes-sur-vault","title":"Activation de l'authentification Kubernetes sur Vault","text":"<pre><code>kubectl -n vault exec -it vault-0 -- sh    # ouverture d'une session shell sur le pod vault-0\nvault login hvs.G145zNl012ApNOap3sn2zhIG   # login \u00e0 Vault avec le token root\n\nvault auth enable kubernetes\nvault auth list\nexit\n</code></pre>"},{"location":"Vault/protect_secrets_using_Vault_and_External_Secrets_Operator/readme/#configuration-of-the-kubernetes-authentication","title":"Configuration of the Kubernetes authentication","text":"<pre><code># source: https://developer.hashicorp.com/vault/docs/auth/kubernetes#kubernetes-auth-method\n#\n#   Use local service account token as the reviewer JWT\n#   \n#   When running Vault in a Kubernetes pod the recommended option is to use the pod's local service account token.\n#   Vault will periodically re-read the file to support short-lived tokens. To use the local token and CA certificate,\n#   omit token_reviewer_jwt and kubernetes_ca_cert when configuring the auth method. Vault will attempt to load them\n#   from token and ca.crt respectively inside the default mount folder /var/run/secrets/kubernetes.io/serviceaccount/.\n\n\nkubectl -n vault exec -it vault-0 -- sh    # ouverture d'une session shell sur le pod vault-0\nvault login hvs.G145zNl012ApNOap3sn2zhIG   # login \u00e0 Vault avec le token root\n\nvault write auth/kubernetes/config kubernetes_host=https://${KUBERNETES_SERVICE_HOST}:${KUBERNETES_SERVICE_PORT}\nvault read auth/kubernetes/config\nexit\n</code></pre> <p>Vault est d\u00e9sormais configur\u00e9 pour permettre \u00e0 notre cluster Kubernetes de s'y authentifier. Les pods utiliseront leur propre (short-lived) token JWT, raison pour laquelle nous n'avons pas pr\u00e9cis\u00e9 dans la derni\u00e8re commande les param\u00e8tres token_reviewer_jwt et kubernetes_ca_cert.</p>"},{"location":"Vault/protect_secrets_using_Vault_and_External_Secrets_Operator/readme/#deploiement-de-lapplication-silly-webapp","title":"D\u00e9ploiement de l'application 'silly-webapp'","text":"<p>Nous prendrons comme exemple le d\u00e9ploiement d'une appli 'custom' qui affichera des secrets dans une page web. Le Helm Chart attend que nous renseignions les variables suivantes :</p> <pre><code>BGCOLOR:  DarkOliveGreen\nCOLOR:    PaleGreen\nLOGIN:    admin\nPASSWORD: m%K@W5JeVyQ4jik2@#Lv3fV@D7PGPv\n</code></pre> <p>Nous pr\u00e9voyons de d\u00e9finir un service account nomm\u00e9 'silly-webapp', de d\u00e9ployer l'application dans un namespace \u00e9galement nomm\u00e9 'silly-webapp'. Mais commen\u00e7ons par pr\u00e9parer Vault \u00e0 h\u00e9berger et d\u00e9livrer les secrets \u00e0 l'application.</p>"},{"location":"Vault/protect_secrets_using_Vault_and_External_Secrets_Operator/readme/#operations-sur-vault","title":"Op\u00e9rations sur Vault","text":"<p>Nous utiliserons le KVv2 engine (moteur de cl\u00e9s/valeurs).</p> <pre><code># Login avec le root token sur Vault (dans le pod vault-0) :\nkubectl -n vault exec -it vault-0 -- sh    # ouverture d'une session shell sur le pod vault-0\nvault login hvs.G145zNl012ApNOap3sn2zhIG   # login \u00e0 Vault avec le token root\n\n# Activation de KVv2 secrets engine :\nvault secrets enable -version=2 kv\nvault secrets list\n\n# Ecriture des secrets dans le path /kv/silly-webapp :\nvault kv put -mount=kv silly-webapp \\\n  BGCOLOR=DarkOliveGreen \\\n  COLOR=PaleGreen \\\n  LOGIN=admin \\\n  PASSWORD=m%K@W5JeVyQ4jik2@#Lv3fV@D7PGPv\nvault kv get -mount kv silly-webapp\n\n# Ecriture d'une policy d'acc\u00e8s en lecture sur le path /kv/silly-webapp :\nvault policy write kv-silly-webapp-read - &lt;&lt; EOF\npath \"kv/metadata/silly-webapp\" {\n  capabilities = [\"list\",\"read\"]\n}\npath \"kv/data/silly-webapp\" {\n  capabilities = [\"list\",\"read\"]\n}\nEOF\nvault policy read kv-silly-webapp-read\n\n# Fin de session\nexit\n\n# D\u00e9finition d'un r\u00f4le autorisant le service-account 'silly-webapp' du namespace 'silly-webapp' de lire les secrets pr\u00e9c\u00e9dents dans Vault :\nvault write auth/kubernetes/role/silly-webapp \\\n    bound_service_account_names=silly-webapp \\\n    bound_service_account_namespaces=silly-webapp \\\n    policies=kv-silly-webapp-read \\\n    ttl=1h\nvault read auth/kubernetes/role/silly-webapp\n</code></pre>"},{"location":"Vault/protect_secrets_using_Vault_and_External_Secrets_Operator/readme/#creation-du-helm-chart-pour-lapplication-silly-webapp","title":"Cr\u00e9ation du Helm Chart pour l'application 'silly-webapp'","text":""},{"location":"Vault/protect_secrets_using_Vault_and_External_Secrets_Operator/readme/#lapplication-en-question","title":"L'application en question","text":""},{"location":"Vault/protect_secrets_using_Vault_and_External_Secrets_Operator/readme/#creation-manuelle","title":"Cr\u00e9ation 'manuelle'","text":"<p>Notre application sera tr\u00e8s simple : un serveur web Apache qui affiche une page web contenant nos secrets prot\u00e9g\u00e9s dans Vault.</p> <p>Pour ce faire, nous devrons cr\u00e9er : * 1 namespace d\u00e9di\u00e9; * 1 service account; * 1 ClusterRoleBinding affectant au service account le ClusterRole qui permette \u00e0 Kubernetes de d\u00e9l\u00e9guer les contr\u00f4les d'authentification et d'autorisation \u00e0 l'application (cf. doc : https://kubernetes.io/docs/reference/access-authn-authz/rbac/#other-component-roles) * 1 Deployment qui lancera 1 pod avec pour image Apache 2.4; * 1 SecretStore ouvrant l'acc\u00e8s \u00e0 Vault au service account de l'application; * 1 ExternalSecret qui g\u00e9n\u00e8rera la page web affich\u00e9e par l'application avec les secrets r\u00e9cup\u00e9r\u00e9s depuis Vault; * 1 service pour exposer l'application.</p> <p>Les manifests YAML se trouvent dans le r\u00e9pertoire ./silly-webapp/manifests</p> <p>Testons l'application en la d\u00e9ployant \u00e0 la main : </p> <pre><code>cd silly-webapp/manifests\nkubectl apply -f namespace.yaml\nkubectl apply -f serviceaccount.yaml\nkubectl apply -f clusterrolebinding.yaml\nkubectl apply -f secretstore.yaml\nkubectl apply -f externalsecret.yaml\nkubectl apply -f deployment.yaml\nkubectl apply -f service.yaml\n\nkubectl port-forward service/silly-webapp 8080:80\ncurl http://localhost:8080 \n&lt;table border=1 style=\"background-color:DarkOliveGreen;color:PaleGreen\"\n  &lt;tr&gt;\n    &lt;th&gt;SECRET&lt;/th&gt;\n    &lt;th&gt;VALUE&lt;/th&gt;\n  &lt;/tr&gt;\n  &lt;tr&gt;\n    &lt;td&gt;BGCOLOR&lt;/td&gt;\n    &lt;td&gt;DarkOliveGreen&lt;/td&gt;\n  &lt;/tr&gt;\n  &lt;tr&gt;\n    &lt;td&gt;COLOR&lt;/td&gt;\n    &lt;td&gt;PaleGreen&lt;/td&gt;\n  &lt;/tr&gt;\n  &lt;tr&gt;\n    &lt;td&gt;LOGIN&lt;/td&gt;\n    &lt;td&gt;admin&lt;/td&gt;\n  &lt;/tr&gt;\n  &lt;tr&gt;\n    &lt;td&gt;PASSWORD&lt;/td&gt;\n    &lt;td&gt;m%K@W5JeVyQ4jik2@#Lv3fV@D7PGPv&lt;/td&gt;\n  &lt;tr&gt;\n&lt;/table&gt;\n</code></pre> <p>Top, \u00e7a marche ! Faison le m\u00e9nage.</p> <pre><code>kubectl delete ns silly-webapp\nkubectl get clusterrolebinding external-secret-silly-webapp\n</code></pre>"},{"location":"Vault/protect_secrets_using_Vault_and_External_Secrets_Operator/readme/#packaging-de-lapplication-avec-helm","title":"Packaging de l'application avec Helm","text":"<p>Sortons du r\u00e9pertoire contenant les manifests :</p> <pre><code>cd ...\npwd\n</code></pre> <p>Nous allons cr\u00e9er un Helm Chart from scratch :</p> <pre><code># Cr\u00e9ation d'un Helm Chart par d\u00e9faut \nhelm create silly-webapp\n\n# Remplacement des templates par les manifests YAML que nous venons de produire :\nrm -rf silly-webapp/templates/*.yaml\ncp manifests/*.yaml silly-webapp/templates/.\nhelm instal silly-webapp ./silly-webapp --dry-run\n\n\n    silly-webapp\n    \u251c\u2500\u2500 Chart.yaml\n    \u251c\u2500\u2500 charts\n    \u251c\u2500\u2500 templates\n    \u2502   \u251c\u2500\u2500 clusterrolebinding.yaml\n    \u2502   \u251c\u2500\u2500 deployment.yaml\n    \u2502   \u251c\u2500\u2500 externalsecret.yaml\n    \u2502   \u251c\u2500\u2500 namespace.yaml\n    \u2502   \u251c\u2500\u2500 secret.yaml\n    \u2502   \u251c\u2500\u2500 secretstore.yaml\n    \u2502   \u251c\u2500\u2500 service.yaml\n    \u2502   \u251c\u2500\u2500 serviceaccount.yaml\n    \u2502   \u2514\u2500\u2500 tests\n    \u2502       \u2514\u2500\u2500 test-connection.yaml\n    \u2514\u2500\u2500 values.yaml\n</code></pre> <p>Le Helm Chart fonctionnera en l'\u00e9tat. Mais il est plus int\u00e9ressant de 'templatiser' les manifests si l'on souhaite pouvoir cr\u00e9er des Helm releases diff\u00e9rentes \u00e0 partir du m\u00eame Helm chart. Cette partie sort du p\u00e9rim\u00e8tre de cete doc. Je laisse les manifests originaux dans le r\u00e9pertoire 'manifests' et le Helm chart 'templatis\u00e9' dans le r\u00e9pertoire 'silly-webapp', pour comparer l'avant et l'apr\u00e8s.</p> <p>TODO :  Expliquer comment on passe des manifests 'manuels' \u00e0 des templates variabilis\u00e9s Helm. D\u00e9tailler la particularit\u00e9 de Helm avec External Secrets (lequel des 2 doit interpr\u00e9ter les variables et comment).</p> <p>Ensuite :</p> <pre><code>helm install silly-webapp ./silly-webapp --create-namespace\nhelm test silly-webapp -n silly-webapp\n\nhelm package ./silly-webapp\nSuccessfully packaged chart and saved it to: (...)/silly-webapp-0.1.0.tgz\n</code></pre>"},{"location":"Vault/protect_secrets_using_Vault_and_External_Secrets_Operator/readme/#creation-dun-repository-de-helm-charts-avec-github-et-github-actions","title":"Cr\u00e9ation d'un repository de Helm Charts avec GitHub et GitHub Actions","text":"<p>Maintenant que nous avons cr\u00e9\u00e9 notre Helm Chart, nous allons le publier pour pouvoir le rendre utilisable autrement qu'en local.</p>"},{"location":"Vault/protect_secrets_using_Vault_and_External_Secrets_Operator/readme/#creation-dun-github-repository-dedie-a-nos-helm-harts","title":"Cr\u00e9ation d'un GitHub Repository d\u00e9di\u00e9 \u00e0 nos Helm harts","text":"<p>Une fois logu\u00e9 sur le site GitHub, cr\u00e9er un nouveau repository : * name : papafrancky/helm-charts * accessibility : public</p> <p>Nous allons placer les Helm Charts sous la forme de manifests dans un r\u00e9pertoire 'charts', et ces m\u00eames charts packag\u00e9s en .tgz dans un r\u00e9pertoire 'packages'. La page d'index du repo Helm (index.yaml) se trouvera donc dans le r\u00e9pertoire 'packages' et sera accessible \u00e0 l'adresse suivante : </p> <pre><code>https://raw.githubusercontent.com/papafrancky/helm-charts/main/packages/index.yaml\n</code></pre> <p>Nous pr\u00e9ciserons cela dans le README du repository.</p> <pre><code>mkdir ~/code/github/helm-charts\ncd ~/code/github/helm-charts\nmkdir charts templates\ntouch charts/.gitkeep packages/.gitkeep\n\nprintf \"# Papa Francky's Helm Charts Repository\\n\\nTo add the papaFrancky Helm repository :\\n\\n    helm repo add papafrancky https://raw.githubusercontent.com/papafrancky/helm-charts/main/packages\\n\\nTo upgrade an existing installation :\\n\\n    helm repo upgrade papafrancky\\n\\nTo search for stable release versons matching the keyword \"papafrancky\" :\\n\\n    helm search repo papafrancky\\n\" &gt; README.md\n</code></pre>"},{"location":"Vault/protect_secrets_using_Vault_and_External_Secrets_Operator/readme/#la-github-action","title":"La GitHub Action","text":"<pre><code>cd ~/code/github/helm-charts\nmkdir -p .github/{workflows,scripts}\n\ncat &lt;&lt; EOF &gt;&gt; .github/workflows/helmchart_release.yml\nname: Helm Chart Release\n\non:\n  push:\n    branches: [ \"main\" ]\n    paths: [ 'charts/**' ]\n  # Allows you to run this workflow manually from the Actions tab\n  workflow_dispatch:\n\njobs:\n  helm_repo_update:\n    runs-on: ubuntu-latest\n    steps:\n\n      - uses: actions/checkout@v3\n\n      - name: Helm tool installer\n        uses: Azure/setup-helm@v4\n        with:\n          version: latest\n\n      - name: Helm packaging and reposotiry indexing\n        run: ./.github/scripts/helm_packaging_and_repo_indexing.sh\n\n      - name: GIT Commit and Push\n        run: |\n          cd ${GITHUB_WORKSPACE}\n          git config user.name \"${GITHUB_ACTOR}\"\n          git config user.email \"${GITHUB_ACTOR}@users.noreply.github.com\"\n          git add .\n          git commit -a -m \"Helm packages publication is up-to-date.\"\n          git push\nEOF\n\nvi .github/scripts/helm_packaging_and_repo_indexing.sh\n\n  #!/bin/sh\n\n  HelmChartsDir=\"${GITHUB_WORKSPACE}/charts\"       # Helm Charts Directory (files)\n  HelmPackagesDir=\"${GITHUB_WORKSPACE}/packages\"   # Helm packaged Charts (.tgz)\n\n  # Package Helm Charts\n  ChartsList=$( find ${HelmChartsDir} -type d -maxdepth 1 | grep -v charts$ )\n  printf \"Packaging the Helm charts ...\\n\"\n  for Chart in $( echo ${ChartsList} ); do\n    helm package ${Chart} --destination ${HelmPackagesDir}\n  done\n\n  # Repo Index\n  printf \"Indexing the Helm charts ...\\n\"\n  helm repo index ${HelmPackagesDir}\n\nchmod +x .github/scripts/helm_packaging_and_repo_indexing.sh\n</code></pre> <p>Notre GitHub Action va r\u00e9cup\u00e9rer le contenu de notre repo sur le runner, installer Helm, packager les Charts en fichiers .tgz et cr\u00e9er un helm repository index. Pour int\u00e9grer les nouveaux fichiers cr\u00e9\u00e9s \u00e0 notre code, la GitHub Action devra les pousser sur notre repo de code. Nous devons modifier les param\u00e8tres de notre repo pour autoriser les GitHub Actions \u00e0 \u00e9crire :</p> <p>https://github.com/papafrancky/helm-charts/settings/actions -&gt; Workflow permissions : Read and write permissions </p> <pre><code>git init\ngit add .\ngit commit -m \"first commit\"\ngit branch -M main\ngit remote add origin git@github.com:papafrancky/helm-charts.git\ngit push -u origin main\n</code></pre>"},{"location":"Vault/protect_secrets_using_Vault_and_External_Secrets_Operator/readme/#copie-de-notre-helm-chart-silly-webapp-dans-notre-helm-repo","title":"Copie de notre Helm Chart 'silly-webapp' dans notre Helm repo","text":"<pre><code>cp -r ../../tmp/protect_secrets_using_Vault_and_External_Secrets_Operator/silly-webapp/silly-webapp charts\ntree\n    .\n    \u251c\u2500\u2500 README.md\n    \u251c\u2500\u2500 charts\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 silly-webapp\n    \u2502\u00a0\u00a0     \u251c\u2500\u2500 Chart.yaml\n    \u2502\u00a0\u00a0     \u251c\u2500\u2500 charts\n    \u2502\u00a0\u00a0     \u251c\u2500\u2500 templates\n    \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 NOTES.txt\n    \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 clusterrolebinding.yaml\n    \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 deployment.yaml\n    \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 externalsecret.yaml\n    \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 secretstore.yaml\n    \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 service.yaml\n    \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 serviceaccount.yaml\n    \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 tests\n    \u2502\u00a0\u00a0     \u2502\u00a0\u00a0     \u2514\u2500\u2500 test-connection.yaml\n    \u2502\u00a0\u00a0     \u2514\u2500\u2500 values.yaml\n    \u2514\u2500\u2500 packages\n\ngit add .\ngit commit -m 'added silly-webapp helm chart.'\ngit push\n\n-&gt; la GitHub Action s'ex\u00e9cute, cr\u00e9\u00e9 le package silly-webapp-0.1.0.tgz, met l'index \u00e0 jour.\ngit pull\ntree\n  .\n  \u251c\u2500\u2500 README.md\n  \u251c\u2500\u2500 charts\n  \u2502\u00a0\u00a0 \u2514\u2500\u2500 silly-webapp\n  \u2502\u00a0\u00a0     \u251c\u2500\u2500 Chart.yaml\n  \u2502\u00a0\u00a0     \u251c\u2500\u2500 charts\n  \u2502\u00a0\u00a0     \u251c\u2500\u2500 templates\n  \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 NOTES.txt\n  \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 clusterrolebinding.yaml\n  \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 deployment.yaml\n  \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 externalsecret.yaml\n  \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 secretstore.yaml\n  \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 service.yaml\n  \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 serviceaccount.yaml\n  \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 tests\n  \u2502\u00a0\u00a0     \u2502\u00a0\u00a0     \u2514\u2500\u2500 test-connection.yaml\n  \u2502\u00a0\u00a0     \u2514\u2500\u2500 values.yaml\n  \u2514\u2500\u2500 packages\n      \u251c\u2500\u2500 index.yaml\n      \u2514\u2500\u2500 silly-webapp-0.1.0.tgz\n\ncat packages/index.yaml\n\n    apiVersion: v1\n    entries:\n      silly-webapp:\n      - apiVersion: v2\n        appVersion: \"2.4\"\n        created: \"2024-03-14T20:10:32.534365859Z\"\n        description: A simple web application exposing credentials retrieved from a Vault\n          instance through the External Secrets Operator (ESO).\n        digest: 95e29f48321a31eee8793ccc94a0da08dad9ddc7bd36fb6730f8f30e0bb59de8\n        name: silly-webapp\n        type: application\n        urls:\n        - silly-webapp-0.1.0.tgz\n        version: 0.1.0\n    generated: \"2024-03-14T20:10:32.53390382Z\"\n</code></pre>"},{"location":"Vault/protect_secrets_using_Vault_and_External_Secrets_Operator/readme/#utilisation-de-notre-helm-repo","title":"Utilisation de notre Helm repo","text":"<p>Pour cela, rien de plus simple, il suffit de suivre les indications dans le README.md :</p> <pre><code>helm repo add papafrancky https://raw.githubusercontent.com/papafrancky/helm-charts/main/packages\nhelm search repo papafrancky\n\n    NAME                        CHART VERSION   APP VERSION DESCRIPTION\n    papafrancky/silly-webapp    0.1.0           2.4         A simple web application exposing credentials r...\n\n\nhelm test silly-webapp\n\n    NAME: silly-webapp\n    LAST DEPLOYED: Mon Mar 11 21:31:21 2024\n    NAMESPACE: silly-webapp\n    STATUS: deployed\n    REVISION: 1\n    TEST SUITE:     silly-webapp-test-connection\n    Last Started:   Sat Mar 16 11:15:43 2024\n    Last Completed: Sat Mar 16 11:15:46 2024\n    Phase:          Succeeded\n    NOTES:\n    'silly-webapp' really is... a silly webapp : it consists in a simple Apache 2.4 webserver exposing a webpage     which     contains credentials retrieved from Vault through the 'External Secrets Operator (ESO)'.\n\n    To see it in action :\n\n        kubectl --namespace silly-webapp port-forward service/silly-webapp 8080:80\n        Visit http://127.0.0.1:8080 to see the secrets.\n</code></pre> <p>L'application est correctement d\u00e9ploy\u00e9e et fonctionne comme attendu. Nous pouvons d\u00e9sormais la confier \u00e0 FluxCD.</p> <p>Commen\u00e7ons par faire du m\u00e9nage :</p> <pre><code>kubectl delete ns silly-webapp\n</code></pre>"},{"location":"Vault/protect_secrets_using_Vault_and_External_Secrets_Operator/readme/#gestion-de-silly-webapp-par-fluxcd","title":"Gestion de 'silly-webapp' par FluxCD","text":""},{"location":"Vault/protect_secrets_using_Vault_and_External_Secrets_Operator/readme/#alerting-discord_1","title":"alerting Discord","text":""},{"location":"Vault/protect_secrets_using_Vault_and_External_Secrets_Operator/readme/#creation-du-salon-prive-sur-le-client-discord_1","title":"cr\u00e9ation du salon priv\u00e9 sur le client Discord","text":"<p>Cr\u00e9ation d'un nouveau salon (priv\u00e9) :    - nom : silly-webapp-development   - webhook :     - nom : FluxCD     - URL : https://discord.com/api/webhooks/1218504455075401788/2NJmyKGT86cz6dC18-o3D2B9d4dT69OUzLLtnHZZqIiZeUP4zHfNxxV-b8UBKQVXGicM</p>"},{"location":"Vault/protect_secrets_using_Vault_and_External_Secrets_Operator/readme/#on-place-le-webhook-du-salon-discord-dans-un-secret-kubernetes_1","title":"on place le webhook du salon discord dans un secret kubernetes","text":"<pre><code># Travaillons sur la copie locale du repo git de notre cluster de d\u00e9veloppement\ncd ~/code/github/kubernetes-development\ngit pull\nmkdir products/silly-webapp\n\n# Cr\u00e9ation du namespace de l'application :\nkubectl create ns silly-webapp --dry-run=client -o yaml | grep -vE \"creationTimestamp|spec|status\" &gt; products/silly-webapp/namespace.yaml\nkubectl apply -f products/silly-webapp/namespace.yaml\n\n# Cr\u00e9ation du secret contenant le webhook du salon Discord :\nDISCORD_WEBHOOK=\"https://discord.com/api/webhooks/1218504455075401788/2NJmyKGT86cz6dC18-o3D2B9d4dT69OUzLLtnHZZqIiZeUP4zHfNxxV-b8UBKQVXGicM\"\nkubectl -n silly-webapp create secret generic discord-webhook --from-literal=address=${DISCORD_WEBHOOK} --dry-run=client -o yaml &gt; products/silly-webapp/discord-webhook.secret.yaml\n</code></pre>"},{"location":"Vault/protect_secrets_using_Vault_and_External_Secrets_Operator/readme/#definition-de-lalert-provider-discord_1","title":"d\u00e9finition de l'alert-provider Discord","text":"<pre><code>flux create alert-provider discord \\\n  --type=discord \\\n  --secret-ref=discord-webhook \\\n  --channel=silly-webapp-development \\\n  --username=FluxCD \\\n  --namespace=silly-webapp \\\n  --export &gt; products/silly-webapp/notification-provider.yaml\n</code></pre>"},{"location":"Vault/protect_secrets_using_Vault_and_External_Secrets_Operator/readme/#configuration-des-alertes-discord_1","title":"configuration des alertes Discord","text":"<pre><code>flux create alert discord \\\n  --event-severity=info \\\n  --event-source='GitRepository/*,Kustomization/*,ImageRepository/*,ImagePolicy/*,HelmRepository/*,HelmRelease/*' \\\n  --provider-ref=discord \\\n  --namespace=silly-webapp \\\n  --export &gt; products/silly-webapp/notification-alert.yaml\n</code></pre>"},{"location":"Vault/protect_secrets_using_Vault_and_External_Secrets_Operator/readme/#poussons-les-manifests-sur-le-repo-central-pour-que-fluxcd-les-gere_1","title":"Poussons les manifests sur le repo central pour que FluxCD les g\u00e8re :","text":"<pre><code>git add products/silly-webapp\ngit commit -m 'feat: configuring discord alerting for silly-webapp.'\ngit push\n\nflux reconcile kustomization flux-system --with-source\nflux events -w\n</code></pre>"},{"location":"Vault/protect_secrets_using_Vault_and_External_Secrets_Operator/readme/#gestion-du-repo-helm_1","title":"Gestion du repo Helm","text":"<pre><code>flux create source helm papafrancky \\\n  --url=https://raw.githubusercontent.com/papafrancky/helm-charts/main/packages \\\n  --namespace=silly-webapp \\\n  --interval=1m \\\n  --export &gt; products/silly-webapp/helm-repository.yml\n</code></pre>"},{"location":"Vault/protect_secrets_using_Vault_and_External_Secrets_Operator/readme/#deploiement-de-silly-webapp-helm-release","title":"D\u00e9ploiement de silly-webapp (helm release)","text":"<p>Nous installerons l'application avec les param\u00e8tres par d\u00e9faut, raison pour laquelle le param\u00e8tre --values est comment\u00e9.</p> <pre><code>flux create helmrelease silly-webapp \\\n  --source=HelmRepository/papafrancky \\\n  --chart=silly-webapp \\\n  #--values=products/silly-webapp/custom-values.txt \\\n  --namespace=silly-webapp \\\n  --export &gt; products/silly-webapp/helm-release.yaml\n\n\ngit status\ngit add products/silly-webapp\ngit commit -m 'feat: managing silly-webapp helm deployment.'\ngit push\n\nflux reconcile kustomization flux-system --with-source\nflux events -w\nhelm test silly-webapp\nkubectl --namespace silly-webapp port-forward service/silly-webapp 8080:80\n</code></pre> <p>Tout fonctionne comme attendu ! ^^</p>"},{"location":"Vault/protect_secrets_using_Vault_and_External_Secrets_Operator/readme/#protection-de-webhook-utilise-pour-la-notification-discord","title":"Protection de webhook utilis\u00e9 pour la notification Discord","text":"<p>Si on regarde bien, nous avons 1 secret qui n'est pas 'prot\u00e9g\u00e9' dans Vault : discord-webhook (le webhook utilis\u00e9 pour nos notifications Discord)</p> <p>Commen\u00e7ons par cr\u00e9er ins\u00e9rer ce secret dans Vault.</p>"},{"location":"Vault/protect_secrets_using_Vault_and_External_Secrets_Operator/readme/#ajout-du-webhook-dans-vault","title":"Ajout du webhook dans Vault","text":"<pre><code># Login avec le root token sur Vault (dans le pod vault-0) :\nkubectl -n vault exec -it vault-0 -- sh    # ouverture d'une session shell sur le pod vault-0\nvault login hvs.G145zNl012ApNOap3sn2zhIG   # login \u00e0 Vault avec le token root\n\n# Ecriture des secrets dans le path /kv/silly-webapp/discord-notifications :\nvault kv put -mount=kv silly-webapp/discord-notifications \\\n  WEBHOOK=https://discord.com/api/webhooks/1218504455075401788/2NJmyKGT86cz6dC18-o3D2B9d4dT69OUzLLtnHZZqIiZeUP4zHfNxxV-b8UBKQVXGicM\nvault kv get -mount kv silly-webapp/discord-notifications\n</code></pre>"},{"location":"Vault/protect_secrets_using_Vault_and_External_Secrets_Operator/readme/#modification-de-la-vault-policy-pour-silly-webapp","title":"Modification de la Vault policy pour silly-webapp","text":"<pre><code># Nous devons modifier la policy d'acc\u00e8s en lecture sur le path /kv/silly-webapp qui \u00e9tait trop restrictive :\nvault policy write kv-silly-webapp-read - &lt;&lt; EOF\npath \"kv/metadata/silly-webapp\" {\n  capabilities = [\"list\",\"read\"]\n}\npath \"kv/data/silly-webapp\" {\n  capabilities = [\"list\",\"read\"]\n}\npath \"kv/metadata/silly-webapp/*\" {\n  capabilities = [\"list\",\"read\"]\n}\npath \"kv/data/silly-webapp/*\" {\n  capabilities = [\"list\",\"read\"]\n}    \nEOF\n\nvault policy read kv-silly-webapp-read\n\n# Fin de session\nexit\n\n\ncat &lt;&lt; EOF &gt;&gt; products/silly-webapp/discord-webhook.externalsecret.yaml\napiVersion: external-secrets.io/v1beta1\nkind: ExternalSecret\nmetadata:\n  name: discord-webhook\n  namespace: silly-webapp\nspec:\n  secretStoreRef:\n    name: vault\n    kind: SecretStore\n  target:\n    name: discord-webhook\n  data:\n  - secretKey: discord-webhook\n    remoteRef:\n      key: silly-webapp/discord-notifications\n      property: WEBHOOK\nEOF\n\nk delete secret discord-silly-webapp-development-webhook\nrm -f products/silly-webapp/discord-webhook.secret.yaml\n</code></pre> <p>Pour tester : </p> <pre><code>helm uninstall silly-webapp\n</code></pre> <p>-&gt; Si lors du reconcile on est alert\u00e9 par Discord que le Helm Release sille-webapp est r\u00e9install\u00e9e, tout aura bien march\u00e9.</p> <pre><code>k get externalsecret,secret\n\n    NAME                                                 STORE   REFRESH INTERVAL   STATUS         READY\n    externalsecret.external-secrets.io/discord-webhook   vault   1h0m0s             SecretSynced   True\n    externalsecret.external-secrets.io/webindex          vault   1h                 SecretSynced   True\n\n    NAME                                              TYPE                 DATA   AGE\n    secret/discord-silly-webapp-development-webhook   Opaque               1      2m38s\n    secret/discord-webhook                            Opaque               1      35m\n    secret/sh.helm.release.v1.silly-webapp.v1         helm.sh/release.v1   1      2m24s\n    secret/webindex                                   Opaque               1      2m24s\n</code></pre> <p>L'external secret 'discord-webhook' aura cr\u00e9\u00e9 un secret 'discord-webhook'. V\u00e9rifions qu'il correspond bien \u00e0 notre webhook :</p> <pre><code>k get secret discord-webhook -o yaml | yq .data | awk '{print $2}' | base64 -d\n\n    https://discord.com/api/webhooks/1218504455075401788/2NJmyKGT86cz6dC18-o3D2B9d4dT69OUzLLtnHZZqIiZeUP4zHfNxxV-b8UBKQVXGicM%\n</code></pre> <p>C'est bon ^^</p>"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/kind_vault_auto-unsealed_eso_fluxcd/","title":"D\u00e9ploiement de Vault (auto-unsealed) et ESO via FluxCD sur un cluster KinD","text":""},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/kind_vault_auto-unsealed_eso_fluxcd/#abstract","title":"Abstract","text":"<p>Ce howto fait suite au howto 'kube-prometheus-stack' managed with FluxCD.</p> <p>Jusqu'\u00e0 pr\u00e9sent, nous disposons d'un cluster KinD pilot\u00e9 par FluxCD et sur lequel nous avons d\u00e9ploy\u00e9 une stack de monitoring Prometheus compl\u00e8te. Nous continuons l'enrichissement de notre cluster en lui ajoutant cette fois-ci une solution de protection de nos donn\u00e9es sensibles (ie. des 'secrets') : HashiCorp Vault OSS.</p> <p>Pour interagir avec ce dernier, nous d\u00e9ploierons \u00e9galement l'External Secrets Operator (ESO).</p> <p>Pour illustrer le bon fonctionnement de ces outils, nous confierons \u00e0 Vault le login et le mot de passe du compte d'administration de Grafana.</p> <p>Tip</p> <p>Nous nous inspirerons fortement des howtos que nous avons d\u00e9j\u00e0 produits sur Vault et External Secrets Operator.</p>"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/kind_vault_auto-unsealed_eso_fluxcd/#preparatifs","title":"Pr\u00e9paratifs","text":"<p>Nous commencerons par pr\u00e9parer notre environnement local, un namespace d\u00e9di\u00e9 \u00e0 la gestion des secrets, l'alerting Discord et d\u00e9finir les d\u00e9p\u00f4ts Helm avant de nous atteler \u00e0 Vault et ESO.</p>"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/kind_vault_auto-unsealed_eso_fluxcd/#preparation-de-notre-environnement-de-developpement-local","title":"Pr\u00e9paration de notre environnement de d\u00e9veloppement (local)","text":"<pre><code># R\u00e9pertoire accueillant nos d\u00e9p\u00f4ts Git en local\nexport LOCAL_GITHUB_REPOS=\"${HOME}/code/github\"\n\n# Mise \u00e0 jour des copies locales des d\u00e9p\u00f4ts d\u00e9di\u00e9s \u00e0 FluxCD et aux applications qu'il g\u00e8re\ncd ${LOCAL_GITHUB_REPOS}/k8s-kind-apps   &amp;&amp; git pull\ncd ${LOCAL_GITHUB_REPOS}/k8s-kind-fluxcd &amp;&amp; git pull\n\n# Cr\u00e9ation d'un r\u00e9pertoire d\u00e9di\u00e9 \u00e0 la gestion des secrets\nmkdir -p ${LOCAL_GITHUB_REPOS}/k8s-kind-fluxcd/apps/vault\n</code></pre>"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/kind_vault_auto-unsealed_eso_fluxcd/#namespace-dedie-a-la-gestion-des-secrets","title":"Namespace d\u00e9di\u00e9 \u00e0 la gestion des secrets","text":"<pre><code>kubectl create ns vault --dry-run=client -o yaml &gt; ${LOCAL_GITHUB_REPOS}/k8s-kind-fluxcd/apps/vault/namespace.yaml\nkubectl apply -f ${LOCAL_GITHUB_REPOS}/k8s-kind-fluxcd/apps/vault/namespace.yaml\n</code></pre>"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/kind_vault_auto-unsealed_eso_fluxcd/#alerting-discord","title":"Alerting Discord","text":"<p>Nous passerons vite sur cette partie, car nous l'avons d\u00e9j\u00e0 bien document\u00e9e dans les howtos pr\u00e9c\u00e9dents.</p> <p>Nous utiliserons notre serveur Discord 'k8s-kind' d\u00e9j\u00e0 existant et partirons du principe que vous avez d\u00e9j\u00e0 cr\u00e9\u00e9 un salon textuel priv\u00e9 nomm\u00e9 'vault' ainsi qu'un webhook 'FluxCD' associ\u00e9.</p>"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/kind_vault_auto-unsealed_eso_fluxcd/#webhook-du-salon-discord","title":"webhook du salon Discord","text":"code'discord-webhook' secret <pre><code>export LOCAL_GITHUB_REPOS=\"${HOME}/code/github\"\nexport WEBHOOK_VAULT=\"https://discord.com/api/webhooks/1243971721745399809/G49lALsZgmXriz5xzJ0GqJ9WizUt9ADc38VrVN_yjENerABboe8k_JGcfG8MXSsiTLyJ\"\n\ncd ${LOCAL_GITHUB_REPOS}/k8s-kind-fluxcd\n\nkubectl -n vault create secret generic discord-webhook --from-literal=address=${WEBHOOK_VAULT} --dry-run=client -o yaml &gt; apps/vault/discord-webhook.secret.yaml\nkubectl apply -f apps/vault/discord-webhook.secret.yaml\n</code></pre> <pre><code>apiVersion: v1\ndata:\n  address: aHR0cHM6Ly9kaXNjb3JkLmNvbS9hcGkvd2ViaG9va3MvMTI0Mzk3MTcyMTc0NTM5OTgwOS9HNDlsQUxzWmdtWHJpejV4ekowR3FKOVdpelV0OUFEYzM4VnJWTl95akVOZXJBQmJvZThrX0pHY2ZHOE1YU3NpVEx5Sg==\nkind: Secret\nmetadata:\n  creationTimestamp: null\n  name: discord-webhook\n  namespace: vault\n</code></pre>"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/kind_vault_auto-unsealed_eso_fluxcd/#alert-provider","title":"Alert-provider","text":"code'discord-webhook' alert-provider <pre><code>export LOCAL_GITHUB_REPOS=\"${HOME}/code/github\"\n\ncd ${LOCAL_GITHUB_REPOS}/k8s-kind-fluxcd\n\nflux create alert-provider discord \\\n  --type=discord \\\n  --secret-ref=discord-webhook \\\n  --channel=vault \\\n  --username=FluxCD \\\n  --namespace=vault \\\n  --export &gt; apps/vault/notification-provider.yaml\n</code></pre> <pre><code>---\napiVersion: notification.toolkit.fluxcd.io/v1beta2\nkind: Provider\nmetadata:\n  name: discord\n  namespace: vault\nspec:\n  channel: vault\n  secretRef:\n    name: discord-webhook\n  type: discord\n  username: FluxCD\n</code></pre>"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/kind_vault_auto-unsealed_eso_fluxcd/#alert","title":"Alert","text":"code'discord' alert <pre><code>export LOCAL_GITHUB_REPOS=\"${HOME}/code/github\"\n\ncd ${LOCAL_GITHUB_REPOS}/k8s-kind-fluxcd\n\nflux create alert discord \\\n  --event-severity=info \\\n  --event-source='GitRepository/*,Kustomization/*,ImageRepository/*,ImagePolicy/*,HelmRepository/*,HelmRelease/*' \\\n  --provider-ref=discord \\\n  --namespace=vault \\\n  --export &gt; apps/vault/notification-alert.yaml\n</code></pre> <pre><code>---\napiVersion: notification.toolkit.fluxcd.io/v1beta2\nkind: Alert\nmetadata:\n  name: discord\n  namespace: vault\nspec:\n  eventSeverity: info\n  eventSources:\n  - kind: GitRepository\n    name: '*'\n  - kind: Kustomization\n    name: '*'\n  - kind: ImageRepository\n    name: '*'\n  - kind: ImagePolicy\n    name: '*'\n  - kind: HelmRepository\n    name: '*'\n  - kind: HelmRelease\n    name: '*'\n  providerRef:\n    name: discord\n</code></pre>"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/kind_vault_auto-unsealed_eso_fluxcd/#activation-de-lalerting","title":"Activation de l'alerting","text":"<pre><code>export LOCAL_GITHUB_REPOS=\"${HOME}/code/github\"\n\ncd ${LOCAL_GITHUB_REPOS}/k8s-kind-fluxcd\n\ngit add .\ngit commit -m \"feat: setting up 'vault' Discord alerting.\"\ngit push\n\nflux reconcile kustomization flux-system --with-source\n</code></pre> <p>V\u00e9rification :</p> codeoutput <pre><code>kubectl -n vault get providers,alerts\n</code></pre> <pre><code>NAME                                              AGE   READY   STATUS\nprovider.notification.toolkit.fluxcd.io/discord   70s   True    Initialized\n\nNAME                                           AGE   READY   STATUS\nalert.notification.toolkit.fluxcd.io/discord   70s   True    Initialized\n</code></pre>"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/kind_vault_auto-unsealed_eso_fluxcd/#helm-repositories","title":"Helm repositories","text":"<p>Nous allons d\u00e9finir au niveau de FluxCD les 'Helm registries' pour installer sur notre cluster l'External Secrets Operator et HashiCorp Vault OSS :</p> code'hashicorp' helm repository'external-secrets' helm repository <pre><code>export LOCAL_GITHUB_REPOS=\"${HOME}/code/github\"\n\ncd ${LOCAL_GITHUB_REPOS}/k8s-kind-fluxcd\n\nflux create source helm hashicorp \\\n  --url=https://helm.releases.hashicorp.com \\\n  --namespace=vault \\\n  --interval=1m \\\n  --export &gt; apps/vault/vault.helm-repository.yaml\n\nflux create source helm external-secrets \\\n  --url=https://charts.external-secrets.io \\\n  --namespace=vault \\\n  --interval=1m \\\n  --export &gt; apps/vault/external-secrets.helm-repository.yaml\n</code></pre> <pre><code>---\napiVersion: source.toolkit.fluxcd.io/v1beta2\nkind: HelmRepository\nmetadata:\n  name: hashicorp\n  namespace: vault\nspec:\n  interval: 1m0s\n  url: https://helm.releases.hashicorp.com\n</code></pre> <pre><code>---\napiVersion: source.toolkit.fluxcd.io/v1beta2\nkind: HelmRepository\nmetadata:\n  name: external-secrets\n  namespace: vault\nspec:\n  interval: 1m0s\n  url: https://charts.external-secrets.io\n</code></pre>"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/kind_vault_auto-unsealed_eso_fluxcd/#prise-en-compte-des-changements","title":"Prise en compte des changements","text":"<p>Il est temps de soumettre nos changements \u00e0 FluxCD :</p> <pre><code>export LOCAL_GITHUB_REPOS=\"${HOME}/code/github\"\n\ncd ${LOCAL_GITHUB_REPOS}/k8s-kind-fluxcd\n\ngit add .\ngit commit -m \"feat: preparing vault -&gt; discord alerting, helm repositories.\" \ngit push\n\nflux reconcile kustomization flux-system --with-source\n</code></pre> <p>Discord nous informe tout de suite de la bonne cr\u00e9ation du 'Helm registry' :</p> <p></p>"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/kind_vault_auto-unsealed_eso_fluxcd/#google-cloud-platform","title":"Google Cloud Platform","text":"<p>Le m\u00e9canisme d'auto-unseal de Vault repose sur les service d'un Cloud Service Provider (CSP). Notre choix s'est port\u00e9 sur Google Cloud Platform (CGP) mais tout autre CSP proposant un service de gestion de cl\u00e9s aurait pu faire l'affaire.</p>"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/kind_vault_auto-unsealed_eso_fluxcd/#compte-gcp-projet-etc","title":"Compte GCP, projet, etc...","text":"<p>Nous disposons d'un compte GCP et avons pr\u00e9alablement cr\u00e9\u00e9 un projet dont voici les informations essentielles :</p> KEY VALUE Project Name vault Project ID vault-415918"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/kind_vault_auto-unsealed_eso_fluxcd/#activation-des-apis","title":"Activation des APIs","text":"<p>Pour consommer les services GCP, il faut activer leurs APIs.</p> <p>Tip</p> <p>APIs &amp; Services &gt; Enabled APIs &amp; Services &gt; + ENABLE APIS AND SERVICES</p> APIs activ\u00e9es Cloud Key Management Service (KMS) API Compute Engine API"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/kind_vault_auto-unsealed_eso_fluxcd/#service-account","title":"Service-account","text":"<p>Vault utilisera un service-account GCP (en fournissant ses credentials) qui disposera des droits d'acc\u00e8s \u00e0 une cl\u00e9 h\u00e9berg\u00e9e chez GCP (via Key Management Service KMS). Param\u00e9tr\u00e9 en mode auto-unseal, Vault se servira de cette cl\u00e9 comme \"root key\" qui prot\u00e8ge l'\"encryption key\".</p>"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/kind_vault_auto-unsealed_eso_fluxcd/#service-account_1","title":"Service-account","text":"<p>Tip</p> <p>IAM &amp; Admin &gt; Service Accounts &gt; + CREATE SERVICE ACCOUNT</p> KEY VALUE Name k8s-kind-vault Email k8s-kind-vault@vault-415918.iam.gserviceaccount.com Key yes"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/kind_vault_auto-unsealed_eso_fluxcd/#service-account-key","title":"Service-account key","text":"<p>Vault aura besoin de la cl\u00e9 priv\u00e9e du service account cr\u00e9\u00e9 pr\u00e9c\u00e9demment pour consommer les APIs de GCP avec les privil\u00e8ges associ\u00e9s \u00e0 ce compte.</p> <p>Tip</p> <p>IAM &amp; Admin &gt; Service Accounts &gt; KEYS &gt; ADD KEY (key type: JSON)</p> <p>La cr\u00e9ation d'une cl\u00e9 d\u00e9clenche le t\u00e9l\u00e9chargement d'un fichier texte au format JSON que nous placerons temporairement \u00e0 l'endroit suivant : ~/tmp/k8s-kind-vault.creds.json</p> service-account key <pre><code>{\n  \"type\": \"service_account\",\n  \"project_id\": \"vault-415918\",\n  \"private_key_id\": \"75f932e7ca96f31247f5328055a7d7d3802bab92\",\n  \"private_key\": \"-----BEGIN PRIVATE KEY-----\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQDIGOQ0njkgaciE\\nNZfVZ0yObQ9nt8l7CzqCeKPcmk5gaxPxm1/fiXhjynqxdcgpzppzJE5gLA3uwhOf\\nVmRVrF9aobinFXZ8iKVbi6tSPSnxEPXreOuhuwicFfsX81UeG    +MozSodj04nKKuL\\nmJdqkesTuRcFRu/2hSojtOG1dyyaOQSZ1hDCRq+dlnoVaJR7ADGJOvwuoPs1EeHo\\nnRavuvTGsSDHqLQwUe20sfTJSVKTXF1S21RDmpxZqEHrETHNzHc8irMMvUteDA28\\nLw6lIy9Ahn6+nxtrBRGyv5K7l1LQg4mdYkAw/REOW83UEWff2Job6v/VWm1lwef8\\nDJsytsPbAgMBAAECggEAATxU+QNa1qv5tJhy/    N9ik8lxfDJXWuWYbQvWiFs4u0Gy\\nmIvK8ergaU5+FOdTKB3LOGDPKWG8Q7gxGWaoLWRoZla9Cwn1mzb8PUnFqO3sn2HE\\nt5TUlWXQJMxUPMV7xhSKSwIRVvEbLuAm/edE5vbck8Z11hOBpCPxhj812sJQEuoD\\nkd0NwiqBtCjJRz/S7f9c6z9zu3RxhqppleqFG5L3T50OCpJxIIDC976SQlkCeml6\\nHxGScFZjua    +VTcZVuM8NVVx71iRVUi77DTBqGaCMGjiWo4oxo9YyhD62q7oBdRu9\\nfbS3beSlr1scijFwNr0uzcjpowCz+OjUXzhGds//OQKBgQDvCKkhMDHxF5deoeCf\\n5ib+ywIeLRWDwZB8249P/WNhiILvsW144iBuWwZFGJ2N/FMulC7MKlVFsfKrlvMm\\nedLh+/    LG9xmxzpOUDvtKPXzwWqvt70hhv4Oo1rpm7LkXfZVxUctFPEzToxqiN56I\\nxtDso30w8oXJo2apb7ro4bHd5wKBgQDWTLzDPmraS6xUFjYQKjcMZ2tNe7IQXiDR\\nXqz7UbJfdZzsvKWCRpU7cDEPhtfimFRMfaAaF9feVV+ocip4qt3uoatQzjwcn0Ys\\nwpg/0LG2Uwcc+RoohtSXenZzMB+J3jxsJD2dlgeG4fC47YTG8uqwYe1QysJ    +DTZg\\n8gVEpmRj7QKBgCWG9Y6ZU23nZ0NbJLnV109vLcDxEQyjafzAN6q2PFEGro/VCjvN\\nPIw2zDAy4iF1eNW6O/Kfvs13V4Lq6veibqI9/OqRxr3skazQAVGxf5j4kz+Crply\\nCMiMFa2tAo4WkEy/K6uOAP3FAJxxIPmWRRyxuijiGnECr05wlSaUsGkHAoGAe0Wr\\nM9i82JO9LqWUNdpCzkTTab/k3xt2X1nJwcvuApGCUn/    16Sm3AHj6D8duei9MFrAR\\nH9FlYMTVgO0jV0Ra48Fl7dakp4ZLdMX/lH31LD84kUcN8BAXTIeqiXo+Oi13rnFu\\nbC74Z3Oi6I3g2hy0OgAq5lWsaZwqErxFoYbhqsUCgYEAtpoLNzhMqGj31yUUP05p\\n2mDn62OKfwtO0pHqv++unJ9edzjGHBGlVcHk4E2TvagHdaWLBhkyhD4dEvTHAW4G\\nIJ5Xf4FgdAeh0ypdM7g7UlluatQC/2z    +S32jlTATpx412mq1SXWJy6AzXHPFdDv8\\nm3ADd7UI+ACitGZW+vFSlmQ=\\n-----END PRIVATE KEY-----\\n\",\n  \"client_email\": \"k8s-kind-vault@vault-415918.iam.gserviceaccount.com\",\n  \"client_id\": \"117555050512332525003\",\n  \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n  \"token_uri\": \"https://oauth2.googleapis.com/token\",\n  \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n  \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/k8s-kind-vault%40vault-415918.iam.gserviceaccount.com\",\n  \"universe_domain\": \"googleapis.com\"\n}\n</code></pre> <p>Nous allons tout de suite int\u00e9grer cette cl\u00e9 sous la forme de secret Kubernetes dans le namespace d\u00e9di\u00e9 \u00e0 Vault :</p> <pre><code>kubectl -n vault create secret generic kms-sa --from-file=/Users/franck/tmp/k8s-kind-vault.creds.json\nkubectl -n vault get secret kms-sa -o jsonpath='{.data.k8s-kind-vault\\.creds\\.json}' | base64 -d\nkubectl -n vault get secret kms-sa -o jsonpath='{.data.k8s-kind-vault\\.creds\\.json}' | base64 -d | yq -r '.private_key'\n</code></pre>"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/kind_vault_auto-unsealed_eso_fluxcd/#kms-key","title":"KMS key","text":"<p>Il faut d'abord cr\u00e9er un trousseau (ie. un 'key ring') avant d'y ajouter une cl\u00e9.</p>"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/kind_vault_auto-unsealed_eso_fluxcd/#key-ring","title":"Key ring","text":"<p>Tip</p> <p>Security &gt; Key Management &gt; + CREATE KEY RING</p> KEY VALUE Key ring name k8s-kind-vault Single/multi region single Region europe-west9"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/kind_vault_auto-unsealed_eso_fluxcd/#kms-key_1","title":"KMS key","text":"<p>Tip</p> <p>Security &gt; Key Management &gt; k8s-kind-vault &gt; + CREATE KEY</p> KEY VALUE Key name k8s-kind-vault Protection level software Key material generated Purpose and algorithm symmetric encrypt/decrypt Key rotation 180d"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/kind_vault_auto-unsealed_eso_fluxcd/#acces-du-service-account-a-la-cle","title":"Acc\u00e8s du service account \u00e0 la cl\u00e9","text":"<p>Il nous reste \u00e0 autoriser notre service account 'k8s-kind-vault@vault-415918.iam.gserviceaccount.com' \u00e0 acc\u00e9der \u00e0 la cl\u00e9 que nous venons de cr\u00e9er et de rattacher \u00e0 son trousseau.</p> <p>Tip</p> <p>Security &gt; Key Management &gt; k8s-kind-vault (key ring) &gt; k8s-kind-vault (key) &gt; PERMISSIONS &gt; + GRANT ACCESS</p> KEY VALUE Principal k8s-kind-vault@vault-415918.iam.gserviceaccount.com Role Cloud KMS Viewer Role Cloud KMS CryptoKey Encrypter/Decrypter <p>Nous en avons fini avec les pr\u00e9paratifs c\u00f4t\u00e9 GCP ^^</p> <p>--- reprendre ici ---</p> <p>Nous allons maintenant cr\u00e9er un service-account dans GCP et lui donner acc\u00e8s \u00e0 une cl\u00e9 KMS que Vault utilisera pour son auto-unsealing.</p> <p>Info</p> <p>https://developer.hashicorp.com/vault/tutorials/auto-unseal/autounseal-gcp-kms</p>"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/kind_vault_auto-unsealed_eso_fluxcd/#mise-en-place-de-vault-en-mode-auto-unseal","title":"Mise en place de Vault en mode 'auto-unseal'","text":"<p>Nous couvrirons dans cette section l'installation de Vault, son initialisation et son unsealing.</p>"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/kind_vault_auto-unsealed_eso_fluxcd/#custom-values","title":"'Custom values'","text":"<p>Pour configurer Vault en mode 'auto-unseal', nous devons modifier la configuration par d\u00e9faut du Helm Chart.</p>"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/kind_vault_auto-unsealed_eso_fluxcd/#recuperation-des-default-values","title":"R\u00e9cup\u00e9ration des 'Default values'","text":"<pre><code>export LOCAL_GITHUB_REPOS=\"${HOME}/code/github\"\n\ncd ${LOCAL_GITHUB_REPOS}/k8s-kind-fluxcd\n\nhelm show values hashicorp/vault &gt; apps/vault/vault.default.values.txt\n</code></pre> <p>Warning</p> <p>Bien que le fichier r\u00e9cup\u00e9r\u00e9 soit en YAML, nous modifierons son extention en .TXT pour qu'il ne soit pas interpr\u00e9t\u00e9 par FluxCD.</p>"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/kind_vault_auto-unsealed_eso_fluxcd/#creation-du-fichier-custom-values","title":"Cr\u00e9ation du fichier 'Custom values'","text":"<p>Dans le m\u00eame r\u00e9pertoire, nous cr\u00e9erons notre fichier 'values' sur la base du fichier que nous venons de r\u00e9cup\u00e9rer, et le nommerons 'vault.custom.values.txt'</p> <p>Nous d\u00e9ploierons ici Vault en mode 'standalone', ce qui ne se pr\u00eate pas \u00e0 un contexte de production.</p> <p>La cl\u00e9 priv\u00e9e du service-account GCP 'k8s-kind-vault est transmise dans les 'extraEnvironmentVars', r\u00e9cup\u00e9r\u00e9s depuis le secret Kubernetes 'kms-sa' et mont\u00e9 dans '/vault/userconfig'.</p> <pre><code>  global:\n    enabled: false\n    namespace: \"vault\"\n\n  injector:\n    enabled: false\n\n  server:\n    enabled: true\n    extraEnvironmentVars:\n      GOOGLE_REGION: europe-west9\n      GOOGLE_PROJECT: vault-415918\n      GOOGLE_APPLICATION_CREDENTIALS: /vault/userconfig/kms-sa/k8s-kind-vault.creds.json\n    extraVolumes:\n      - type: secret\n        name: kms-sa\n        path: /vault/userconfig\n    dataStorage:\n      size: 1Gi\n    standalone:\n      enabled: true\n      config: |\n        ui = true\n\n        listener \"tcp\" {\n          tls_disable = 1\n          address = \"[::]:8200\"\n          cluster_address = \"[::]:8201\"\n          # Enable unauthenticated metrics access (necessary for Prometheus Operator)\n          #telemetry {\n          #  unauthenticated_metrics_access = \"true\"\n          #}\n        }\n        storage \"file\" {\n          path = \"/vault/data\"\n        }\n\n        seal \"gcpckms\" {\n           project     = \"vault-helm-dev-246514\"\n           region      = \"euope-west9\"\n           key_ring    = \"k8s-kind-vault\"\n           crypto_key  = \"k8s-kind-vault\"\n        }\n    serviceAccount:\n      create: true\n      name: \"vault\"\n\n  ui:\n    enabled: true\n</code></pre>"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/kind_vault_auto-unsealed_eso_fluxcd/#helm-release","title":"Helm release","text":""},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/kind_vault_auto-unsealed_eso_fluxcd/#installation-de-la-release","title":"Installation de la Release","text":"<p>Nous pouvons d\u00e9sormais d\u00e9finir notre 'helm release' pour que FluxCD puiss eg\u00e9rer le d\u00e9ploiement de Vault :</p> code'vault' helm release <pre><code>export LOCAL_GITHUB_REPOS=\"${HOME}/code/github\"\n\ncd ${LOCAL_GITHUB_REPOS}/k8s-kind-fluxcd\n\nflux create helmrelease vault \\\n  --source=HelmRepository/hashicorp \\\n  --chart=vault \\\n  --namespace=vault \\\n  --from-values=ConfigMap/vault-values\n  --export &gt; ./apps/vault/vault.helm-release.yaml\n</code></pre> <pre><code>---\napiVersion: helm.toolkit.fluxcd.io/v2\nkind: HelmRelease\nmetadata:\n  name: vault\n  namespace: vault\nspec:\n  chart:\n    spec:\n      chart: vault\n      reconcileStrategy: ChartVersion\n      sourceRef:\n        kind: HelmRepository\n        name: hashicorp\n  interval: 1m0s\n  valuesFrom:\n  - kind: ConfigMap\n    name: vault-values\n</code></pre> <p>Poussons les modifications jusqu'\u00e0 FluxCD :</p> <pre><code>export LOCAL_GITHUB_REPOS=\"${HOME}/code/github\"\n\ncd ${HOME}/code/github/k8s-kind-fluxcd\n\ngit add .\ngit commit -m \"feat: vault helm release with custom values\"\ngit push\n\nflux reconcile kustomization flux-system --with-source\n</code></pre> <p>Discord nous informe tout de suite de la cr\u00e9ation de la Helm Release nomm\u00e9e 'vault' dans le namespace 'vault' (\"helmrelease/vault.vault\") :</p> <p></p> <p>Regardons l'\u00e9tat de nos objets dans le namespace 'vault' :</p>  codeoutput <pre><code>kubectl -n vault get all\n</code></pre> <pre><code>NAME                                        READY   STATUS    RESTARTS   AGE\npod/vault-0                                 0/1     Running   0          9s\npod/vault-agent-injector-755c8bb799-j7f9w   1/1     Running   0          10s\n\nNAME                               TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)             AGE\nservice/vault                      ClusterIP   10.96.166.217   &lt;none&gt;        8200/TCP,8201/TCP   10s\nservice/vault-active               ClusterIP   10.96.17.218    &lt;none&gt;        8200/TCP,8201/TCP   10s\nservice/vault-agent-injector-svc   ClusterIP   10.96.181.53    &lt;none&gt;        443/TCP             10s\nservice/vault-internal             ClusterIP   None            &lt;none&gt;        8200/TCP,8201/TCP   10s\nservice/vault-standby              ClusterIP   10.96.123.15    &lt;none&gt;        8200/TCP,8201/TCP   10s\n\nNAME                                   READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/vault-agent-injector   1/1     1            1           10s\n\nNAME                                              DESIRED   CURRENT   READY   AGE\nreplicaset.apps/vault-agent-injector-755c8bb799   1         1         1       10s\n\nNAME                     READY   AGE\nstatefulset.apps/vault   0/1     10s\n</code></pre> <p>Nous voyons que le pod 'vault-0' \u00e0 un status 'Running' mais qu'il n'est pas 'ready'. V\u00e9rifions l'\u00e9tat de Vault sur le pod : </p> codeoutput <pre><code>kubectl -n vault exec -it vault-0 -- vault status\n</code></pre> <pre><code>Key                      Value\n---                      -----\nSeal Type                gcpckms\nRecovery Seal Type       n/a\nInitialized              false\nSealed                   true\nTotal Recovery Shares    0\nThreshold                0\nUnseal Progress          0/0\nUnseal Nonce             n/a\nVersion                  1.16.1\nBuild Date               2024-04-03T12:35:53Z\nStorage Type             raft\nHA Enabled               true\ncommand terminated with exit code 2\n</code></pre> <p>Vault doit \u00eatre initialis\u00e9 !</p>"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/kind_vault_auto-unsealed_eso_fluxcd/#initialisation-de-vault","title":"Initialisation de Vault","text":"<p>L'initialisation de Vault passe par une commande \u00e0 passer directement sur les pods (dans notre cas, nous n'en avons qu'un) :</p> codeoutput <pre><code>kubectl -n vault exec -it vault-0 -- vault operator init\n</code></pre> <pre><code>Recovery Key 1: xhiaiaNYaJG6IjCSgvtlDOktdl1D8pEQiuuflLF4TFn6\nRecovery Key 2: i6Z/xCFSOottTsabjYemf182h80c4gz8S8pP0Uv5kmws\nRecovery Key 3: iCYiSqb8MwMIb34GGyy2+pUMfL7774gAXb6BVV24v+EZ\nRecovery Key 4: cMFdU8okh5OZ2VSdhpRk7965EE+hO+N+M9OlHEtZBfdl\nRecovery Key 5: QmMRWjhJrzEJ+Oc0UnWhN9hlJff4seCmBkr7Ne8uP3ay\n\nInitial Root Token: hvs.VPcxxUbQjWt66U3jRzMjfIaI\n\nSuccess! Vault is initialized\n\nRecovery key initialized with 5 key shares and a key threshold of 3. Please\nsecurely distribute the key shares printed above.\n</code></pre> <p>Warning</p> <p>Le 'Root Token' ainsi que les 'Recovery Keys' doivent \u00eatre conserv\u00e9s, et dans un lieu s\u00fbr !</p> <p>V\u00e9rifions que Vault est bien op\u00e9rationnel :</p> codeoutput <pre><code>kubectl -n vault exec -it vault-0 -- vault status\n</code></pre> <pre><code>Key                      Value\n---                      -----\nSeal Type                gcpckms\nRecovery Seal Type       shamir\nInitialized              true\nSealed                   false\nTotal Recovery Shares    5\nThreshold                3\nVersion                  1.16.1\nBuild Date               2024-04-03T12:35:53Z\nStorage Type             raft\nCluster Name             vault-cluster-877de470\nCluster ID               8a3d7616-1771-aa7d-bf00-e587e88f9f4d\nHA Enabled               true\nHA Cluster               https://vault-0.vault-internal:8201\nHA Mode                  active\nActive Since             2024-06-01T16:21:00.144765355Z\nRaft Committed Index     67\nRaft Applied Index       67\n</code></pre> <p>Vault est bien initialis\u00e9. Assurons-nous malgr\u00e9 tout que le pod est d\u00e9sormais bien 'ready' :</p>  codeoutput <pre><code>kubectl -n vault get pod vault-0\n</code></pre> <pre><code>NAME      READY   STATUS    RESTARTS   AGE\nvault-0   1/1     Running   0          25m\n</code></pre> <p>Tout est comme attendu ! </p>"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/kind_vault_auto-unsealed_eso_fluxcd/#test-de-lauto-unseal","title":"Test de l'auto-unseal","text":"<p>Vault est install\u00e9 en 'statefulset', sa configuration est p\u00e9renne, aussi allons-nous le d\u00e9sinstaller et attendre que FluxCD le r\u00e9installe pour nous assurer que Vault sera r\u00e9install\u00e9 dans un \u00e9tat initialis\u00e9 et 'unsealed'.</p> <p>helm -n vault list</p> <p>NAME    NAMESPACE   REVISION    UPDATED                                 STATUS      CHART           APP VERSION vault   vault       1           2024-06-01 16:13:04.681229835 +0000 UTC deployed    vault-0.28.0    1.16.1</p> codeoutput <pre><code>helm -n vault uninstall vault\nkubectl -n vault get all\n</code></pre> <pre><code>No resources found in vault namespace.\n</code></pre> <p>Discord nous pr\u00e9vient que FluxCD a red\u00e9ploy\u00e9 la Helm release :</p> <p></p> <p>Regardons sur le pod nouvellement re-d\u00e9ploy\u00e9 l'\u00e9tat de Vault :</p> codeoutput <pre><code>kubectl -n vault exec -it vault-0 -- vault status\n</code></pre> <pre><code>Key                      Value\n---                      -----\nSeal Type                gcpckms\nRecovery Seal Type       shamir\nInitialized              true\nSealed                   false\nTotal Recovery Shares    5\nThreshold                3\nVersion                  1.16.1\nBuild Date               2024-04-03T12:35:53Z\nStorage Type             raft\nCluster Name             vault-cluster-877de470\nCluster ID               8a3d7616-1771-aa7d-bf00-e587e88f9f4d\nHA Enabled               true\nHA Cluster               https://vault-0.vault-internal:8201\nHA Mode                  active\nActive Since             2024-06-01T16:45:14.602109688Z\nRaft Committed Index     109\nRaft Applied Index       109\n</code></pre> <p>Success</p> <p>Nous venons de valider le bon fonctionnement de l''auto-unsealing' de Vault.</p>"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/kind_vault_auto-unsealed_eso_fluxcd/#external-secrets-operator","title":"External Secrets Operator","text":"<p>Info</p> <p>https://external-secrets.io/latest/introduction/overview/</p>"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/kind_vault_auto-unsealed_eso_fluxcd/#helm-repository","title":"Helm repository","text":"<p>Commen\u00e7ons par d\u00e9finir le Helm repository :</p> code'external-secrets' Helm repository <pre><code>export LOCAL_GITHUB_REPOS=\"${HOME}/code/github\"\n\nflux create source helm external-secrets \\\n  --url=https://charts.external-secrets.io \\\n  --namespace=vault \\\n  --interval=1m \\\n  --export &gt; ${LOCAL_GITHUB_REPOS}/k8s-kind-fluxcd/apps/vault/external-secrets.helm-repository.yaml\n</code></pre> <pre><code>---\napiVersion: source.toolkit.fluxcd.io/v1beta2\nkind: HelmRepository\nmetadata:\n  name: external-secrets\n  namespace: vault\nspec:\n  interval: 1m0s\n  url: https://charts.external-secrets.io\n</code></pre>"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/kind_vault_auto-unsealed_eso_fluxcd/#helm-release_1","title":"Helm release","text":"<p>Nous avions d\u00e9j\u00e0 d\u00e9fini le Helm repository dans la premi\u00e8re partie  de ce howto. </p> <p>Il nous reste \u00e0 d\u00e9finir la Helm release asoci\u00e9e :</p> code'external-secrets' Helm release <pre><code>export LOCAL_GITHUB_REPOS=\"${HOME}/code/github\"\n\n    flux create helmrelease external-secrets \\\n      --source=HelmRepository/external-secrets \\\n      --chart=external-secrets \\\n      --namespace=vault \\\n      --export &gt; ${LOCAL_GITHUB_REPOS}/k8s-kind-fluxcd/apps/vault/external-secrets.helm-release.yaml\n</code></pre> <pre><code>---\napiVersion: helm.toolkit.fluxcd.io/v2beta1\nkind: HelmRelease\nmetadata:\n  name: external-secrets\n  namespace: vault\nspec:\n  chart:\n    spec:\n      chart: external-secrets\n      reconcileStrategy: ChartVersion\n      sourceRef:\n        kind: HelmRepository\n        name: external-secrets\n  interval: 1m0s\n</code></pre>"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/kind_vault_auto-unsealed_eso_fluxcd/#deploiement-sur-le-cluster","title":"D\u00e9ploiement sur le cluster","text":"<pre><code>export LOCAL_GITHUB_REPOS=\"${HOME}/code/github\"\n\ncd ${LOCAL_GITHUB_REPOS}/k8s-kind-fluxcd\n\ngit add .\ngit commit -m \"feat: deploying external-secrets operator on the cluster.\"\ngit push\n\nflux reconcile kustomization flux-system --with-source\n</code></pre> <p>Nous recevons tout de suite des alertes dans notre salon Discord d\u00e9di\u00e9 \u00e0 Vault : </p> <p></p> <p>Regardons quels objets ont \u00e9t\u00e9 d\u00e9ploy\u00e9s sur le cluster :</p> codeoutput <pre><code>kubectl -n vault  get all -l app.kubernetes.io/name=external-secrets\n</code></pre> <pre><code>NAME                                    READY   STATUS    RESTARTS   AGE\npod/external-secrets-7f9f5fd4d6-gfc6h   1/1     Running   0          16m\n\nNAME                               READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/external-secrets   1/1     1            1           16m\n\nNAME                                          DESIRED   CURRENT   READY   AGE\nreplicaset.apps/external-secrets-7f9f5fd4d6   1         1         1       16m\n</code></pre> <p>Faisons une derni\u00e8re v\u00e9rification :</p> codeoutput <pre><code>kubectl -n vault get externalsecret,secretstore\n</code></pre> <pre><code>No resources found in vault namespace.\n</code></pre> <p>M\u00eame si la derni\u00e8re commande ne retourne aucun objet, au moins nous sommes s\u00fbrs que les objets de type 'externalsecret' et 'secretstore' sont bien d\u00e9finis au niveau de notre cluster.</p> <p>Success</p> <p>'External-Secrets Operator (ESO)' est d\u00e9ploy\u00e9 correctement sur notre cluster ! </p>"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/kind_vault_auto-unsealed_eso_fluxcd/#integration-de-vault-et-external-secrets-a-la-helm-release-kube-prometheus-stack","title":"Int\u00e9gration de Vault et External-Secrets \u00e0 la Helm Release 'kube-prometheus-stack'","text":"<p>La stack de monitoring d\u00e9finit un mot de passe par d\u00e9faut pour le compte admin de Grafana. Et c'est moche.</p> <p>Pour corriger cela, nous nous proposons de d\u00e9finir un nouveau mot de passe pour ce compte et de le prot\u00e9ger dans Vault.</p> <p>Nous utiliserons l'op\u00e9rateur External Secrets synchroniser le mot de passe h\u00e9berg\u00e9 dans Vault avec une ConfigMap qui sera utilis\u00e9e par Flux pour d\u00e9finir les 'custom values' de la Helm Release 'kube-prometheus-stack'.</p> <p>Tout un programme. ^^</p>"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/kind_vault_auto-unsealed_eso_fluxcd/#ajout-du-secret-dans-vault","title":"Ajout du 'secret' dans Vault","text":"<p>Connectons-nous au pod 'Vault-0' pour activer le 'secret engine' 'KVv2' et y h\u00e9berger le mot de passe du compte d'administration de Grafana :</p> <pre><code># Acc\u00e8s au pod du micro-service 'vault'\nkubectl -n vault exec -it vault-0 -- sh\n\n# Login sur Vault avec le Root token\nvault login hvs.VPcxxUbQjWt66U3jRzMjfIaI\n\n# Activation du 'secret engine' KVv2\nvault secrets enable -version=2 kv\n\n# Ecriture du secret \nvault kv put -mount kv monitoring/grafana/admin-account login=admin password=my-vaulted-custom-password\n\n\n# V\u00e9rification\nvault kv get -mount=kv monitoring/grafana/admin-account\n\n============== Secret Path ==============\nkv/data/monitoring/grafana/admin-account\n\n======= Metadata =======\nKey                Value\n---                -----\ncreated_time       2024-06-04T14:45:27.639679075Z\ncustom_metadata    &lt;nil&gt;\ndeletion_time      n/a\ndestroyed          false\nversion            2\n\n====== Data ======\nKey         Value\n---         -----\nlogin       admin\npassword    my-vaulted-custom-password\n\n# Deconnexion du pod \nexit\n</code></pre>"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/kind_vault_auto-unsealed_eso_fluxcd/#definition-dune-policy-permettant-dacceder-en-lecture-aux-secrets-dedies-a-grafana","title":"D\u00e9finition d'une 'policy' permettant d'acc\u00e9der en lecture aux secrets d\u00e9di\u00e9s \u00e0 Grafana","text":"<p>Maintenant, \u00e9crivons une 'policy' nous permettant de r\u00e9cup\u00e9rer notre mot de passe :</p> <pre><code># Acc\u00e8s au pod du micro-service 'vault'\nkubectl -n vault exec -it vault-0 -- sh\n\n# Login sur Vault avec le Root token\nvault login hvs.VPcxxUbQjWt66U3jRzMjfIaI\n\n# Definition de la 'policy' donnant acc\u00e8s aux 'secrets' de Grafana en lecture\nvault policy write monitoring-grafana--ro - &lt;&lt; EOF     \npath \"kv/metadata/monitoring/grafana*\" {\n  capabilities = [\"list\",\"read\"]\n}\npath \"kv/data/monitoring/grafana*\" {\n  capabilities = [\"list\",\"read\"]\n}\n\npath \"kv/metadata/monitoring\" {\n  capabilities = [\"list\"]\n}\npath \"kv/data/monitoring\" {\n  capabilities = [\"list\"]\n}\n\npath \"kv/metadata\" {\n  capabilities = [\"list\"]\n}\n\npath \"kv/metadata*\" {\n  capabilities = [\"deny\"]\n}\npath \"kv/data*\" {\n  capabilities = [\"deny\"]\n}\nEOF\n\n# Deconnexion du pod \nexit\n</code></pre>"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/kind_vault_auto-unsealed_eso_fluxcd/#authentification-kubernetes-sur-vault","title":"Authentification Kubernetes sur Vault","text":"<p>L'application Grafana doit pouvoir r\u00e9cup\u00e9rer le mot de passe h\u00e9berg\u00e9 dans Vault. Voici comment nous allons nous y prendre pour arriver \u00e0 nos fins :</p> <ul> <li>nous allons activer sur Vault l'authentification Kubernetes;</li> <li>nous attacherons au service-account avec lequel le pod Grafana sera ex\u00e9cut\u00e9  'ClusterRole' auth-delegator;</li> <li>enfin, il nous restera \u00e0 d\u00e9finit au niveau de Vault un r\u00f4le visant \u00e0 rattacher la policy cr\u00e9\u00e9e pr\u00e9c\u00e9demment \u00e0 notre service-account Kubernetes.</li> </ul> <p>Info</p> <p>https://developer.hashicorp.com/vault/docs/auth/kubernetes#kubernetes-auth-method</p> <p>Use local service account token as the reviewer JWT :</p> <p>When running Vault in a Kubernetes pod the recommended option is to use the pod's local service account token. Vault will periodically re-read the file to support short-lived tokens. To use the local token and CA certificate, omit token_reviewer_jwt and kubernetes_ca_cert when configuring the auth method. Vault will attempt to load them from token and ca.crt respectively inside the default mount folder /var/run/secrets/kubernetes.io/serviceaccount/.</p> <p>Each client of Vault would need the system:auth-delegator ClusterRole</p> <p>Commen\u00e7ons par activer et configurer l'authentification Kubernetes sur Vault :</p> <pre><code># Acc\u00e8s au pod du micro-service 'vault'\nkubectl -n vault exec -it vault-0 -- sh\n\n# Login sur Vault avec le Root token\nvault login hvs.VPcxxUbQjWt66U3jRzMjfIaI\n\n\n# Activation de l'authentification Kubernetes\nvault auth enable kubernetes\nvault auth list\n\n# Configuration de l'authentification Kubernetes \nvault write auth/kubernetes/config kubernetes_host=https://${KUBERNETES_SERVICE_HOST}:${KUBERNETES_SERVICE_PORT}\n\n# Deconnexion du pod \nexit\n</code></pre>"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/kind_vault_auto-unsealed_eso_fluxcd/#etablissement-de-la-relation-entre-le-service-account-kubernetes-et-celui-de-vault","title":"Etablissement de la relation entre le service-account Kubernetes et celui de Vault","text":""},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/kind_vault_auto-unsealed_eso_fluxcd/#service-account-kubernetes","title":"Service-account Kubernetes","text":"<p>Lors du d\u00e9ploiement de la Helm release 'kube-prometheus-stack', Grafana devra \u00eatre en mesure de r\u00e9cup\u00e9rer ses custom values dans un Secret Kubernetes. Ce dernier doit \u00eatre g\u00e9n\u00e9r\u00e9 en amont par l'op\u00e9rateur 'External Secrets' \u00e0 partir d'un template sous forme de ConfigMap indiquant le besoin de r\u00e9cup\u00e9rer le mot de passe du compte d'administration depuis Vault.</p> <p>Le Secret Kubernetes doit donc \u00eatre pr\u00eat avant le d\u00e9ploiement de la Helm release. Or le service-account qui sera utilis\u00e9 par Grafana ne sera cr\u00e9\u00e9 que lors de son d\u00e9ploiement. Il n'est donc pas envisageable de l'utiliser pour s'authentifier \u00e0 Vault et r\u00e9cup\u00e9rer le secret recherch\u00e9.</p> <p>Nous devons donc cr\u00e9er un service-account d\u00e9di\u00e9, que nous nommerons 'eso-grafana'.</p> code'eso-grafana' service-account <pre><code>export LOCAL_GITHUB_REPOS=\"${HOME}/code/github\"\n\nkubectl -n monitoring create serviceaccount eso-grafana --dry-run=client -o yaml | grep -v creationTimestamp &gt; ${LOCAL_GITHUB_REPOS}/k8s-kind-fluxcd/apps/monitoring/eso-grafana.serviceaccount.yaml\n</code></pre> <pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: eso-grafana\n  namespace: monitoring\n</code></pre> <p>Cr\u00e9ons le service-account car nous en aurons besoin pour les tests un peu plus loin :</p> <pre><code>export LOCAL_GITHUB_REPOS=\"${HOME}/code/github\"\n\ncd ${LOCAL_GITHUB_REPOS}/k8s-kind-fluxcd\n\ngit add .\ngit commit -m \"feat: setting up eso-grafana service-account.\"\ngit push\n\nflux reconcile kustomization flux-system --with-source\n</code></pre>"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/kind_vault_auto-unsealed_eso_fluxcd/#clusterrolebinding","title":"ClusterRoleBinding","text":"<p>Nous allons donner \u00e0 notre nouveau service-account Kubernetes le droit de d\u00e9l\u00e9guer son authentification en le rattachant au ClusterRole 'system:auth-delegator'.</p> <p>Info</p> <p>https://kubernetes.io/docs/reference/access-authn-authz/rbac/#other-component-roles</p> <p>\"system:auth-delegator allows delegated authentication and authorization checks. This is commonly used by add-on API servers for unified authentication and authorization.\"</p> codeoutput <pre><code>kubectl create clusterrolebinding eso-grafana-tokenreview-access \\\n  --clusterrole=system:auth-delegator \\\n  --serviceaccount=monitoring:eso-grafana\n</code></pre> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  creationTimestamp: \"2024-07-06T11:06:53Z\"\n  name: eso-grafana-tokenreview-access\n  resourceVersion: \"2860551\"\n  uid: e74382bd-dd09-4b2e-a896-a1d3fd578a24\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: system:auth-delegator\nsubjects:\n- kind: ServiceAccount\n  name: eso-grafana\n  namespace: monitoring\n</code></pre>"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/kind_vault_auto-unsealed_eso_fluxcd/#rattachement-de-la-policy-vault-au-service-account-kubernetes","title":"Rattachement de la policy Vault au service-account Kubernetes","text":"<p>Pour ce faire, nous allons d\u00e9finir un r\u00f4le au niveau de l'authentification Kubernetes de Vault.</p> <pre><code># Acc\u00e8s au pod du micro-service 'vault'\nkubectl -n vault exec -it vault-0 -- sh\n\n# Login sur Vault avec le Root token\nvault login hvs.VPcxxUbQjWt66U3jRzMjfIaI\n\n# Role autorisant le service-account Kubernetes \u00e0 lire les secrets de Grafana\nvault write auth/kubernetes/role/monitoring-grafana--ro \\\n  bound_service_account_names=eso-grafana \\\n  bound_service_account_namespaces=monitoring \\\n  policies=monitoring-grafana--ro \\\n  ttl=1h\n\n# V\u00e9rification\nvault read auth/kubernetes/role/monitoring-grafana--ro\n\n  # Key                                         Value\n  # ---                                         -----\n  # alias_name_source                           serviceaccount_uid\n  # bound_service_account_names                 [eso-grafana]\n  # bound_service_account_namespace_selector    n/a\n  # bound_service_account_namespaces            [monitoring]\n  # policies                                    [monitoring-grafana--ro]\n  # token_bound_cidrs                           []\n  # token_explicit_max_ttl                      0s\n  # token_max_ttl                               0s\n  # token_no_default_policy                     false\n  # token_num_uses                              0\n  # token_period                                0s\n  # token_policies                              [monitoring-grafana--ro]\n  # token_ttl                                   1h\n  # token_type                                  def\n\n# Deconnexion du pod \nexit\n</code></pre>"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/kind_vault_auto-unsealed_eso_fluxcd/#test-de-lacces-du-service-account-kubernetes-au-secret-vault","title":"Test de l'acc\u00e8s du service-account Kubernetes au secret Vault","text":"<p>Pour tester que le service-account Kubernetes 'eso-grafana' du namespace 'monitoring' acc\u00e8de bien au secret de Grafana dans Vault, nous allons d\u00e9ployer un pod temporaire qui s'ex\u00e9cutera avec ce service-account.</p> <p>Voici ce que nous cherchons \u00e0 v\u00e9rifier :</p> <ol> <li>Le pod est ex\u00e9cut\u00e9 avec un service-account Kubernetes auquel est rattach\u00e9 le ClusterRole 'system:auth-delegator';</li> <li>L'application ex\u00e9cut\u00e9e dans le pod s'authentifie \u00e0 Vault (authentification Kubernetes) en utilisant le token de son service-account Kubernetes et rattach\u00e9 le r\u00f4le Vault 'monitoring-grafana--ro' ;</li> <li>Ce r\u00f4le Vault autorise pr\u00e9cis\u00e9ment ce service-account Kubernetes d'utiliser la policy Vault qui donne acc\u00e8s en lecture aux login et mot de passe du compte d'administration de Grafana;</li> <li>Vault valide le token du service-account Kubernetes aupr\u00e8s de Kubernetes et renvoie \u00e0 l'application du pod un token d'authentification \u00e0 Vault, auquel est rattach\u00e9 la policy d'acc\u00e8s aux credentials d'admin de Grafana;</li> <li>L'application peut d\u00e9sormais de loguer \u00e0 Vault avec le token ainsi r\u00e9cup\u00e9r\u00e9 et acc\u00e9der ensuite au compte d'administration de Grafana.</li> </ol>"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/kind_vault_auto-unsealed_eso_fluxcd/#test-en-interrogeant-directement-lapi","title":"Test en interrogeant directement l'API","text":"<pre><code># Lancement d'un pod Alpine avec le service-account 'monitoring:kube-prometheus-stack-grafana'\nkubectl -n monitoring run --tty --stdin test --image=alpine --rm --overrides='{ \"spec\": { \"serviceAccount\": \"eso-grafana\" }  }' -- /bin/sh\n\n# Installation de cURL\napk update &amp;&amp; apk add curl jq\n\n# R\u00e9cup\u00e9ration du service-token JWT\nSA_JWT_TOKEN=$( cat /var/run/secrets/kubernetes.io/serviceaccount/token )\n    # -&gt; Pour regarder son contenu : https://jwt.io/ website.\n\n# Authentification sur Vault et r\u00e9cup\u00e9ration du token de session\nCLIENT_TOKEN=$( curl --silent --request POST --data '{\"jwt\": \"'\"${SA_JWT_TOKEN}\"'\", \"role\": \"monitoring-grafana--ro\"}' http://vault.vault:8200/v1/auth/kubernetes/login | jq -r .auth.client_token )\n\n# R\u00e9cup\u00e9ration du mot de passe du compte admin de Grafana\ncurl --silent --header \"X-Vault-Token:${CLIENT_TOKEN}\"  http://vault.vault:8200/v1/kv/data/monitoring/grafana/admin-account | jq .data.data\n\n# {\n#   \"login\": \"admin\",\n#   \"password\": \"my-vaulted-custom-password\"\n# }\n</code></pre>"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/kind_vault_auto-unsealed_eso_fluxcd/#test-avec-la-cli-vault","title":"Test avec la CLI 'vault'","text":"<pre><code># Lancement d'un pod Alpine avec le service-account 'monitoring:kube-prometheus-stack-grafana'\nkubectl -n monitoring run --tty --stdin fedora --image=fedora --rm --overrides='{ \"spec\": { \"serviceAccount\": \"eso-grafana\" }  }' -- /bin/bash\n\n# Installation de Vault : \ndnf install -y dnf-plugins-core\ndnf config-manager --add-repo https://rpm.releases.hashicorp.com/fedora/hashicorp.repo\ndnf -y install vault jq\n\n# Pour une raison que j'ignore, la CLI 'vault' ne fonctionne pas apr\u00e8s installation, mais une r\u00e9installation semble r\u00e9gler le probl\u00e8me :\nrpm -e vault &amp;&amp; dnf -y install vault\n\n# Test d'acc\u00e8s aux secrets de Grafana\nexport VAULT_ADDR=\"http://vault.vault:8200\"\nSA_TOKEN=$( cat /var/run/secrets/kubernetes.io/serviceaccount/token )\nVAULT_TOKEN=$( vault write auth/kubernetes/login role=monitoring-grafana--ro jwt=${SA_TOKEN} | grep -w ^token | awk '{print $2}' )\n\n\nvault login ${VAULT_TOKEN}\n\n  # Success! You are now authenticated. The token information displayed below\n  # is already stored in the token helper. You do NOT need to run \"vault login\"\n  # again. Future Vault requests will automatically use this token.\n  # \n  # Key                                       Value\n  # ---                                       -----\n  # token                                     hvs.CAESIKPEIR-x0Sx8oK2Yc5wICr13blMQSHWmS6SdTCt5jCExGh4KHGh2cy5zWHByV2ZkY2hHb2Q2VjY2YzBBcVk3QWE\n  # token_accessor                            sTrkp8An57VCbuBWy1fDaHnd\n  # token_duration                            59m49s\n  # token_renewable                           true\n  # token_policies                            [\"default\" \"monitoring-grafana--ro\"]\n  # identity_policies                         []\n  # policies                                  [\"default\" \"monitoring-grafana--ro\"]\n  # token_meta_service_account_namespace      monitoring\n  # token_meta_service_account_secret_name    n/a\n  # token_meta_service_account_uid            665cb92c-90ba-4ad8-9313-9cae30e72203\n  # token_meta_role                           monitoring-grafana--ro\n  # token_meta_service_account_name           eso-grafana\n\n\nvault kv list -mount=kv monitoring/grafana\n\n# Keys\n# ----\n# admin-account\n\n\nvault kv get -mount=kv monitoring/grafana/admin-account\n\n  # ============== Secret Path ==============\n  # kv/data/monitoring/grafana/admin-account\n  # \n  # ======= Metadata =======\n  # Key                Value\n  # ---                -----\n  # created_time       2024-06-08T15:52:46.958211047Z\n  # custom_metadata    &lt;nil&gt;\n  # deletion_time      n/a\n  # destroyed          false\n  # version            1\n  # \n  # ====== Data ======\n  # Key         Value\n  # ---         -----\n  # login       admin\n  # password    my-vaulted-custom-password\n\n\nvault kv get -mount=kv -field=password monitoring/grafana/admin-account\n\n  # my-vaulted-custom-password\n</code></pre> <p>Success</p> <p>Notre pod, par le biais du service-account avec lequel il est ex\u00e9cut\u00e9, r\u00e9cup\u00e8re comme attendu le secret dans Vault!  </p>"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/kind_vault_auto-unsealed_eso_fluxcd/#configuration-dexternal-secrets-operator-eso","title":"Configuration d'External Secrets Operator (ESO)","text":"<p>Cet op\u00e9rateur a pour r\u00f4le de synchroniser des objets Kubernetes de type Secret ou ConfigMap avec des 'secrets' stock\u00e9s dans un Secrets Manager (dans notre cas, HashiCorp Vault).</p>"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/kind_vault_auto-unsealed_eso_fluxcd/#definition-du-secret-store","title":"Definition du Secret Store","text":"<p>Le 'SecretStore' est un objet qui d\u00e9finit dans notre cas de figure l'adresse de Vault, le 'secret engine' \u00e0 utiliser (en renseignant son 'path' et dans le cas de KV, la version du moteur), la mani\u00e8re de s'y authentifier (ici, on choisit l'authentification Kubernetes), avec quel service-account Kubernetes et quel r\u00f4le demander.</p> <pre><code>export LOCAL_GITHUB_REPOS=\"${HOME}/code/github\"\n\n# D\u00e9finition du SecretStore 'grafana' :\ncat &lt;&lt; EOF &gt; ${LOCAL_GITHUB_REPOS}/k8s-kind-fluxcd/apps/monitoring/grafana.secretstore.yaml\napiVersion: external-secrets.io/v1beta1\nkind: SecretStore\nmetadata:\n  name: grafana\n  namespace: monitoring\nspec:\n  provider:\n    vault:\n      server: \"http://vault.vault:8200\"\n      path: \"kv\"\n      version: \"v2\"\n      auth:\n        kubernetes:\n          mountPath: \"kubernetes\"\n          role: \"monitoring-grafana--ro\"\n          serviceAccountRef:\n            name: \"eso-grafana\"\nEOF\n</code></pre>"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/kind_vault_auto-unsealed_eso_fluxcd/#definition-de-lexternal-secret","title":"D\u00e9finition de l'External Secret","text":"<p>Une fois le 'SecretStore' d\u00e9fini, nous pouvons nous int\u00e9resser aux 'External Secrets' : il s'agit cette fois de pr\u00e9ciser quel(s) secret(s) nous souhaitons r\u00e9cup\u00e9rer depuis SecretStore donn\u00e9 :</p> <pre><code>export LOCAL_GITHUB_REPOS=\"${HOME}/code/github\"\n\ncat &lt;&lt; EOF &gt; ${LOCAL_GITHUB_REPOS}/k8s-kind-fluxcd/apps/monitoring/grafana.externalsecret.yaml\napiVersion: external-secrets.io/v1beta1\nkind: ExternalSecret\nmetadata:\n  name: grafana-secrets\n  namespace: monitoring\nspec:\n  refreshInterval: \"15s\"\n  secretStoreRef:\n    name: grafana\n    kind: SecretStore\n  target:\n    name: admin-password\n  data:\n  - secretKey: admin_password\n    remoteRef:\n      key: kv/monitoring/grafana/admin-account\n      property: password\nEOF\n</code></pre>"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/kind_vault_auto-unsealed_eso_fluxcd/#autorisation-dacces-dans-vault-a-ce-service-account-aux-secrets-de-grafana","title":"Autorisation d'acc\u00e8s dans Vault \u00e0 ce service-account aux secrets de Grafana","text":"<p>Bien \u00e9videmment, notre nouveau service-account doit pouvoir acc\u00e9der aux secrets de Grafana contenus dans Vault. Nous devons adapter notre r\u00f4le en cons\u00e9quence :</p> <pre><code># Login sur le pod Vault :\nkubectl -n vault exec -it vault-0  -- sh\n\n# Ouverture d'une session Vault avec le *root token*\nvault login hvs.VPcxxUbQjWt66U3jRzMjfIaI\n\n# Autorisation de lecture des secrets Grafana aux service-accounts 'kube-prometheus-grafana' et 'init-grafana' :\nvault write auth/kubernetes/role/monitoring-grafana--ro \\\n  bound_service_account_names=eso-grafana \\\n  bound_service_account_namespaces=monitoring \\\n  policies=monitoring-grafana--ro \\\n  ttl=1h\n\n# Fin de session sur le pod :\nexit\n</code></pre>"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/kind_vault_auto-unsealed_eso_fluxcd/#prise-en-compte-des-modifications","title":"Prise en compte des modifications","text":"<pre><code>export LOCAL_GITHUB_REPOS=\"${HOME}/code/github\"\n\ncd ${LOCAL_GITHUB_REPOS}/k8s-kind-fluxcd\n\ngit add .\ngit commit -m \"feat: setting up grafana secretstore and external-secret.\"\ngit push\n\nflux reconcile kustomization flux-system --with-source\n</code></pre> <p>V\u00e9rifions la bonne cr\u00e9ation des nouveaux objets ESO :</p> codeoutput <pre><code>kubectl -n monitoring get secretstore,externalsecret\n</code></pre> <pre><code>NAME                                      AGE     STATUS   CAPABILITIES   READY\nsecretstore.external-secrets.io/grafana   3d15h   Valid    ReadWrite      True\n\nNAME                                                                                    STORE     REFRESH INTERVAL   STATUS         READY\nexternalsecret.external-secrets.io/grafana-secrets                                      grafana   15s                SecretSynced   True\nexternalsecret.external-secrets.io/kube-prometheus-stack-custom-values-externalsecret   grafana   1h                 SecretSynced   True\n</code></pre>"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/kind_vault_auto-unsealed_eso_fluxcd/#recuperation-de-lexternal-secret-depuis-un-pod-de-test","title":"R\u00e9cup\u00e9ration de l'External Secret depuis un pod de test","text":"<pre><code># Cr\u00e9ation d'un pod Alpine excut\u00e9 avec le service-account d\u00e9di\u00e9 \u00e0 l'application Grafana\n# et affichant le mot de passe du compte d'administration :\ncat &lt;&lt; EOF | kubectl apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  labels:\n    run: test\n  name: test\n  namespace: monitoring\nspec:\n  containers:\n  - name: test\n    image: alpine\n    command: [\"printenv\"]\n    args: [\"ADMIN_PASSWORD\"]\n    env:\n    - name: ADMIN_PASSWORD\n      valueFrom:\n        secretKeyRef:\n          name: admin-password\n          key: admin_password\n  restartPolicy: Never\n  serviceAccount: eso-grafana\nEOF\n</code></pre> <p>Le pod pase \u00e0 l'\u00e9tat 'Completed'. Consultons ses logs :</p> codeoutput <pre><code>kubectl -n monitoring logs test\n</code></pre> <pre><code>my-vaulted-custom-password\n</code></pre> <p>Success</p> <p>Nous r\u00e9cup\u00e9rons comme attendu le mot de passe du compte d'administration de Grafana pr\u00e9sent dans Vault.  </p> <p>Supprimons le pod :</p> <pre><code>    kubectl -n monioring delete pod test\n</code></pre>"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/kind_vault_auto-unsealed_eso_fluxcd/#integration-de-lexternal-secret-de-la-helm-release-kube-prometheus-stack","title":"Int\u00e9gration de l'external secret de la Helm Release 'kube-prometheus-stack'","text":"<p>Nous avan\u00e7ons \u00e0 petits pas, mais nous avan\u00e7ons!</p>"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/kind_vault_auto-unsealed_eso_fluxcd/#deploiement-de-la-helm-release-kube-prometheus-stack","title":"D\u00e9ploiement de la Helm release 'kube-prometheus-stack'","text":"<p>Nous allons maintenant d\u00e9ployer la Helm Release 'kube-prometheus-stack' avec les valeurs par d\u00e9faut (pr\u00e9sentes dans le fichier 'values.yaml').</p> <p>Nous irons tr\u00e8s vite sur l'installation car nous l'avons d\u00e9j\u00e0 couverte dans le howto 'kube-prometheus-stack' managed with FluxCD.</p> <pre><code>export LOCAL_GITHUB_REPOS=\"${HOME}/code/github\"\n\n\n# R\u00e9pertoire qui contiendra tous les objets Kubernetes :\nmkdir -p ${LOCAL_GITHUB_REPOS}/k8s-kind-fluxcd/apps/monitoring\n\n\n# Namespace 'monitoring' :\nkubectl create namespace monitoring --dry-run=client -o yaml | grep -vE \"creationTimestamp|spec|status\" &gt; ${LOCAL_GITHUB_REPOS}/k8s-kind-fluxcd/apps/monitoring/namespace.yaml\n\n\n# Secret contenant le webhook du salon Discord :\nexport WEBHOOK_FOO=\"https://discord.com/api/webhooks/1242845059800633425/zyTYEpNZGf6vpd6C1sRLqeW_TGyFEMP2EM8BXAzockt20eeennkSHDKoO2-UxEG0K4ah\"\nkubectl -n monitoring create secret generic discord-webhook --from-literal=address=${WEBHOOK_FOO} --dry-run=client -o yaml &gt; ${LOCAL_GITHUB_REPOS}/k8s-kind-fluxcd/apps/monitoring/discord-webhook.secret.yaml\n\n\n# Notification Discord : d\u00e9finition de l'alert-provider :\nflux create alert-provider discord \\\n  --type=discord \\\n  --secret-ref=discord-webhook \\\n  --channel=monitoring \\\n  --username=FluxCD \\\n  --namespace=monitoring \\\n  --export &gt; ${LOCAL_GITHUB_REPOS}/k8s-kind-fluxcd/apps/monitoring/notification-provider.yaml\n\n\n# Notification Discord : d\u00e9finition des alertes :\nflux create alert discord \\\n  --event-severity=info \\\n  --event-source='GitRepository/*,Kustomization/*,ImageRepository/*,ImagePolicy/*,HelmRepository/*,HelmRelease/*' \\\n  --provider-ref=discord \\\n  --namespace=monitoring \\\n  --export &gt;  ${LOCAL_GITHUB_REPOS}/k8s-kind-fluxcd/apps/monitoring/notification-alert.yaml\n\n\n# Helm repository :\nflux create source helm prometheus-community \\\n  --url=https://prometheus-community.github.io/helm-charts \\\n  --namespace=monitoring \\\n  --interval=1m \\\n  --export &gt; ${LOCAL_GITHUB_REPOS}/k8s-kind-fluxcd/apps/monitoring/helm-repository.yaml\n\n\n# Helm Release :\nflux create helmrelease kube-prometheus-stack \\\n  --source=HelmRepository/prometheus-community \\\n  --chart=kube-prometheus-stack \\\n  --namespace=monitoring \\\n  --export &gt; ${LOCAL_GITHUB_REPOS}/k8s-kind-fluxcd/apps/monitoring/helm-release.yaml\n\n\n# Prise en compte des modifications :\ncd ${LOCAL_GITHUB_REPOS}/k8s-kind-fluxcd\ngit add apps/monitoring\ngit commit -m \"feat: init monitoring (namespace, alerting Discord, helm repo and release).\"\ngit push\nflux reconcile kustomization flux-system --with-source\n</code></pre> <p>Tout de suite Discord nous informe du bon d\u00e9ploiement de la Helm release : </p> <p></p> <p>V\u00e9rifions malgr\u00e9 tout notre installation :</p> <pre><code>kubectl get ns monitoring\n\n  # NAME         STATUS   AGE\n  # monitoring   Active   49s\n\n\nkubectl -n monitoring get helmrepositories,helmreleases\n\n  # NAME                                                           URL                                                  AGE   READY   STATUS\n  # helmrepository.source.toolkit.fluxcd.io/prometheus-community   https://prometheus-community.github.io/helm-charts   80s   True    stored artifact: revision 'sha256:10ee9c60cbd4bf6ec4d73e99b80c5c54ca1600edfaea593f0f65f2a92ba1b35d'\n  # \n  # NAME                                                       AGE   READY   STATUS\n  # helmrelease.helm.toolkit.fluxcd.io/kube-prometheus-stack   80s   True    Release reconciliation succeeded\n\n\nkubectl -n monitoring get all\n\n  # NAME                                                            READY   STATUS    RESTARTS   AGE\n  # pod/alertmanager-kube-prometheus-stack-alertmanager-0           2/2     Running   0          97s\n  # pod/kube-prometheus-stack-grafana-86844f6b47-s7wph              3/3     Running   0          98s\n  # pod/kube-prometheus-stack-kube-state-metrics-7c8d64d446-d4fgk   1/1     Running   0          98s\n  # pod/kube-prometheus-stack-operator-75fc8896c7-4stcp             1/1     Running   0          98s\n  # pod/kube-prometheus-stack-prometheus-node-exporter-xmb9j        1/1     Running   0          98s\n  # pod/prometheus-kube-prometheus-stack-prometheus-0               2/2     Running   0          97s\n  # \n  # NAME                                                     TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                      AGE\n  # service/alertmanager-operated                            ClusterIP   None            &lt;none&gt;        9093/TCP,9094/TCP,9094/UDP   97s\n  # service/kube-prometheus-stack-alertmanager               ClusterIP   10.96.222.244   &lt;none&gt;        9093/TCP,8080/TCP            98s\n  # service/kube-prometheus-stack-grafana                    ClusterIP   10.96.24.39     &lt;none&gt;        80/TCP                       98s\n  # service/kube-prometheus-stack-kube-state-metrics         ClusterIP   10.96.99.205    &lt;none&gt;        8080/TCP                     98s\n  # service/kube-prometheus-stack-operator                   ClusterIP   10.96.119.105   &lt;none&gt;        443/TCP                      98s\n  # service/kube-prometheus-stack-prometheus                 ClusterIP   10.96.52.206    &lt;none&gt;        9090/TCP,8080/TCP            98s\n  # service/kube-prometheus-stack-prometheus-node-exporter   ClusterIP   10.96.28.184    &lt;none&gt;        9100/TCP                     98s\n  # service/prometheus-operated                              ClusterIP   None            &lt;none&gt;        9090/TCP                     97s\n  # \n  # NAME                                                            DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE\n  # daemonset.apps/kube-prometheus-stack-prometheus-node-exporter   1         1         1       1            1           kubernetes.io/os=linux   98s\n  # \n  # NAME                                                       READY   UP-TO-DATE   AVAILABLE   AGE\n  # deployment.apps/kube-prometheus-stack-grafana              1/1     1            1           98s\n  # deployment.apps/kube-prometheus-stack-kube-state-metrics   1/1     1            1           98s\n  # deployment.apps/kube-prometheus-stack-operator             1/1     1            1           98s\n  # \n  # NAME                                                                  DESIRED   CURRENT   READY   AGE\n  # replicaset.apps/kube-prometheus-stack-grafana-86844f6b47              1         1         1       98s\n  # replicaset.apps/kube-prometheus-stack-kube-state-metrics-7c8d64d446   1         1         1       98s\n  # replicaset.apps/kube-prometheus-stack-operator-75fc8896c7             1         1         1       98s\n  # \n  # NAME                                                               READY   AGE\n  # statefulset.apps/alertmanager-kube-prometheus-stack-alertmanager   1/1     97s\n  # statefulset.apps/prometheus-kube-prometheus-stack-prometheus       1/1     97s\n</code></pre> <p>Nous avons d\u00e9ploy\u00e9 la stack avec ses valeurs par d\u00e9faut. Regardons tout de suite si nous pouvons effectivement nous connecter \u00e0 Grafana avec le login et le mode de passe par d\u00e9faut du compte d'administration.</p> <p>Pour identifier le mot de passe, nous devons r\u00e9cup\u00e9rer le fichier 'values.yaml' du Helm Chart utilis\u00e9 :</p> <pre><code>export LOCAL_GITHUB_REPOS=\"${HOME}/code/github\"\nhelm show values prometheus-community/kube-prometheus-stack &gt; ${LOCAL_GITHUB_REPOS}/k8s-kind-fluxcd/apps/monitoring/kube-prometheus-stack.default.values.txt\ncat ${LOCAL_GITHUB_REPOS}/k8s-kind-fluxcd/apps/monitoring/kube-prometheus-stack.default.values.txt | yq .grafana.adminPassword\n\n  # prom-operator\n</code></pre> <p>Le mot de passe propos\u00e9 par d\u00e9faut pour le compte 'admin' de Grafana est donc : 'prom-operator'.</p> <p>Note</p> <p>Le fichier 'values.yaml' r\u00e9cup\u00e9r\u00e9 est enregistr\u00e9 en '.txt' et non en '.yaml' pour qu'il ne soit pas interpr\u00e9t\u00e9 plus tard par FluxCD.</p> <p>Tentons une connexion \u00e0 Grafana avec le compte d'administration :</p> <pre><code># Identification du service Grafana (nom et port TCP) :\nkubectl -n monitoring get services | grep -i grafana\n\n  # kube-prometheus-stack-grafana                    ClusterIP   10.96.24.39     &lt;none&gt;        80/TCP                       25m\n\n# Port-forwarding\nkubectl -n monitoring port-forward service/kube-prometheus-stack-grafana 8080:80\n</code></pre> <p>Ouvrons enfin un navigateur \u00e0 l'URL suivante :  <code>http://localhost:8080</code></p> <p></p> <p>Success</p> <p>nous acc\u00e9dons \u00e0 Grafana avec le compte 'admin' et le mot de passe par d\u00e9faut 'prom-operator' </p>"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/kind_vault_auto-unsealed_eso_fluxcd/#modification-du-mot-de-passe-du-compte-dadministration-de-grafana","title":"Modification du mot de passe du compte d'administration de Grafana","text":"<p>Nous allons produire un fichier contenant le seul param\u00e8tre que nous souhaitons surcharger aux valeurs par d\u00e9faut du Helm chart :</p> <pre><code>export LOCAL_GITHUB_REPOS=\"${HOME}/code/github\"\ncat &lt;&lt; EOF &gt;&gt; ${LOCAL_GITHUB_REPOS}/k8s-kind-fluxcd/apps/monitoring/kube-prometheus-stack.custom.values.txt\ngrafana:\n  adminPassword: my-cleartext-custom-password\nEOF\n</code></pre> <p>Appliquons ce nouveau mot de passe \u00e0 notre Helm Release d\u00e9j\u00e0 d\u00e9ploy\u00e9e :</p> codehelm release avant modificationhelm release apr\u00e8s modification <pre><code>export LOCAL_GITHUB_REPOS=\"${HOME}/code/github\"\n\nflux create helmrelease kube-prometheus-stack \\\n  --source=HelmRepository/prometheus-community \\\n  --chart=kube-prometheus-stack \\\n  --namespace=monitoring \\\n  --values=${LOCAL_GITHUB_REPOS}/k8s-kind-fluxcd/apps/monitoring/kube-prometheus-stack.custom.values.txt \\\n  --export &gt; ${LOCAL_GITHUB_REPOS}/k8s-kind-fluxcd/apps/monitoring/helm-release.yaml\n</code></pre> <pre><code>---\napiVersion: helm.toolkit.fluxcd.io/v2beta1\nkind: HelmRelease\nmetadata:\n  name: kube-prometheus-stack\n  namespace: monitoring\nspec:\n  chart:\n    spec:\n      chart: kube-prometheus-stack\n      reconcileStrategy: ChartVersion\n      sourceRef:\n        kind: HelmRepository\n        name: prometheus-community\n  interval: 1m0s\n</code></pre> <pre><code>---\napiVersion: helm.toolkit.fluxcd.io/v2beta1\nkind: HelmRelease\nmetadata:\n  name: kube-prometheus-stack\n  namespace: monitoring\nspec:\n  chart:\n    spec:\n      chart: kube-prometheus-stack\n      reconcileStrategy: ChartVersion\n      sourceRef:\n        kind: HelmRepository\n        name: prometheus-community\n  interval: 1m0s\n  values:\n    grafana:\n      adminPassword: my-cleartext-custom-password\n</code></pre> <p>Appliquons les changements :</p> <pre><code>export LOCAL_GITHUB_REPOS=\"${HOME}/code/github\"\n\ncd ${LOCAL_GITHUB_REPOS}/k8s-kind-fluxcd\n\ngit add .\ngit commit -m \"feat: manually changed the password for grafana's admin account.\"\ngit push\n\nflux reconcile kustomization flux-system --with-source\n</code></pre> <p>Nous observons que les pods Grafana redescendent :</p> <pre><code>kubectl -n monitoring get po -w\n\n  # NAME                                                        READY   STATUS    RESTARTS   AGE\n  # alertmanager-kube-prometheus-stack-alertmanager-0           2/2     Running   0          37m\n  # kube-prometheus-stack-grafana-86844f6b47-s7wph              3/3     Running   0          37m\n  # kube-prometheus-stack-kube-state-metrics-7c8d64d446-d4fgk   1/1     Running   0          37m\n  # kube-prometheus-stack-operator-75fc8896c7-4stcp             1/1     Running   0          37m\n  # kube-prometheus-stack-prometheus-node-exporter-xmb9j        1/1     Running   0          37m\n  # prometheus-kube-prometheus-stack-prometheus-0               2/2     Running   0          37m\n  # kube-prometheus-stack-admission-create-784kh                0/1     Pending   0          0s\n  # kube-prometheus-stack-admission-create-784kh                0/1     Pending   0          1s\n  # kube-prometheus-stack-admission-create-784kh                0/1     ContainerCreating   0          2s\n  # kube-prometheus-stack-admission-create-784kh                1/1     Running             0          18s\n  # kube-prometheus-stack-admission-create-784kh                0/1     Completed           0          24s\n  # kube-prometheus-stack-admission-create-784kh                0/1     Completed           0          30s\n  # kube-prometheus-stack-admission-create-784kh                0/1     Completed           0          30s\n  # kube-prometheus-stack-admission-create-784kh                0/1     Completed           0          32s\n  # kube-prometheus-stack-admission-create-784kh                0/1     Terminating         0          33s\n  # kube-prometheus-stack-admission-create-784kh                0/1     Terminating         0          33s\n  # kube-prometheus-stack-grafana-57595d7d49-wrt4s              0/3     Pending             0          0s\n  # kube-prometheus-stack-grafana-57595d7d49-wrt4s              0/3     Pending             0          0s\n  # kube-prometheus-stack-grafana-57595d7d49-wrt4s              0/3     ContainerCreating   0          1s\n  # kube-prometheus-stack-grafana-57595d7d49-wrt4s              2/3     Running             0          30s\n  # kube-prometheus-stack-grafana-57595d7d49-wrt4s              2/3     Running             1 (10s ago)   3m13s\n  # kube-prometheus-stack-grafana-57595d7d49-wrt4s              3/3     Running             1 (30s ago)   3m33s\n  # kube-prometheus-stack-grafana-86844f6b47-s7wph              3/3     Terminating         0             46m\n  # kube-prometheus-stack-grafana-86844f6b47-s7wph              0/3     Terminating         0             46m\n  # kube-prometheus-stack-grafana-86844f6b47-s7wph              0/3     Terminating         0             46m\n  # kube-prometheus-stack-grafana-86844f6b47-s7wph              0/3     Terminating         0             46m\n  # kube-prometheus-stack-grafana-86844f6b47-s7wph              0/3     Terminating         0             46m\n  # kube-prometheus-stack-admission-patch-grgxs                 0/1     Pending             0             0s\n  # kube-prometheus-stack-admission-patch-grgxs                 0/1     Pending             0             0s\n  # kube-prometheus-stack-admission-patch-grgxs                 0/1     ContainerCreating   0             0s\n  # kube-prometheus-stack-admission-patch-grgxs                 1/1     Running             0             5s\n  # kube-prometheus-stack-admission-patch-grgxs                 0/1     Completed           0             6s\n  # kube-prometheus-stack-admission-patch-grgxs                 0/1     Completed           0             7s\n  # kube-prometheus-stack-admission-patch-grgxs                 0/1     Completed           0             8s\n  # kube-prometheus-stack-admission-patch-grgxs                 0/1     Completed           0             9s\n  # kube-prometheus-stack-admission-patch-grgxs                 0/1     Terminating         0             9s\n  # kube-prometheus-stack-admission-patch-grgxs                 0/1     Terminating         0             9s\n</code></pre> <p>Discord nous informe \u00e9galement de la bonne mise \u00e0 jour de la Helm release :</p> <p></p> <p>V\u00e9rifions la bonne prise en compte de notre mot de passe 'custom' :</p> <pre><code>kubectl -n monitoring port-forward service/kube-prometheus-stack-grafana 8080:80\n</code></pre> <p>Ouvrons un navigateur sur l'adresse de port-forwarding http://localhost:8080 :</p> <p></p> <p>Success</p> <p>nous acc\u00e9dons \u00e0 Grafana avec le compte 'admin' et le mot de passe 'my-cleartext-custom-password' </p>"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/kind_vault_auto-unsealed_eso_fluxcd/#protection-du-mot-de-passe-avec-vault-et-external-secrets-operator-eso","title":"Protection du mot de passe avec Vault et External Secrets Operator (ESO)","text":"<p>Jusque-l\u00e0, rien de bien sorcier : nous avons simplement demand\u00e9 \u00e0 FluxCD de d\u00e9ployer une Helm Release avec des custom values.</p> <p>Mais nous n'avons pas r\u00e9gl\u00e9 notre probl\u00e8me : si le mot de passe n'est plus celui par-d\u00e9faut, il reste en clair dans un fichier au milieu de notre d\u00e9p\u00f4t de code. Et c'est plut\u00f4t moche.</p> <p>Pour le r\u00e9soudre, nous allons faire usage de l'op\u00e9rateur 'External Secrets'.</p> <p>Info</p> <p>https://blog.gitguardian.com/how-to-handle-secrets-in-helm/#external-secrets-operator</p> <p>\"ESO r\u00e9cup\u00e8re automatiquement les 'secrets managers' via des API externes et les injecte dans Kubernetes Secrets.</p> <p>Contrairement \u00e0 helm-secrets qui fait r\u00e9f\u00e9rence \u00e0 des secrets stock\u00e9s dans des 'Cloud secrets managers' dans le fichier 'values', ESO ne n\u00e9cessite pas d'inclure secrets.yaml dans les 'Helm templates'. Il utilise une autre ressource personnalis\u00e9e 'ExternalSecret', qui contient la r\u00e9f\u00e9rence aux gestionnaires de secrets dans le Cloud.\"</p> <p>Info</p> <p>https://external-secrets.io/latest/guides/templating/#templatefrom</p> <p>https://fluxcd.io/flux/cmd/flux_create_helmrelease/#options</p> <p>Il est possible de d\u00e9finir une 'Helm Release' avec la CLI 'flux' en surchargeant les 'default values' \u00e0 partir d'un objet Kubernetes de type 'Secret' ou 'ConfigMap'.</p> <p>Nous allons (re)d\u00e9finir notre 'Helm Release' 'kube-prometheus-stack' en lui indiquant de r\u00e9cup\u00e9rer ses 'custom values' depuis un 'Secret Kubernetes'. </p> <p>Ce 'Secret' (ie. le fichier YAML qui surchargera les valeurs par d\u00e9faut du Helm Chart) aura pr\u00e9alablement \u00e9t\u00e9 forg\u00e9 par l''External Secrets Operator' en r\u00e9cup\u00e9rant le mot de passe du compte d'administration de Grafana depuis Vault et en l'appliquant \u00e0 un template stock\u00e9 dans un objet ConfigMap. Voyons \u00e7a de plus pr\u00e8s...</p> codekube-prometheus-stack.custom.values.ESO.txt <pre><code>export LOCAL_GITHUB_REPOS=\"${HOME}/code/github\"\n\n# D\u00e9finissons le template \u00e0 partir duquel le fichier de *custom values* de la *Helm release* sera g\u00e9n\u00e9r\u00e9 :\ncat &lt;&lt; EOF &gt; ${LOCAL_GITHUB_REPOS}/k8s-kind-fluxcd/apps/monitoring/kube-prometheus-stack.custom.values.ESO.txt\ngrafana:\n  adminPassword: {{ .grafanaadminpassword }}\nEOF\n\n# Encodons ce fichier en base 64 :\ncat ${LOCAL_GITHUB_REPOS}/k8s-kind-fluxcd/apps/monitoring/kube-prometheus-stack.custom.values.ESO.txt | base64\n\n  # Z3JhZmFuYToKICBhZG1pblBhc3N3b3JkOiB7eyAuZ3JhZmFuYWFkbWlucGFzc3dvcmQgfX0K\n</code></pre> <pre><code>grafana:\n  adminPassword: {{ .grafanaadminpassword }}\n</code></pre> <p>D\u00e9finissons les objets n\u00e9cessaires \u00e0 la cr\u00e9ation du Secret Kubernetes contenant le fichier 'values.yaml' qui viendra surcharger les valeurs par \u00e9faut de notre *Helm release :</p> <p>Warning</p> <p>Il est important que la ConfigMap cr\u00e9\u00e9e ait comme cl\u00e9 'values.yaml', car c'est le fichier attendu par la Helm Release pour surcharger ses valeurs par d\u00e9faut !</p> <p>2 objets sont n\u00e9cessaires :</p> <ul> <li>la ConfigMap qui contiendra le template de fichier YAML \u00e0 produire ;</li> <li>l'ExternalSecret permettant de g\u00e9n\u00e9rer le Secret Kubernetes avec le template et le mot de passe stock\u00e9 dans Vault.</li> </ul> codekube-prometheus-stack.custom.values.yaml <pre><code>export LOCAL_GITHUB_REPOS=\"${HOME}/code/github\"\n\ncat &lt;&lt; EOF &gt; ${LOCAL_GITHUB_REPOS}/k8s-kind-fluxcd/apps/monitoring/kube-prometheus-stack.custom.values.yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: kube-prometheus-stack-custom-values-configmap\n  namespace: monitoring\ndata:\n  values.yaml: |\n    grafana:\n      adminPassword: \"{{ .grafanaadminpassword }}\"\n\n---\napiVersion: external-secrets.io/v1beta1\nkind: ExternalSecret\nmetadata:\n  name: kube-prometheus-stack-custom-values-externalsecret\n  namespace: monitoring\nspec:\n  secretStoreRef:\n    kind: SecretStore\n    name: grafana\n  target:\n    name: kube-prometheus-stack-custom-values\n    template:\n      engineVersion: v2\n      templateFrom:\n      - configMap:\n          name: kube-prometheus-stack-custom-values-configmap\n          items:\n          - key: values.yaml\n            templateAs: Values\n  data:\n  - secretKey: grafanaadminpassword\n    remoteRef:\n      key: kv/monitoring/grafana/admin-account\n      property: password\nEOF\n</code></pre> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: kube-prometheus-stack-custom-values-configmap\n  namespace: monitoring\ndata:\n  values.yaml: |\n    grafana:\n      adminPassword: \"{{ .grafanaadminpassword }}\"\n\n---\napiVersion: external-secrets.io/v1beta1\nkind: ExternalSecret\nmetadata:\n  name: kube-prometheus-stack-custom-values-externalsecret\n  namespace: monitoring\nspec:\n  secretStoreRef:\n    kind: SecretStore\n    name: grafana\n  target:\n    name: kube-prometheus-stack-custom-values\n    template:\n      engineVersion: v2\n      templateFrom:\n      - configMap:\n          name: kube-prometheus-stack-custom-values-configmap\n          items:\n          - key: values.yaml\n            templateAs: Values\n  data:\n  - secretKey: grafanaadminpassword\n    remoteRef:\n      key: kv/monitoring/grafana/admin-account\n      property: password\n</code></pre> <p>Il ne nous reste plus qu'\u00e0 d\u00e9finir notre 'HelmRelease' en lui indiquant qu'il doit r\u00e9cup\u00e9rer ses 'custom values' (values.yaml) depuis un objet Kubernetes de type 'ConfigMap' que nous venons de d\u00e9finir plus haut :</p> codeversion pr\u00e9c\u00e9dentenouvelle d\u00e9finition <pre><code>export LOCAL_GITHUB_REPOS=\"${HOME}/code/github\"\n\nflux create helmrelease kube-prometheus-stack \\\n  --source=HelmRepository/prometheus-community \\\n  --chart=kube-prometheus-stack \\\n  --namespace=monitoring \\\n  --values-from=Secret/kube-prometheus-stack-custom-values \\\n  --export &gt; ${LOCAL_GITHUB_REPOS}/k8s-kind-fluxcd/apps/monitoring/helm-release.yaml\n</code></pre> <pre><code>---\napiVersion: helm.toolkit.fluxcd.io/v2beta1\nkind: HelmRelease\nmetadata:\n  name: kube-prometheus-stack\n  namespace: monitoring\nspec:\n  chart:\n    spec:\n      chart: kube-prometheus-stack\n      reconcileStrategy: ChartVersion\n      sourceRef:\n        kind: HelmRepository\n        name: prometheus-community\n  interval: 1m0s\n  values:\n    grafana:\n      adminPassword: my-cleartext-custom-password\n</code></pre> <pre><code>---\napiVersion: helm.toolkit.fluxcd.io/v2beta1\nkind: HelmRelease\nmetadata:\n  name: kube-prometheus-stack\n  namespace: monitoring\nspec:\n  chart:\n    spec:\n      chart: kube-prometheus-stack\n      reconcileStrategy: ChartVersion\n      sourceRef:\n        kind: HelmRepository\n        name: prometheus-community\n  interval: 1m0s\n  valuesFrom:\n  - kind: Secret\n    name: kube-prometheus-stack-custom-values\n</code></pre> <p>Appliquons les modifications sur notre cluster :</p> <pre><code>export LOCAL_GITHUB_REPOS=\"${HOME}/code/github\"\n\ncd ${LOCAL_GITHUB_REPOS}/k8s-kind-fluxcd\n\ngit add .\ngit commit -m \"feat: Helm release is now using vaulted custom values.\"\ngit push\n\nflux reconcile kustomization flux-system --with-source\n</code></pre> <p>Discord nous informe de la bonne mise \u00e0 jour de notre stack Prometheus :</p> <p></p> <p>V\u00e9rifions la bonne prise en compte du mot de passe stock\u00e9 dans Vault :</p> <pre><code>kubectl -n monitoring port-forward service/kube-prometheus-stack-grafana 8080:80\n</code></pre> <p></p> <p>Success</p> <p>Nous acc\u00e9dons \u00e0 Grafana avec le compte 'admin' et le mot de passe 'my-vaulted-custom-password' contenu dans Vault !  </p>"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/vault_auto-unsealed_external-secrets-operator_fluxcd/","title":"D\u00e9ploiement de Vault (auto-unsealed) et External Secrets Operator via FluxCD","text":""},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/vault_auto-unsealed_external-secrets-operator_fluxcd/#abstract","title":"Abstract","text":"<p>Ce howto fait suite au howto 'FluxCD / FluxCD - D\u00e9monstration par l'exemple.</p> <p>Jusqu'\u00e0 pr\u00e9sent, nous disposons d'un cluster Kubernetes pilot\u00e9 par FluxCD et sur lequel nous avons d\u00e9ploy\u00e9 deux applications : </p> <ul> <li>'agnhost', dont le code source est accessible depuis un d\u00e9p\u00f4t Git (GitHub dans notre cas);</li> <li>'podinfo', application packag\u00e9e avec Helm et que nous avons r\u00e9cup\u00e9r\u00e9e directement depuis sont d\u00e9p\u00f4t Helm (ou 'Helm Repository').</li> </ul> <p>Ces applications sont monitor\u00e9es via 'Discord', une messagerie instantan\u00e9e, qui nous alerte lorsqu'une modification est apport\u00e9e aux applications.</p> <p>Pour d\u00e9ployer nos applications, nous avons d\u00fb d\u00e9finir des 'secrets' sur notre cluster Kubernetes. Pour des raisons \u00e9videntes de s\u00e9curit\u00e9, nous ne les avons pas \u00e9crits dans notre code pour qu'ils ne se retrouvent pas sur nos d\u00e9p\u00f4ts Git.</p> <p>Le pr\u00e9sent HOWTO d\u00e9crit la mise en place d'un coffre ('HashiCorp Vault') dans lequel nous placerons nos 'secrets', et installerons \u00e9galement 'External Secrets Operator' (ou 'ESO') pour interagir avec ce dernier depuis les namespaces Kubernetes.</p> <p>Doc</p> <p>Vault - https://developer.hashicorp.com/vault</p> <p>External Secrets Operator - https://external-secrets.io/latest/</p>"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/vault_auto-unsealed_external-secrets-operator_fluxcd/#mode-operatoire","title":"Mode op\u00e9ratoire","text":"<p>Nous commencerons par pr\u00e9parer notre environnement local, un namespace d\u00e9di\u00e9 \u00e0 'Vault' et un autre \u00e0 'External Secrets Operator'; puis nous mettrons l'alerting 'Discord' en place, avant de d\u00e9ployer les 2 composants depuis leurs 'Helm Repositories' respectifs avec une configuration adapt\u00e9e \u00e0 notre besoin.</p>"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/vault_auto-unsealed_external-secrets-operator_fluxcd/#preparation-de-notre-environnement-de-developpement-local","title":"Pr\u00e9paration de notre environnement de d\u00e9veloppement (local)","text":"<pre><code># R\u00e9pertoire accueillant nos d\u00e9p\u00f4ts Git en local\nexport LOCAL_GITHUB_REPOS=\"${HOME}/code/github\"\n\n# Mise \u00e0 jour des copies locales des d\u00e9p\u00f4ts d\u00e9di\u00e9s \u00e0 FluxCD et aux applications qu'il g\u00e8re\ncd ${LOCAL_GITHUB_REPOS}/k8s-kind-apps   &amp;&amp; git pull\ncd ${LOCAL_GITHUB_REPOS}/k8s-kind-fluxcd &amp;&amp; git pull\n\n# Cr\u00e9ation d'un r\u00e9pertoire d\u00e9di\u00e9 \u00e0 la gestion des secrets\nmkdir -p ${LOCAL_GITHUB_REPOS}/k8s-kind-fluxcd/apps/vault\nmkdir -p ${LOCAL_GITHUB_REPOS}/k8s-kind-fluxcd/apps/external-secrets\n</code></pre>"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/vault_auto-unsealed_external-secrets-operator_fluxcd/#namespaces-dedies-a-vault-et-external-secrets-operator-eso","title":"Namespaces d\u00e9di\u00e9s \u00e0 Vault et External-Secrets Operator (ESO)","text":"<pre><code>kubectl create ns vault --dry-run=client -o yaml &gt; ${LOCAL_GITHUB_REPOS}/k8s-kind-fluxcd/apps/vault/vault.namespace.yaml\nkubectl create ns external-secrets --dry-run=client -o yaml &gt; ${LOCAL_GITHUB_REPOS}/k8s-kind-fluxcd/apps/external-secrets/external-secrets.namespace.yaml\n\nkubectl apply -f ${LOCAL_GITHUB_REPOS}/k8s-kind-fluxcd/apps/vault/vault.namespace.yaml\nkubectl apply -f ${LOCAL_GITHUB_REPOS}/k8s-kind-fluxcd/apps/external-secrets/external-secrets.namespace.yaml\n</code></pre>"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/vault_auto-unsealed_external-secrets-operator_fluxcd/#alerting-discord","title":"Alerting Discord","text":"<p>Nous passerons vite sur cette partie, car nous l'avons d\u00e9j\u00e0 bien document\u00e9e dans le HOWTO pr\u00e9c\u00e9dent.</p> <p>Nous utiliserons notre serveur Discord 'k8s' d\u00e9j\u00e0 existant et cr\u00e9erons pour Vault et pour External Secrets leur propre 'channel' ainsi qu'un 'webhook' associ\u00e9.</p> <p></p> Discord Channel WebHook URL vault https://discord.com/api/webhooks/1426889931195813968/5IuDXdjRNlpmaszWgRxHt4-P1QottSfWgFdg9bmuTSbwuquRpDZNus1U0AvRMyp26VMu external-secrets https://discord.com/api/webhooks/1426890931122208919/_isXhWPYEX1b2l_ST80xwUAuofsrPyWBUns5MyMkfXBKcsg_aK2Ay2Qtmbg0wU5Xe1Et"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/vault_auto-unsealed_external-secrets-operator_fluxcd/#definition-des-webhooks-des-channels-discord-vault-et-external-secrets","title":"D\u00e9finition des webhooks des channels Discord 'vault' et 'external-secrets'","text":"codevaultexternal-secrets <pre><code>export LOCAL_GITHUB_REPOS=\"${HOME}/code/github\"\nexport WEBHOOK_VAULT=\"https://discord.com/api/webhooks/1426889931195813968/5IuDXdjRNlpmaszWgRxHt4-P1QottSfWgFdg9bmuTSbwuquRpDZNus1U0AvRMyp26VMu\"\nexport WEBHOOK_EXTERNAL_SECRETS=\"https://discord.com/api/webhooks/1426890931122208919/_isXhWPYEX1b2l_ST80xwUAuofsrPyWBUns5=MyMkfXBKcsg_aK2Ay2Qtmbg0wU5Xe1Et\"\n\nkubectl -n vault            create secret generic discord-webhook --from-literal=address=${WEBHOOK_VAULT}\nkubectl -n external-secrets create secret generic discord-webhook --from-literal=address=${WEBHOOK_EXTERNAL_SECRETS} \n</code></pre> <pre><code>apiVersion: v1\ndata:\n  address: aHR0cHM6Ly9kaXNjb3JkLmNvbS9hcGkvd2ViaG9va3MvMTQyNjg4OTkzMTE5NTgxMzk2OC81SXVEWGRqUk5scG1hc3pXZ1J4SHQ0LVAxUW90dFNmV2dGZGc5Ym11VFNid3VxdVJwRFpOdXMxVTBBdlJNeXAyNlZNdQ==\nkind: Secret\nmetadata:\n  creationTimestamp: \"2025-10-12T11:40:41Z\"\n  name: discord-webhook\n  namespace: vault\n  resourceVersion: \"472479\"\n  uid: 3fcebb03-80a6-4edd-abf6-b9c41d8657da\ntype: Opaque\n</code></pre> <pre><code>apiVersion: v1\ndata:\n  address: aHR0cHM6Ly9kaXNjb3JkLmNvbS9hcGkvd2ViaG9va3MvMTQyNjg5MDkzMTEyMjIwODkxOS9faXNYaFdQWUVYMWIybF9TVDgweHdVQXVvZnNyUHlXQlVuczU9TXlNa2ZYQktjc2dfYUsyQXkyUXRtYmcwd1U1WGUxRXQ=\nkind: Secret\nmetadata:\n  creationTimestamp: \"2025-10-12T11:40:41Z\"\n  name: discord-webhook\n  namespace: external-secrets\n  resourceVersion: \"472480\"\n  uid: f16617d0-129b-4ce5-b0f4-a4bc084f223f\ntype: Opaque\n</code></pre>"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/vault_auto-unsealed_external-secrets-operator_fluxcd/#discord-notification-provider","title":"Discord notification provider","text":"codevaultexternal-secrets <pre><code>export LOCAL_GITHUB_REPOS=\"${HOME}/code/github\"\n\ncd ${LOCAL_GITHUB_REPOS}/k8s-kind-fluxcd\n\nflux create alert-provider discord \\\n  --type=discord \\\n  --secret-ref=discord-webhook \\\n  --channel=vault \\\n  --username=FluxCD \\\n  --namespace=vault \\\n  --export &gt; apps/vault/discord.notification-provider.yaml\n\nflux create alert-provider discord \\\n  --type=discord \\\n  --secret-ref=discord-webhook \\\n  --channel=external-secrets \\\n  --username=FluxCD \\\n  --namespace=external-secrets \\\n  --export &gt; apps/external-secrets/discord.notification-provider.yaml\n</code></pre> <pre><code>---\napiVersion: notification.toolkit.fluxcd.io/v1beta3\nkind: Provider\nmetadata:\n  name: discord\n  namespace: vault\nspec:\n  channel: vault\n  secretRef:\n    name: discord-webhook\n  type: discord\n  username: FluxCD\n</code></pre> <pre><code>---\napiVersion: notification.toolkit.fluxcd.io/v1beta3\nkind: Provider\nmetadata:\n  name: discord\n  namespace: external-secrets\nspec:\n  channel: external-secrets\n  secretRef:\n    name: discord-webhook\n  type: discord\n  username: FluxCD\n</code></pre>"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/vault_auto-unsealed_external-secrets-operator_fluxcd/#discord-alert","title":"Discord alert","text":"codevaultexternal-secrets <pre><code>export LOCAL_GITHUB_REPOS=\"${HOME}/code/github\"\n\ncd ${LOCAL_GITHUB_REPOS}/k8s-kind-fluxcd\n\nflux create alert discord \\\n  --event-severity=info \\\n  --event-source='GitRepository/*,Kustomization/*,ImageRepository/*,ImagePolicy/*,HelmRepository/*,HelmRelease/*' \\\n  --provider-ref=discord \\\n  --namespace=vault \\\n  --export &gt; apps/vault/discord.notification-alert.yaml\n\nflux create alert discord \\\n  --event-severity=info \\\n  --event-source='GitRepository/*,Kustomization/*,ImageRepository/*,ImagePolicy/*,HelmRepository/*,HelmRelease/*' \\\n  --provider-ref=discord \\\n  --namespace=external-secrets \\\n  --export &gt; apps/external-secrets/discord.notification-alert.yaml\n</code></pre> <pre><code>---\napiVersion: notification.toolkit.fluxcd.io/v1beta2\nkind: Alert\nmetadata:\n  name: discord\n  namespace: vault\nspec:\n  eventSeverity: info\n  eventSources:\n  - kind: GitRepository\n    name: '*'\n  - kind: Kustomization\n    name: '*'\n  - kind: ImageRepository\n    name: '*'\n  - kind: ImagePolicy\n    name: '*'\n  - kind: HelmRepository\n    name: '*'\n  - kind: HelmRelease\n    name: '*'\n  providerRef:\n    name: discord\n</code></pre> <pre><code>---\napiVersion: notification.toolkit.fluxcd.io/v1beta3\nkind: Alert\nmetadata:\n  name: discord\n  namespace: external-secrets\nspec:\n  eventSeverity: info\n  eventSources:\n  - kind: GitRepository\n    name: '*'\n  - kind: Kustomization\n    name: '*'\n  - kind: ImageRepository\n    name: '*'\n  - kind: ImagePolicy\n    name: '*'\n  - kind: HelmRepository\n    name: '*'\n  - kind: HelmRelease\n    name: '*'\n  providerRef:\n    name: discord\n</code></pre>"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/vault_auto-unsealed_external-secrets-operator_fluxcd/#activation-de-lalerting","title":"Activation de l'alerting","text":"<pre><code>export LOCAL_GITHUB_REPOS=\"${HOME}/code/github\"\n\ncd ${LOCAL_GITHUB_REPOS}/k8s-kind-fluxcd\n\ngit add .\ngit commit -m \"feat: setting up 'vault' and 'external secrets' Discord alerting.\"\ngit push\n\nflux reconcile kustomization flux-system --with-source\n</code></pre> <p>V\u00e9rification :</p> codenamespace 'vault'namespace 'external-secrets' <pre><code>kubectl -n vault            get providers,alerts\nkubectl -n external-secrets get providers,alerts\n</code></pre> <pre><code>NAME                                              AGE\nprovider.notification.toolkit.fluxcd.io/discord   2m49s\n\nNAME                                           AGE\nalert.notification.toolkit.fluxcd.io/discord   2m49s\n</code></pre> <pre><code>NAME                                              AGE\nprovider.notification.toolkit.fluxcd.io/discord   2m49s\n\nNAME                                           AGE\nalert.notification.toolkit.fluxcd.io/discord   2m49s\n</code></pre>"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/vault_auto-unsealed_external-secrets-operator_fluxcd/#helm-repositories","title":"Helm repositories","text":"<p>Doc</p> <p>Vault helm chart - https://developer.hashicorp.com/vault/docs/deploy/kubernetes/helm</p> <p>External secrets operator - https://external-secrets.io/latest/introduction/getting-started/#installing-with-helm</p> <p>Nous allons d\u00e9finir au niveau de FluxCD les 'Helm registries' pour installer sur notre cluster l'External Secrets Operator et HashiCorp Vault OSS :</p> code'hashicorp' helm repository'external-secrets' helm repository <pre><code>export LOCAL_GITHUB_REPOS=\"${HOME}/code/github\"\n\ncd ${LOCAL_GITHUB_REPOS}/k8s-kind-fluxcd\n\nflux create source helm hashicorp \\\n  --url=https://helm.releases.hashicorp.com \\\n  --namespace=vault \\\n  --interval=1m \\\n  --export &gt; apps/vault/hashicorp.helmrepository.yaml\n\nflux create source helm external-secrets \\\n  --url=https://charts.external-secrets.io \\\n  --namespace=external-secrets \\\n  --interval=1m \\\n  --export &gt; apps/external-secrets/external-secrets.helmrepository.yaml\n\ngit add .\ngit commit -m \"feat: Defined 'hashicorp' and 'external-secrets' helm repositories.\" \ngit push\n\nflux reconcile kustomization flux-system --with-source\n</code></pre> <pre><code>---\napiVersion: source.toolkit.fluxcd.io/v1\nkind: HelmRepository\nmetadata:\n  name: hashicorp\n  namespace: vault\nspec:\n  interval: 1m0s\n  url: https://helm.releases.hashicorp.com\n</code></pre> <pre><code>---\napiVersion: source.toolkit.fluxcd.io/v1\nkind: HelmRepository\nmetadata:\n  name: external-secrets\n  namespace: externel-secrets\nspec:\n  interval: 1m0s\n  url: https://charts.external-secrets.io\n</code></pre> <p>Discord nous informe tout de suite de la bonne cr\u00e9ation des 'Helm registries' :</p> <p></p> <p></p>"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/vault_auto-unsealed_external-secrets-operator_fluxcd/#deploiement-de-lexternal-secrets-operator-eso","title":"D\u00e9ploiement de l'External Secrets Operator (ESO)","text":"<p>Doc</p> <p>https://external-secrets.io/latest/introduction/overview/</p> <p>La solution fonctionne 'as-is' : il n'est pas n\u00e9cessaire de personnaliser la configuration de l'op\u00e9rateur.</p> <p>Nous allons donc nous contenter de d\u00e9finir dans le d\u00e9p\u00f4t GitHub que nous avons d\u00e9di\u00e9 \u00e0 nos applications (<code>k8s-kind-apps</code>) une Helm Release depuis la Helm Chart 'external-secrets' sans 'custom values'.</p>"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/vault_auto-unsealed_external-secrets-operator_fluxcd/#le-gitrepository-dedie-aux-applications","title":"Le GitRepository d\u00e9di\u00e9 aux applications","text":"<p>Commen\u00e7ons par d\u00e9finir le d\u00e9p\u00f4t Git o\u00f9 nous d\u00e9finirons notre Helm Release.</p>"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/vault_auto-unsealed_external-secrets-operator_fluxcd/#creation-et-installation-de-la-deploy-key","title":"Cr\u00e9ation et installation de la 'Deploy Key'","text":"<p>Pour acc\u00e9der \u00e0 notre GitRepository, nous devons nous authentifier.</p> codeoutput <pre><code>export GITHUB_USERNAME=papafrancky\n\nflux create secret git k8s-kind-apps-gitrepository-deploykeys \\\n  --url=ssh://github.com/${GITHUB_USERNAME}/k8s-kind-apps \\\n  --namespace=external-secrets\n\nkubectl -n external-secrets get secret k8s-kind-apps-gitrepository-deploykeys -o yaml\n</code></pre> <pre><code>apiVersion: v1\ndata:\n  identity: LS0tLS1CRUdJTiBQUklWQVRFIEtFWS0tLS0tCk1JRzJBZ0VBTUJBR0J5cUdTTTQ5QWdFR0JTdUJCQUFpQklHZU1JR2JBZ0VCQkREdnlWSWRMWjNlamYvTHBPb1AKdnVrbVp4Uzg3ZG15dFROaFV0MDFoTUlTU29KVHJseW94TjNKdndTUnJkY2VRdjZoWkFOaUFBUUwvYlB5SGhYRApmVElKSkR4OEZKTXhSMG5aNDRuU091a25lWEtWUUZWRVlobFBYVHB0Wm80SzR1MDl0UysvSHZVT0FUdG8rTkM2CklzWDNMLzJQazVieFdXZlVSRHFRWWtOcElPekpwcG1lMUJLZ2Z2cklpaEJCSEFpY2NHWGRpdVE9Ci0tLS0tRU5EIFBSSVZBVEUgS0VZLS0tLS0K\n  identity.pub: ZWNkc2Etc2hhMi1uaXN0cDM4NCBBQUFBRTJWalpITmhMWE5vWVRJdGJtbHpkSEF6T0RRQUFBQUlibWx6ZEhBek9EUUFBQUJoQkF2OXMvSWVGY045TWdra1BId1VrekZIU2RuamlkSTY2U2Q1Y3BWQVZVUmlHVTlkT20xbWpncmk3VDIxTDc4ZTlRNEJPMmo0MExvaXhmY3YvWStUbHZGWlo5UkVPcEJpUTJrZzdNbW1tWjdVRXFCKytzaUtFRUVjQ0p4d1pkMks1QT09Cg==\n  known_hosts: Z2l0aHViLmNvbSBlY2RzYS1zaGEyLW5pc3RwMjU2IEFBQUFFMlZqWkhOaExYTm9ZVEl0Ym1semRIQXlOVFlBQUFBSWJtbHpkSEF5TlRZQUFBQkJCRW1LU0VOalFFZXpPbXhrWk15N29wS2d3RkI5bmt0NVlScllNak51RzVOODd1UmdnNkNMcmJvNXdBZFQveTZ2MG1LVjBVMncwV1oyWUIvKytUcG9ja2c9\nkind: Secret\nmetadata:\n  creationTimestamp: \"2025-10-12T17:10:52Z\"\n  name: k8s-kind-apps-gitrepository-deploykeys\n  namespace: external-secrets\n  resourceVersion: \"486819\"\n  uid: 632669dc-9f66-4876-bc67-ece788d3ca55\ntype: Opaque\n</code></pre> <p>Nous devons d\u00e9ployer la Deploy Key (ie. la cl\u00e9 publique du jeu de cl\u00e9s que nous venons de cr\u00e9er) sur notre d\u00e9p\u00f4t GitHub 'k8s-kind-apps'.</p> <p>Commen\u00e7ons par r\u00e9cup\u00e9rer notre cl\u00e9 publique depuis le secret que nous venons de cr\u00e9er : </p> codeoutput <pre><code>kubectl -n external-secrets get secret k8s-kind-apps-gitrepository-deploykeys -o jsonpath='{.data.identity\\.pub}' | base64 -D\n</code></pre> <pre><code>ecdsa-sha2-nistp384 AAAAE2VjZHNhLXNoYTItbmlzdHAzODQAAAAIbmlzdHAzODQAAABhBAv9s/IeFcN9MgkkPHwUkzFHSdnjidI66Sd5cpVAVURiGU9dOm1mjgri7T21L78e9Q4BO2j40Loixfcv/Y+TlvFZZ9REOpBiQ2kg7MmmmZ7UEqB++siKEEEcCJxwZd2K5A==\n</code></pre> <p>Int\u00e9grons-la enfin sur le d\u00e9p\u00f4t GitHub d\u00e9di\u00e9 \u00e0 nos applications :</p> <p></p> <p></p>"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/vault_auto-unsealed_external-secrets-operator_fluxcd/#definition-du-gitrepository","title":"D\u00e9finition du GitRepository","text":"<p>Nous avons d\u00e9fini la 'Deploy Key', nous pouvons d\u00e9sormais d\u00e9finir le d\u00e9p\u00f4t Git :</p> code'k8s-kind-apps' GitRepository <pre><code>export LOCAL_GITHUB_REPOS=\"${HOME}/code/github\"\nexport GITHUB_USERNAME=papafrancky\n\ncd ${LOCAL_GITHUB_REPOS}/k8s-kind-fluxcd\n\nflux create source git k8s-kind-apps \\\n  --url=ssh://git@github.com/${GITHUB_USERNAME}/k8s-kind-apps.git \\\n  --branch=main \\\n  --secret-ref=k8s-kind-apps-gitrepository-deploykeys \\\n  --namespace=external-secrets \\\n  --export &gt; apps/external-secrets/k8s-kind-apps.gitrepository.yaml\n</code></pre> <pre><code>---\napiVersion: source.toolkit.fluxcd.io/v1\nkind: GitRepository\nmetadata:\n  name: k8s-kind-apps\n  namespace: external-secrets\nspec:\n  interval: 1m0s\n  ref:\n    branch: main\n  secretRef:\n    name: k8s-kind-apps-gitrepository-deploykeys\n  url: ssh://git@github.com/papafrancky/k8s-kind-apps.git\n</code></pre>"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/vault_auto-unsealed_external-secrets-operator_fluxcd/#la-kustomization-external-secrets","title":"La Kustomization 'external-secrets'","text":"<p>Nous devons demander \u00e0 FluxCD de surveiller le sous-r\u00e9pertoire que nous d\u00e9dierons \u00e0 l'External Secrets Operator sur le d\u00e9p\u00f4t GitHub d\u00e9di\u00e9 aux applications :</p> code'external-secrets' kustomization <pre><code>export LOCAL_GITHUB_REPOS=\"${HOME}/code/github\"\ncd ${LOCAL_GITHUB_REPOS}/k8s-kind-fluxcd\n\nflux create kustomization external-secrets \\\n  --source=GitRepository/k8s-kind-apps.external-secrets \\\n  --path=./external-secrets \\\n  --prune=true \\\n  --namespace=external-secrets \\\n  --export  &gt; apps/external-secrets/external-secrets.kustomization.yaml\n\ngit add .\ngit commit -m 'feat: Defined a deploy key, a git repository and a kustomization for external secrets operator.'\ngit push\n\nflux reconcile kustomization flux-system --with-source\n</code></pre> <pre><code>---\napiVersion: kustomize.toolkit.fluxcd.io/v1\nkind: Kustomization\nmetadata:\n  name: external-secrets\n  namespace: external-secrets\nspec:\n  interval: 1m0s\n  path: ./external-secrets\n  prune: true\n  sourceRef:\n    kind: GitRepository\n    name: k8s-kind-apps\n    namespace: external-secrets\n</code></pre> <p>Discord pr\u00e9vient d'une erreur li\u00e9e \u00e0 la kustomization 'external-secrets' :</p> <p></p> <p>C'est peut-\u00eatre normal : dans le d\u00e9p\u00f4t <code>k8s-kind-apps</code>, le sous-r\u00e9pertoire <code>./external-secrets</code> est vide. Il est temps d'y d\u00e9finir la HelmRelease.</p>"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/vault_auto-unsealed_external-secrets-operator_fluxcd/#la-helm-release-external-secrets","title":"La Helm Release 'external-secrets'","text":"code'external-secrets' Helm release <pre><code>export LOCAL_GITHUB_REPOS=\"${HOME}/code/github\"\n\ncd ${LOCAL_GITHUB_REPOS}/k8s-kind-apps\nmkdir external-secrets\n\nflux create helmrelease external-secrets \\\n  --source=HelmRepository/external-secrets \\\n  --chart=external-secrets \\\n  --namespace=external-secrets \\\n  --export &gt; ./external-secrets/external-secrets.helmrelease.yaml\n\ngit add .\ngit commit -m 'feat: Defined external-secrets helm release.'\ngit push\n\nflux -n external-secrets reconcile kustomization external-secrets --with-source\n</code></pre> <pre><code>---\napiVersion: helm.toolkit.fluxcd.io/v2\nkind: HelmRelease\nmetadata:\n  name: external-secrets\n  namespace: external-secrets\nspec:\n  chart:\n    spec:\n      chart: external-secrets\n      reconcileStrategy: ChartVersion\n      sourceRef:\n        kind: HelmRepository\n        name: external-secrets\n  interval: 1m0s\n</code></pre> <p>Discord nous annonce la r\u00e9solution du probl\u00e8me autour de la kustomization 'external secrets' ainsi que le bon d\u00e9ploiement de la Helm release :</p> <p></p> <p>Regardons quels objets ont \u00e9t\u00e9 d\u00e9ploy\u00e9s sur le cluster :</p> codeoutput <pre><code>kubectl -n external-secrets get all -l app.kubernetes.io/name=external-secrets\n</code></pre> <pre><code>NAME                                   READY   STATUS    RESTARTS   AGE\npod/external-secrets-f44d64679-446nt   1/1     Running   0          10m\n\nNAME                               READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/external-secrets   1/1     1            1           10m\n\nNAME                                         DESIRED   CURRENT   READY   AGE\nreplicaset.apps/external-secrets-f44d64679   1         1         1       10m\n</code></pre> <p>Effectons une derni\u00e8re v\u00e9rification : </p> codeoutput <pre><code>kubectl -n external-secrets get all\n</code></pre> <pre><code>NAME                                                   READY   STATUS    RESTARTS   AGE\npod/external-secrets-cert-controller-6658f6fb8-2bsrj   1/1     Running   0          12m\npod/external-secrets-f44d64679-446nt                   1/1     Running   0          12m\npod/external-secrets-webhook-955948cd4-599dt           1/1     Running   0          12m\n\nNAME                               TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)   AGE\nservice/external-secrets-webhook   ClusterIP   10.43.39.24   &lt;none&gt;        443/TCP   12m\n\nNAME                                               READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/external-secrets                   1/1     1            1           12m\ndeployment.apps/external-secrets-cert-controller   1/1     1            1           12m\ndeployment.apps/external-secrets-webhook           1/1     1            1           12m\n\nNAME                                                         DESIRED   CURRENT   READY   AGE\nreplicaset.apps/external-secrets-cert-controller-6658f6fb8   1         1         1       12m\nreplicaset.apps/external-secrets-f44d64679                   1         1         1       12m\nreplicaset.apps/external-secrets-webhook-955948cd4           1         1         1       12m\n</code></pre> <p>Success</p> <p>'External-Secrets Operator (ESO)' est d\u00e9ploy\u00e9 correctement sur notre cluster ! </p>"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/vault_auto-unsealed_external-secrets-operator_fluxcd/#deploiement-de-vault-oss","title":"D\u00e9ploiement de Vault OSS","text":""},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/vault_auto-unsealed_external-secrets-operator_fluxcd/#google-cloud-platform","title":"Google Cloud Platform","text":"<p>Le m\u00e9canisme d'auto-unseal de Vault repose sur l'utilisation d'un service de gestion de cl\u00e9s (ie. 'Key Management System', ou 'KMS') propos\u00e9 par un 'Cloud Service Provider' ('CSP'). Notre choix s'est port\u00e9 sur la plateforme 'Google Cloud Platform' ('CGP') mais tout autre CSP proposant un KMS aurait pu faire l'affaire.</p> <p>Info</p> <p>https://developer.hashicorp.com/vault/tutorials/auto-unseal/autounseal-gcp-kms</p>"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/vault_auto-unsealed_external-secrets-operator_fluxcd/#projet-gcp","title":"Projet GCP","text":"<p>Nous disposons d'un compte GCP et avons pr\u00e9alablement cr\u00e9\u00e9 le projet suivant :</p> KEY VALUE Project Name papaFrancky Project ID papafrancky"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/vault_auto-unsealed_external-secrets-operator_fluxcd/#activation-des-apis","title":"Activation des APIs","text":"<p>Pour consommer les services GCP, il faut activer leurs APIs.</p> APIs \u00e0 activer Cloud Key Management Service (KMS) API Compute Engine API"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/vault_auto-unsealed_external-secrets-operator_fluxcd/#activation-des-apis-gcp-via-la-console-web-ui","title":"Activation des APIs GCP via la console (web UI)","text":"<p>Tip</p> <p>APIs &amp; Services &gt; Enabled APIs &amp; Services &gt; + ENABLE APIS AND SERVICES</p> <p>S\u00e9lectionner 'Enabled APIs and Services' dans la colonne de gauche : </p> <p>Ajout d'une nouvelle API : </p> <p>Recherche de l'API KMS : </p> <p>S\u00e9lection de l'API KMS : </p> <p>Activation de l'API KMS : </p> <p>Ajout d'une nouvelle API : </p> <p>Validation et retour \u00e0 la recherche d'APIs : </p> <p>Recherche de l'API Compute : </p> <p>S\u00e9lection de l'API Compute : </p> <p>Activation de l'API Compute : </p>"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/vault_auto-unsealed_external-secrets-operator_fluxcd/#activation-des-apis-gcp-via-gcloud-cli","title":"Activation des APIs GCP via gcloud (CLI)","text":"<pre><code># gcloud update\ngcloud components update --quiet\n\n# Authentification\ngcloud auth login\n\n# Affichage des projets existants\ngcloud projects list\n  # PROJECT_ID   NAME         PROJECT_NUMBER\n  # papafrancky  papaFrancky  11623004919\n\n# S\u00e9lection du projet papaFrancky\ngcloud config set project papafrancky\n\n# Activation des APIS 'cloudkms' et 'compute'\ngcloud services enable cloudkms.googleapis.com compute.googleapis.com\n\n# V\u00e9rification de la bonne activation des APIS\ngcloud services list | grep -E 'kms|compute'    \n</code></pre>"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/vault_auto-unsealed_external-secrets-operator_fluxcd/#service-account","title":"Service account","text":"<p>Vault utilisera un service-account GCP (en fournissant ses credentials) qui disposera des droits d'acc\u00e8s \u00e0 une cl\u00e9 h\u00e9berg\u00e9e dans le Key Management Service (KMS) de GCP. Param\u00e9tr\u00e9 en mode auto-unseal, Vault se servira de cette cl\u00e9 comme root key pour prot\u00e8ger l'encryption key.</p>"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/vault_auto-unsealed_external-secrets-operator_fluxcd/#creation-du-service-account-via-la-console-web-ui","title":"Cr\u00e9ation du service account via la console (Web UI)","text":"<p>Tip</p> <p>IAM &amp; Admin &gt; Service Accounts &gt; + CREATE SERVICE ACCOUNT</p> <p>S\u00e9lection de 'APIs and services' &gt; 'service account' : </p> <p>Cr\u00e9ation d'un nouveau service account : </p> <p></p> <p>Validation de la cr\u00e9ation du service account : </p>"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/vault_auto-unsealed_external-secrets-operator_fluxcd/#creation-du-service-account-via-gcloud-cli","title":"Cr\u00e9ation du service account via gcloud (CLI)","text":"code <pre><code>gcloud iam service-accounts create k8s-vault-test \\\n  --project=\"papafrancky\" \\\n  --description=\"Vault OSS\" \\\n  --display-name=\"k8s-vault\"\n</code></pre>"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/vault_auto-unsealed_external-secrets-operator_fluxcd/#service-account-key","title":"Service-account key","text":"<p>Pour consommer les APIS activ\u00e9es sur notre projet GCP avec les privil\u00e8ges associ\u00e9s au service account 'k8s-vault', Vault OSS aura besoin de la cl\u00e9 priv\u00e9e de ce dernier. Nous D\u00e9crirons ici la cr\u00e9ation de cette cl\u00e9 priv\u00e9e.</p>"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/vault_auto-unsealed_external-secrets-operator_fluxcd/#creation-de-la-cle-via-la-console-web-ui","title":"Cr\u00e9ation de la cl\u00e9 via la console (Web UI)","text":"<p>La cr\u00e9ation d'une cl\u00e9 d\u00e9clenche le t\u00e9l\u00e9chargement d'un fichier texte au format JSON : </p> <p></p>"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/vault_auto-unsealed_external-secrets-operator_fluxcd/#creation-de-la-cle-via-gcloud-cli","title":"Cr\u00e9ation de la cl\u00e9 via gcloud (CLI)","text":"<p>Note</p> <p>Nous pr\u00e9ciserons que nous souhaitons r\u00e9cup\u00e9rer le fichier JSON contenant la cl\u00e9 priv\u00e9e de notre service account en local dans le fichier <code>${HOME}/vault-sa-key</code>.</p> codeoutputCl\u00e9 priv\u00e9e au format JSON <pre><code>gcloud iam service-accounts keys create ${HOME}/vault-sa-key \\\n  --iam-account=k8s-vault@papafrancky.iam.gserviceaccount.com\n</code></pre> <pre><code>created key [22a3198bbbd9984c615b3600d0eef0183091c4bb] of type [json] as [/Users/franck/vault-sa-key] for [k8s-vault@papafrancky.iam.gserviceaccount.com]\n</code></pre> <pre><code>{\n  \"type\": \"service_account\",\n  \"project_id\": \"papafrancky\",\n  \"private_key_id\": \"9888481c9848a94a8549c8b58112642fa8032a2f\",\n  \"private_key\": \"-----BEGIN PRIVATE KEY-----\\nMIIEvAIBADANBgkqhkiG9w0BAQEFAASCBKYwggSiAgEAAoIBAQCfKVHMPAX5sbri\\n8lQUMELRGipMxsd    +FwFQQ0uYSYRW9YgK6YgyJojTnnh/XJqPk/JLP/tEP4cg2wFe\\nrNw7+PWe3j1t0TZPMHzSUylm6CKHfqdXGiaDRmmDp8pbTtLGCk   +NrqN7IIXZrZq1\\nNnEQ4WRQciRZmHba2NL7jM6kPgpkRR2vAr0rnz0LJe1UI0em1EPqAtOTyRIwn3fV\\n3Sccbl2WI2G5Dhq1DaLkz9D/  dWfpmGzDPURfGQJ9Igen7Erw0mzcQQ81F2j/+RuH\\npmOKUrH8I9thb00BHIa54VwSENzAL/UV1p   +7W22O61uqdpjpDiUm6eaY27DstBP4\\nSeVJeOWvAgMBAAECggEAO1VshH413g1XNcaP+Iy7Q/eEoGWUNhm+aB0GP+ncN4zF\\n4AeKOePqqzVPB2/   OwMqvD0V1Vs52AhYSzygS3Bql+kkwTlGpdOEHD2NoMlBEmw1o\\ntcxuYg7sQ+PXyK08Xw619IQPttV7gJg67eYzRU85FdJPYcU4PcKJ+LPXzxu/  xPv2\\nCK0KIeX6taQNPpTY48lqRAs0gl9yHpRwTEGmWkWcic5qH9F6hAnfp4SPbOFAXtTm\\n7JTtsE0hd1GSnZFl2aGep6yfpU6MrhXNZLjUBFnB7XZw3ZBHRnsEx2GAkTaRs3   Uj\\nXSKt6STH+p1q+u3HaeNwl9PvyykBNb7P4JxGxSij7QKBgQDcsisG0Oym0zI+Wc2T\\nG9lElL4cn7Xa62qys9tQggd4i6CMI/  nBlPXqkQ1or2JVTfehB0grRJAVeE2VWseT\\nJaum7imkadp259/LvgRqrZDbWMZ9rfbi9fBE0CCvTakLs7Y5ZDGiVqYEOFzrzwh/\\ne0/    9xPMOn6nyEFq0PErZWs969QKBgQC4nzftBSWEQe4BZ2GZMB1pqWzMeu+pxtO4\\ntstEK4gbihhVdkcLNW7NjqvX1W3+zEFDQTzFmZCdNivyLpaw5gdPI9Y9Kr7ZqaG/   \\n4PR8bPowDAXqh28UbEWgvheKUqblj0cL4EW54OPRh6n2Xk5neXAmBId1Ux2iRKT3\\n29k/Wuc/kwKBgFnrRdibGzDFb/  0zfazoddeZevQSpnex32E8IqlksUKOMTWoGsSi\\nuqd9vibe/oOfJru3SdJHNyVoRMQLnrD3cj6rXtAcSOSViPtsSkRkv1Z/jy/5x1Ol\\nvOVsn0SNsciQyjgL   +KUaBL5HcKSrT90REwBkVFuq9gUoYKx6vExe0ZZZAoGARHxT\\nq6noE9q9JmoqK0BM+OAvit7jvrAR5Ahy  +LPJRqYAhttcWU0V8EzHdEYpCobMgt1V\\nZNulaJwqyyj7H14FQhdCJuiPaYDijqItL2bhnCcpnqlkzEepwIojg11LqgZvTXmB\\nhAVnnVdc9hZfsUS8FG5DrYMKkCI0q/   ky6qyW8d8CgYAkH9WQFS7xR2oIzSkCH8NW\\nZKlO9ruGUSIUT0pIdh5yD0oGgFN/cR3f2sEgj5wX7zXg3+fJ/f7wBKx438hWw540\\nzD58uxl/9brOYx95/   dld58AGiLWvYhJBgVI1QZNPmXZxMHdW86/G+3S/JQC6nprE\\nMXVybj1Axo6H+6LsSbb71w==\\n-----END PRIVATE KEY-----\\n\",\n  \"client_email\": \"k8s-vault@papafrancky.iam.gserviceaccount.com\",\n  \"client_id\": \"114409726477469875395\",\n  \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n  \"token_uri\": \"https://oauth2.googleapis.com/token\",\n  \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n  \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/k8s-vault%40papafrancky.iam.gserviceaccount.com\",\n  \"universe_domain\": \"googleapis.com\"\n}\n</code></pre> <p>Nous allons tout de suite int\u00e9grer cette cl\u00e9 sous la forme de secret Kubernetes dans le namespace d\u00e9di\u00e9 \u00e0 Vault :</p> <pre><code>kubectl -n vault create secret generic vault-sa-key --from-file=${HOME}/vault-sa-key\n</code></pre> <p>V\u00e9rifions notre objet nouvellement cr\u00e9\u00e9 :</p> codeoutput <pre><code>kubectl -n vault get secret vault-sa-key -o yaml\n</code></pre> <pre><code>apiVersion: v1\ndata:\n  vault-sa-key:     ewogICJ0eXBlIjogInNlcnZpY2VfYWNjb3VudCIsCiAgInByb2plY3RfaWQiOiAicGFwYWZyYW5ja3kiLAogICJwcml2YXRlX2tleV9pZCI6ICI5ODg4NDgxYzk4NDhhOTRhODU0OW    M4YjU4MTEyNjQyZmE4MDMyYTJmIiwKICAicHJpdmF0ZV9rZXkiOiAiLS0tLS1CRUdJTiBQUklWQVRFIEtFWS0tLS0tXG5NSUlFdkFJQkFEQU5CZ2txaGtpRzl3MEJBUUVGQUFTQ0JL    WXdnZ1NpQWdFQUFvSUJBUUNmS1ZITVBBWDVzYnJpXG44bFFVTUVMUkdpcE14c2QrRndGUVEwdVlTWVJXOVlnSzZZZ3lKb2pUbm5oL1hKcVBrL0pMUC90RVA0Y2cyd0ZlXG5yTnc3K1    BXZTNqMXQwVFpQTUh6U1V5bG02Q0tIZnFkWEdpYURSbW1EcDhwYlR0TEdDaytOcnFON0lJWFpyWnExXG5ObkVRNFdSUWNpUlptSGJhMk5MN2pNNmtQZ3BrUlIydkFyMHJuejBMSmUx    VUkwZW0xRVBxQXRPVHlSSXduM2ZWXG4zU2NjYmwyV0kyRzVEaHExRGFMa3o5RC9kV2ZwbUd6RFBVUmZHUUo5SWdlbjdFcncwbXpjUVE4MUYyai8rUnVIXG5wbU9LVXJIOEk5dGhiMD    BCSElhNTRWd1NFTnpBTC9VVjFwKzdXMjJPNjF1cWRwanBEaVVtNmVhWTI3RHN0QlA0XG5TZVZKZU9XdkFnTUJBQUVDZ2dFQU8xVnNoSDQxM2cxWE5jYVArSXk3US9lRW9HV1VOaG0r    YUIwR1ArbmNONHpGXG40QWVLT2VQcXF6VlBCMi9Pd01xdkQwVjFWczUyQWhZU3p5Z1MzQnFsK2trd1RsR3BkT0VIRDJOb01sQkVtdzFvXG50Y3h1WWc3c1ErUFh5SzA4WHc2MTlJUV    B0dFY3Z0pnNjdlWXpSVTg1RmRKUFljVTRQY0tKK0xQWHp4dS94UHYyXG5DSzBLSWVYNnRhUU5QcFRZNDhscVJBczBnbDl5SHBSd1RFR21Xa1djaWM1cUg5RjZoQW5mcDRTUGJPRkFY    dFRtXG43SlR0c0UwaGQxR1NuWkZsMmFHZXA2eWZwVTZNcmhYTlpMalVCRm5CN1hadzNaQkhSbnNFeDJHQWtUYVJzM1VqXG5YU0t0NlNUSCtwMXErdTNIYWVOd2w5UHZ5eWtCTmI3UD    RKeEd4U2lqN1FLQmdRRGNzaXNHME95bTB6SStXYzJUXG5HOWxFbEw0Y243WGE2MnF5czl0UWdnZDRpNkNNSS9uQmxQWHFrUTFvcjJKVlRmZWhCMGdyUkpBVmVFMlZXc2VUXG5KYXVt    N2lta2FkcDI1OS9MdmdScXJaRGJXTVo5cmZiaTlmQkUwQ0N2VGFrTHM3WTVaREdpVnFZRU9GenJ6d2gvXG5lMC85eFBNT242bnlFRnEwUEVyWldzOTY5UUtCZ1FDNG56ZnRCU1dFUW    U0QloyR1pNQjFwcVd6TWV1K3B4dE80XG50c3RFSzRnYmloaFZka2NMTlc3TmpxdlgxVzMrekVGRFFUekZtWkNkTml2eUxwYXc1Z2RQSTlZOUtyN1pxYUcvXG40UFI4YlBvd0RBWHFo    MjhVYkVXZ3ZoZUtVcWJsajBjTDRFVzU0T1BSaDZuMlhrNW5lWEFtQklkMVV4MmlSS1QzXG4yOWsvV3VjL2t3S0JnRm5yUmRpYkd6REZiLzB6ZmF6b2RkZVpldlFTcG5leDMyRThJcW    xrc1VLT01UV29Hc1NpXG51cWQ5dmliZS9vT2ZKcnUzU2RKSE55Vm9STVFMbnJEM2NqNnJYdEFjU09TVmlQdHNTa1JrdjFaL2p5LzV4MU9sXG52T1ZzbjBTTnNjaVF5amdMK0tVYUJM    NUhjS1NyVDkwUkV3QmtWRnVxOWdVb1lLeDZ2RXhlMFpaWkFvR0FSSHhUXG5xNm5vRTlxOUptb3FLMEJNK09Bdml0N2p2ckFSNUFoeStMUEpScVlBaHR0Y1dVMFY4RXpIZEVZcENvYk    1ndDFWXG5aTnVsYUp3cXl5ajdIMTRGUWhkQ0p1aVBhWURpanFJdEwyYmhuQ2NwbnFsa3pFZXB3SW9qZzExTHFnWnZUWG1CXG5oQVZublZkYzloWmZzVVM4Rkc1RHJZTUtrQ0kwcS9r    eTZxeVc4ZDhDZ1lBa0g5V1FGUzd4UjJvSXpTa0NIOE5XXG5aS2xPOXJ1R1VTSVVUMHBJZGg1eUQwb0dnRk4vY1IzZjJzRWdqNXdYN3pYZzMrZkovZjd3Qkt4NDM4aFd3NTQwXG56RD    U4dXhsLzlick9ZeDk1L2RsZDU4QUdpTFd2WWhKQmdWSTFRWk5QbVhaeE1IZFc4Ni9HKzNTL0pRQzZucHJFXG5NWFZ5YmoxQXhvNkgrNkxzU2JiNzF3PT1cbi0tLS0tRU5EIFBSSVZB    VEUgS0VZLS0tLS1cbiIsCiAgImNsaWVudF9lbWFpbCI6ICJrOHMtdmF1bHRAcGFwYWZyYW5ja3kuaWFtLmdzZXJ2aWNlYWNjb3VudC5jb20iLAogICJjbGllbnRfaWQiOiAiMTE0ND    A5NzI2NDc3NDY5ODc1Mzk1IiwKICAiYXV0aF91cmkiOiAiaHR0cHM6Ly9hY2NvdW50cy5nb29nbGUuY29tL28vb2F1dGgyL2F1dGgiLAogICJ0b2tlbl91cmkiOiAiaHR0cHM6Ly9v    YXV0aDIuZ29vZ2xlYXBpcy5jb20vdG9rZW4iLAogICJhdXRoX3Byb3ZpZGVyX3g1MDlfY2VydF91cmwiOiAiaHR0cHM6Ly93d3cuZ29vZ2xlYXBpcy5jb20vb2F1dGgyL3YxL2Nlcn    RzIiwKICAiY2xpZW50X3g1MDlfY2VydF91cmwiOiAiaHR0cHM6Ly93d3cuZ29vZ2xlYXBpcy5jb20vcm9ib3QvdjEvbWV0YWRhdGEveDUwOS9rOHMtdmF1bHQlNDBwYXBhZnJhbmNr    eS5pYW0uZ3NlcnZpY2VhY2NvdW50LmNvbSIsCiAgInVuaXZlcnNlX2RvbWFpbiI6ICJnb29nbGVhcGlzLmNvbSIKfQo=\nkind: Secret\nmetadata:\n  creationTimestamp: \"2025-11-10T15:10:23Z\"\n  name: vault-sa-key\n  namespace: vault\n  resourceVersion: \"1050214\"\n  uid: eef1fcc1-555e-4e88-90de-36195257616f\ntype: Opaque\n</code></pre> <p>S'agissant d'un objet de type 'secret', le contenu est en base 64. D\u00e9chiffrons-le pour nous assurer que son contenu correspond bien au fichier JSON t\u00e9l\u00e9charg\u00e9 depuis GCP et contenant la cl\u00e9 priv\u00e9e de notre 'service account':</p> codeoutput <pre><code>kubectl -n vault get secret vault-sa-key -o yaml | yq -r .data.vault-sa-key | base64 -d\n</code></pre> <pre><code>{\n  \"type\": \"service_account\",\n  \"project_id\": \"papafrancky\",\n  \"private_key_id\": \"9888481c9848a94a8549c8b58112642fa8032a2f\",\n  \"private_key\": \"-----BEGIN PRIVATE KEY-----\\nMIIEvAIBADANBgkqhkiG9w0BAQEFAASCBKYwggSiAgEAAoIBAQCfKVHMPAX5sbri\\n8lQUMELRGipMxsd    +FwFQQ0uYSYRW9YgK6YgyJojTnnh/XJqPk/JLP/tEP4cg2wFe\\nrNw7+PWe3j1t0TZPMHzSUylm6CKHfqdXGiaDRmmDp8pbTtLGCk   +NrqN7IIXZrZq1\\nNnEQ4WRQciRZmHba2NL7jM6kPgpkRR2vAr0rnz0LJe1UI0em1EPqAtOTyRIwn3fV\\n3Sccbl2WI2G5Dhq1DaLkz9D/  dWfpmGzDPURfGQJ9Igen7Erw0mzcQQ81F2j/+RuH\\npmOKUrH8I9thb00BHIa54VwSENzAL/UV1p   +7W22O61uqdpjpDiUm6eaY27DstBP4\\nSeVJeOWvAgMBAAECggEAO1VshH413g1XNcaP+Iy7Q/eEoGWUNhm+aB0GP+ncN4zF\\n4AeKOePqqzVPB2/OwMqvD0V1Vs52AhYSzygS3Bql  +kkwTlGpdOEHD2NoMlBEmw1o\\ntcxuYg7sQ+PXyK08Xw619IQPttV7gJg67eYzRU85FdJPYcU4PcKJ+LPXzxu/   xPv2\\nCK0KIeX6taQNPpTY48lqRAs0gl9yHpRwTEGmWkWcic5qH9F6hAnfp4SPbOFAXtTm\\n7JTtsE0hd1GSnZFl2aGep6yfpU6MrhXNZLjUBFnB7XZw3ZBHRnsEx2GAkTaRs3Uj\\n  XSKt6STH+p1q+u3HaeNwl9PvyykBNb7P4JxGxSij7QKBgQDcsisG0Oym0zI+Wc2T\\nG9lElL4cn7Xa62qys9tQggd4i6CMI/   nBlPXqkQ1or2JVTfehB0grRJAVeE2VWseT\\nJaum7imkadp259/LvgRqrZDbWMZ9rfbi9fBE0CCvTakLs7Y5ZDGiVqYEOFzrzwh/\\ne0/   9xPMOn6nyEFq0PErZWs969QKBgQC4nzftBSWEQe4BZ2GZMB1pqWzMeu+pxtO4\\ntstEK4gbihhVdkcLNW7NjqvX1W3+zEFDQTzFmZCdNivyLpaw5gdPI9Y9Kr7ZqaG/  \\n4PR8bPowDAXqh28UbEWgvheKUqblj0cL4EW54OPRh6n2Xk5neXAmBId1Ux2iRKT3\\n29k/Wuc/kwKBgFnrRdibGzDFb/   0zfazoddeZevQSpnex32E8IqlksUKOMTWoGsSi\\nuqd9vibe/oOfJru3SdJHNyVoRMQLnrD3cj6rXtAcSOSViPtsSkRkv1Z/jy/5x1Ol\\nvOVsn0SNsciQyjgL  +KUaBL5HcKSrT90REwBkVFuq9gUoYKx6vExe0ZZZAoGARHxT\\nq6noE9q9JmoqK0BM+OAvit7jvrAR5Ahy   +LPJRqYAhttcWU0V8EzHdEYpCobMgt1V\\nZNulaJwqyyj7H14FQhdCJuiPaYDijqItL2bhnCcpnqlkzEepwIojg11LqgZvTXmB\\nhAVnnVdc9hZfsUS8FG5DrYMKkCI0q/  ky6qyW8d8CgYAkH9WQFS7xR2oIzSkCH8NW\\nZKlO9ruGUSIUT0pIdh5yD0oGgFN/cR3f2sEgj5wX7zXg3+fJ/f7wBKx438hWw540\\nzD58uxl/9brOYx95/    dld58AGiLWvYhJBgVI1QZNPmXZxMHdW86/G+3S/JQC6nprE\\nMXVybj1Axo6H+6LsSbb71w==\\n-----END PRIVATE KEY-----\\n\",\n  \"client_email\": \"k8s-vault@papafrancky.iam.gserviceaccount.com\",\n  \"client_id\": \"114409726477469875395\",\n  \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n  \"token_uri\": \"https://oauth2.googleapis.com/token\",\n  \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n  \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/k8s-vault%40papafrancky.iam.gserviceaccount.com\",\n  \"universe_domain\": \"googleapis.com\"\n}\n</code></pre>"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/vault_auto-unsealed_external-secrets-operator_fluxcd/#kms-key","title":"KMS key","text":"<p>Il s'agit maintenant de cr\u00e9er dans le service GCP 'KMS' la cl\u00e9 qui servira de 'root key' pour Vault OSS.</p> <p>Warning</p> <p>Avant de cr\u00e9er la cl\u00e9, nous devons cr\u00e9er son trousseau (ou 'key ring').</p> <p>Informations concernant le trousseau :</p> KEY VALUE Key ring name k8s-vault Single/multi region single Region europe-west9 <p>Informations concernant la cl\u00e9  :</p> KEY VALUE Key name k8s-vault Protection level software Key material generated Purpose and algorithm symmetric encrypt/decrypt Key rotation 90d"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/vault_auto-unsealed_external-secrets-operator_fluxcd/#creation-du-trousseau-ou-key-ring-et-de-sa-cle-via-la-console-web-ui","title":"Cr\u00e9ation du trousseau (ou 'key ring') et de sa cl\u00e9 via la console (Web UI)","text":""},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/vault_auto-unsealed_external-secrets-operator_fluxcd/#creation-du-trousseau-ou-key-ring-et-de-sa-cle-via-gcloud-cli","title":"Cr\u00e9ation du trousseau (ou 'key ring') et de sa cl\u00e9 via gcloud (CLI)","text":"code <pre><code># Cr\u00e9ation du trousseau (ou 'key ring') :\ngcloud kms keyrings create k8s-vault \\\n  --location europe-west9\n\n# Cr\u00e9ation de la cl\u00e9 :\ngcloud kms keys create k8s-vault \\\n  --keyring k8s-vault \\\n  --location europe-west9 \\\n  --purpose encryption \\\n  --rotation-period \"90d\" \\\n  --next-rotation-time \"2025-11-01T00:00:00\"\n</code></pre>"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/vault_auto-unsealed_external-secrets-operator_fluxcd/#acces-du-service-account-a-la-cle","title":"Acc\u00e8s du service account \u00e0 la cl\u00e9","text":"<p>Il nous reste \u00e0 autoriser notre service account 'k8s-vault@papafrancky.iam.gserviceaccount.com' \u00e0 acc\u00e9der \u00e0 la cl\u00e9 que nous venons de cr\u00e9er.</p> KEY VALUE Principal k8s-vault@papafrancky.iam.gserviceaccount.com Role Cloud KMS Viewer Role Cloud KMS CryptoKey Encrypter/Decrypter"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/vault_auto-unsealed_external-secrets-operator_fluxcd/#acces-du-service-account-a-la-cle-via-la-console-web-ui","title":"Acc\u00e8s du service account \u00e0 la cl\u00e9 via la console (Web UI)","text":"<p>Tip</p> <p>Security &gt; Key Management &gt; k8s-vault (key ring) &gt; k8s-vault (key) &gt; PERMISSIONS &gt; + GRANT ACCESS</p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p>"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/vault_auto-unsealed_external-secrets-operator_fluxcd/#acces-du-service-account-a-la-cle-via-gcloud-cli","title":"Acc\u00e8s du service account \u00e0 la cl\u00e9 via gcloud (CLI)","text":"<p>Doc</p> <p>https://cloud.google.com/kms/docs/iam#granting_permissions_to_use_keys</p> <p>Donnons les acc\u00e8s souhait\u00e9s \u00e0 notre service account 'k8s-vault' sur la cl\u00e9 KMS :</p> <pre><code>gcloud kms keys add-iam-policy-binding k8s-vault \\\n  --keyring k8s-vault \\\n  --location europe-west9 \\\n  --member serviceAccount:k8s-vault@papafrancky.iam.gserviceaccount.com \\\n  --role roles/cloudkms.viewer\n\ngcloud kms keys add-iam-policy-binding k8s-vault \\\n  --keyring k8s-vault \\\n  --location europe-west9 \\\n  --member serviceAccount:k8s-vault@papafrancky.iam.gserviceaccount.com \\\n  --role roles/cloudkms.cryptoKeyEncrypterDecrypter\n</code></pre> <p>V\u00e9rifions l'op\u00e9ration :</p> codeoutput <pre><code>gcloud kms keys get-iam-policy k8s-vault \\\n  --keyring k8s-vault \\\n  --location europe-west9\n</code></pre> <pre><code>bindings:\n- members:\n  - serviceAccount:k8s-vault@papafrancky.iam.gserviceaccount.com\n  role: roles/cloudkms.cryptoKeyEncrypterDecrypter\n- members:\n  - serviceAccount:k8s-vault@papafrancky.iam.gserviceaccount.com\n  role: roles/cloudkms.viewer\netag: BwZCnYs5aTQ=\nversion: 1\n</code></pre> <p>Nous en avons fini avec les pr\u00e9paratifs c\u00f4t\u00e9 GCP </p>"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/vault_auto-unsealed_external-secrets-operator_fluxcd/#mise-en-place-de-vault-en-mode-auto-unseal","title":"Mise en place de Vault en mode 'auto-unseal'","text":"<p>Nous couvrirons dans cette section l'installation de Vault, son initialisation et son unsealing.</p>"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/vault_auto-unsealed_external-secrets-operator_fluxcd/#depot-github","title":"D\u00e9p\u00f4t GitHub","text":"<p>La d\u00e9finition de la 'HelmRelease' ainsi que ses 'custom values' sera h\u00e9berg\u00e9e dans un sous-r\u00e9pertoire d\u00e9ddi\u00e9 \u00e0 Vault, dans le d\u00e9p\u00f4t GitHub des applications g\u00e9r\u00e9es par FluxCD :</p> <pre><code>export LOCAL_GITHUB_REPOS=\"${HOME}/code/github\"\ncd ${LOCAL_GITHUB_REPOS}/k8s-kind-apps\nmkdir vault\n</code></pre>"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/vault_auto-unsealed_external-secrets-operator_fluxcd/#custom-values","title":"'Custom values'","text":"<p>Les 'Helm Charts' viennent toujours avec des 'default values' et Vault ne fait pas exception \u00e0 la r\u00e8gle.</p> <p>Pour consulter les 'custom values' de Vault :</p> <pre><code>helm show values hashicorp/vault\n</code></pre> <p>Nous souhaitons d\u00e9ployer sur notre cluster Kubernetes Vault en mode standalone et en 'auto-unseal', ce qui ne correspond pas au param\u00e9trage propos\u00e9 par les 'default values'.</p> <p>Sur la base de ces derni\u00e8res, nous allons \u00e9crire les 'custom values' de notre instance Vault pour arriver \u00e0 nos fins. Nous stockerons nos 'custom values' dans un 'ConfigMap' :</p> <p>Warning</p> <p>Nous d\u00e9ploierons ici Vault en mode 'standalone', ce qui ne se pr\u00eate pas \u00e0 un contexte de production.</p> <p>Notre pod Vault aura besoin de la cl\u00e9 priv\u00e9e du service-account GCP 'k8s-vault' pour acc\u00e9der \u00e0 la cl\u00e9 KMS qui d\u00e9bloquera Vault ('unsealing'). Cette cl\u00e9 priv\u00e9e est r\u00e9cup\u00e9r\u00e9e \u00e0 partir du secret Kubernetes 'k8s-vault-gcp-service-account-key' qui sera mont\u00e9 dans le r\u00e9pertoire '/vault/userconfig' du pod.</p> <pre><code>export LOCAL_GITHUB_REPOS=\"${HOME}/code/github\"\ncd ${LOCAL_GITHUB_REPOS}/k8s-kind-apps\nmkdir vault\n# Ecriture des 'custom values' du HelmChart vault dans un fichier 'values.yaml' :\ncat &lt;&lt; EOF &gt; ./vault/values.yaml\nglobal:\n  enabled: false\n  namespace: \"vault\"\ninjector:\n  enabled: false\nserver:\n  enabled: true\n  extraEnvironmentVars:\n    GOOGLE_REGION: europe-west9\n    GOOGLE_PROJECT: papafrancky\n    GOOGLE_APPLICATION_CREDENTIALS: /vault/userconfig/vault-sa-key/vault-sa-key\n  extraVolumes:\n    - type: secret\n      name: vault-sa-key\n      path: /vault/userconfig\n  standalone:\n    enabled: true\n    config: |\n      ui = true\n\n      listener \"tcp\" {\n        tls_disable = 1\n        address = \"[::]:8200\"\n        cluster_address = \"[::]:8201\"\n        # Enable unauthenticated metrics access (necessary for Prometheus Operator)\n        #telemetry {\n        #  unauthenticated_metrics_access = \"true\"\n        #}\n      }\n      storage \"file\" {\n        path = \"/vault/data\"\n      }\n\n      # Example configuration for using auto-unseal, using Google Cloud KMS. The\n      # GKMS keys must already exist, and the cluster must have a service account\n      # that is authorized to access GCP KMS.\n      seal \"gcpckms\" {\n        project     = \"papafrancky\"\n        region      = \"euope-west9\"\n        key_ring    = \"k8s-vault\"\n        crypto_key  = \"k8s-vault\"\n      }\n  serviceAccount:\n    create: true\n    name: \"vault\"\nui:\n  enabled: true\nEOF\n\n# Cr\u00e9ation du ConfigMap :\nkubectl -n vault create configmap vault-values \\\n  --from-file=./vault/values.yaml \\\n  --dry-run=client \\\n  -o yaml &gt; ./vault/vault-values.configmap.yaml\n\n# Suppression du fichier values.yaml :\n/bin/rm ./vault/values.yaml\n</code></pre>"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/vault_auto-unsealed_external-secrets-operator_fluxcd/#helm-release-vault","title":"Helm release Vault","text":""},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/vault_auto-unsealed_external-secrets-operator_fluxcd/#le-gitrepository-dedie-a-vault","title":"Le GitRepository d\u00e9di\u00e9 \u00e0 Vault","text":""},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/vault_auto-unsealed_external-secrets-operator_fluxcd/#creation-et-installation-de-la-deploy-key_1","title":"Cr\u00e9ation et installation de la 'Deploy Key'","text":"<p>La 'deploy key' va permettre \u00e0 FluxCD de s'authentifier aupr\u00e8s du d\u00e9p\u00f4t GitHub d\u00e9di\u00e9 \u00e0 nos applications. Nous avons pris le partir de fournir une 'deploy key' par application.</p> codeoutput <pre><code>export GITHUB_USERNAME=papafrancky\n\nflux create secret git k8s-kind-apps-gitrepository-deploykeys \\\n  --url=ssh://github.com/${GITHUB_USERNAME}/k8s-kind-apps \\\n  --namespace=vault\n\nkubectl -n vault get secret k8s-kind-apps-gitrepository-deploykeys -o yaml\n</code></pre> <pre><code>  apiVersion: v1\n  data:\n    identity:       LS0tLS1CRUdJTiBQUklWQVRFIEtFWS0tLS0tCk1JRzJBZ0VBTUJBR0J5cUdTTTQ5QWdFR0JTdUJCQUFpQklHZU1JR2JBZ0VCQkRCZ     kIrbnV3VUVuOWxKc0NQMTIKR0NoZzIrYVJ5c2dKem95b2xVMEJZT09KL29kRjJIdFRMd0k1a1FXb2lGYVBld3FoWkFOaUFBVHZTal     ZrNWF0eQpMSDhWUVhVV0pCdlFBbXJYVFd2K1RCQVRoMGZwcmsxbmJxS1hlc1pab0d4eGFJWElYMEJiZi9FZXdMTWxNRWUrClVqRVV   FSE4veGF4b1E3ZnAwV2NkeGtnV1loVkF4ZU15Z09EQXU3TzhBK1l6K3RVT0h5U1NKSTA9Ci0tLS0tRU5EIFBSSVZBVEUgS0VZLS0t      LS0K\n    identity.pub:       ZWNkc2Etc2hhMi1uaXN0cDM4NCBBQUFBRTJWalpITmhMWE5vWVRJdGJtbHpkSEF6T0RRQUFBQUlibWx6ZEhBek9EUUFBQUJoQk85S     05XVGxxM0lzZnhWQmRSWWtHOUFDYXRkTmEvNU1FQk9IUittdVRXZHVvcGQ2eGxtZ2JIRm9oY2hmUUZ0LzhSN0FzeVV3Ujc1U01SUV     FjMy9GckdoRHQrblJaeDNHU0JaaUZVREY0ektBNE1DN3M3d0Q1alA2MVE0ZkpKSWtqUT09Cg==\n    known_hosts:      Z2l0aHViLmNvbSBlY2RzYS1zaGEyLW5pc3RwMjU2IEFBQUFFMlZqWkhOaExYTm9ZVEl0Ym1semRIQXlOVFlBQUFBSWJtbHpkSEF5T     lRZQUFBQkJCRW1LU0VOalFFZXpPbXhrWk15N29wS2d3RkI5bmt0NVlScllNak51RzVOODd1UmdnNkNMcmJvNXdBZFQveTZ2MG1LVj   BVMncwV1oyWUIvKytUcG9ja2c9\n  kind: Secret\n  metadata:\n    creationTimestamp: \"2025-11-02T17:40:12Z\"\n    name: k8s-kind-apps-gitrepository-deploykeys\n    namespace: vault\n    resourceVersion: \"829103\"\n    uid: b213f750-58a1-40af-afb7-53eb765a2985\n  type: Opaque\n</code></pre> <p>Nous devons d\u00e9ployer la Deploy Key sur notre d\u00e9p\u00f4t GitHub 'k8s-kind-apps' comme nous l'avons fait pr\u00e9c\u00e9demment pour 'external secrets operator'.</p> <p>Commen\u00e7ons par r\u00e9cup\u00e9rer notre cl\u00e9 publique depuis le secret que nous venons de cr\u00e9er : </p> codeoutput <pre><code>kubectl -n vault get secret k8s-kind-apps-gitrepository-deploykeys -o jsonpath='{.data.identity\\.pub}' | base64 -D\n</code></pre> <pre><code>ecdsa-sha2-nistp384 AAAAE2VjZHNhLXNoYTItbmlzdHAzODQAAAAIbmlzdHAzODQAAABhBO9KNWTlq3IsfxVBdRYkG9ACatdNa/5MEBOHR+muTWduopd6xlmgbHFohchfQFt/8R7AsyUwR75SMRQQc3/FrGhDt+nRZx3GSBZiFUDF4zKA4MC7s7wD5jP61Q4fJJIkjQ==\n</code></pre> <p>Int\u00e9grons-la enfin sur le d\u00e9p\u00f4t GitHub d\u00e9di\u00e9 \u00e0 nos applications :</p> <p></p> <p></p> <p></p>"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/vault_auto-unsealed_external-secrets-operator_fluxcd/#le-gitrepository-dedie-aux-applications_1","title":"Le GitRepository d\u00e9di\u00e9 aux applications","text":"<p>Nous avons d\u00e9fini la 'Deploy Key', nous pouvons d\u00e9sormais d\u00e9finir le d\u00e9p\u00f4t Git :</p> code'k8s-kind-apps' GitRepository <pre><code>export LOCAL_GITHUB_REPOS=\"${HOME}/code/github\"\nexport GITHUB_USERNAME=papafrancky\n\ncd ${LOCAL_GITHUB_REPOS}/k8s-kind-fluxcd\n\nflux create source git k8s-kind-apps \\\n  --url=ssh://git@github.com/${GITHUB_USERNAME}/k8s-kind-apps.git \\\n  --branch=main \\\n  --secret-ref=k8s-kind-apps-gitrepository-deploykeys \\\n  --namespace=vault \\\n  --export &gt; apps/vault/k8s-kind-apps.gitrepository.yaml\n</code></pre> <pre><code>---\napiVersion: source.toolkit.fluxcd.io/v1\nkind: GitRepository\nmetadata:\n  name: k8s-kind-apps\n  namespace: external-secrets\nspec:\n  interval: 1m0s\n  ref:\n    branch: main\n  secretRef:\n    name: k8s-kind-apps-gitrepository-deploykeys\n  url: ssh://git@github.com/papafrancky/k8s-kind-apps.git\n</code></pre> <p>For\u00e7ons FluxCD \u00e0 cr\u00e9er ce nouvel objet :</p> <pre><code>export LOCAL_GITHUB_REPOS=\"${HOME}/code/github\"\ncd ${LOCAL_GITHUB_REPOS}/k8s-kind-fluxcd\ngit add .\ngit commit -m 'Defined k8s-kind-apps GitRepository in namespace vault.'\ngit push\n\nflux reconcile kustomization flux-system --with-source\n</code></pre>"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/vault_auto-unsealed_external-secrets-operator_fluxcd/#la-kustomization-pour-vault","title":"La Kustomization pour Vault","text":"<p>Nous devons dire \u00e0 Flux qu'il doit g\u00e9rer le sous-r\u00e9pertoire 'vault' dans le d\u00e9p\u00f4t Git d\u00e9di\u00e9 \u00e0 nos applications :</p> code'vault' kustomization <pre><code>export LOCAL_GITHUB_REPOS=\"${HOME}/code/github\"\ncd ${LOCAL_GITHUB_REPOS}/k8s-kind-fluxcd\n\n# D\u00e9finition de la kustomization Vault :\nflux create kustomization vault \\\n  --source=GitRepository/k8s-kind-apps.vault \\\n  --path=./vault \\\n  --prune=true \\\n  --namespace=vault \\\n  --export  &gt; apps/vault/vault.kustomization.yaml\n</code></pre> <pre><code>---\napiVersion: kustomize.toolkit.fluxcd.io/v1\nkind: Kustomization\nmetadata:\n  name: vault\n  namespace: vault\nspec:\n  interval: 1m0s\n  path: ./vault\n  prune: true\n  sourceRef:\n    kind: GitRepository\n    name: k8s-kind-apps\n    namespace: vault\n</code></pre> <p>Poussons nos modifications sur GitHub pour une prise en compte par FluxCD :</p> <pre><code>   export LOCAL_GITHUB_REPOS=\"${HOME}/code/github\"\n\n   cd ${LOCAL_GITHUB_REPOS}/k8s-kind-fluxcd\n\n   git add .\n   git commit -m 'Defined kustomization for Vault application.'\n   git push\n\n   flux reconcile kustomization flux-system --with-source\n</code></pre> <p>Discord nous remonte une erreur dans le channel vault : </p> <p></p> <p>Rien d'anormal, car nous n'avons \u00e9crit aucun manifest YAML dans le sous-r\u00e9pertoire 'vault' du d\u00e9p\u00f4t GitHub d\u00e9di\u00e9 \u00e0 nos applications 'k8s-kind-apps'. Dans Git, sans fichiers dans un r\u00e9pertoire, le r\u00e9pertoire n'existe pas. Le probl\u00e8me sera corrig\u00e9 lorsque nous aurons d\u00e9fini notre 'Helm Release' et ses 'custom values'.</p>"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/vault_auto-unsealed_external-secrets-operator_fluxcd/#la-helm-release-vault","title":"La Helm Release Vault","text":"<p>Nous d\u00e9finirons la HelmRelease de Vault et ses 'custom values' dans le d\u00e9p\u00f4t Git d\u00e9di\u00e9 \u00e0 nos applications, comme nous l'avons fait pr\u00e9c\u00e9demment pour 'external secrets operator' :</p>"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/vault_auto-unsealed_external-secrets-operator_fluxcd/#definition-de-la-release","title":"D\u00e9finition de la Release","text":"<p>Nous pouvons d\u00e9sormais d\u00e9finir notre 'helm release' pour que FluxCD puisse g\u00e9rer le d\u00e9ploiement de Vault :</p> code'vault' helm release <pre><code>export LOCAL_GITHUB_REPOS=\"${HOME}/code/github\"\n\ncd ${LOCAL_GITHUB_REPOS}/k8s-kind-apps\n\nflux create helmrelease vault \\\n  --source=HelmRepository/hashicorp \\\n  --chart=vault \\\n  --namespace=vault \\\n  --values-from=ConfigMap/vault-values \\\n  --export &gt; ./vault/vault.helm-release.yaml\n</code></pre> <pre><code>---\napiVersion: helm.toolkit.fluxcd.io/v2\nkind: HelmRelease\nmetadata:\n  name: vault\n  namespace: vault\nspec:\n  chart:\n    spec:\n      chart: vault\n      reconcileStrategy: ChartVersion\n      sourceRef:\n        kind: HelmRepository\n        name: hashicorp\n  interval: 1m0s\n  valuesFrom:\n  - kind: ConfigMap\n    name: vault-values\n</code></pre> <p>Poussons les modifications jusqu'\u00e0 FluxCD :</p> <pre><code>export LOCAL_GITHUB_REPOS=\"${HOME}/code/github\"\n\ncd ${LOCAL_GITHUB_REPOS}/k8s-kind-apps\n\ngit add .\ngit commit -m \"feat: vault helm release with custom values\"\ngit push\n\nflux reconcile kustomization fluxsystem --with-source\nflux -n vault reconcile kustomization vault --with-source\n</code></pre> <p>Discord nous annonce que la kustomization 'vault' est d\u00e9sormais bien fonctionnelle (notre dernier commit a cr\u00e9\u00e9 le sous-r\u00e9pertoire 'vault' attendu) et que Vault est bien d\u00e9ploy\u00e9 sur notre cluster :</p> <p></p> <p>Assurons-nous que la Helm release est bien d\u00e9ploy\u00e9e dans le namespace 'vault' :</p> codeoutput <pre><code>helm -n vault list\n</code></pre> <pre><code>NAME    NAMESPACE   REVISION    UPDATED                                 STATUS      CHART           APP VERSION\nvault   vault       1           2025-11-10 15:48:01.835257 +0000 UTC    deployed    vault-0.31.0    1.20.4\n</code></pre> <p>Regardons l'\u00e9tat de nos objets dans le namespace 'vault' :</p>  codeoutput <pre><code>kubectl -n vault get all\n</code></pre> <pre><code>NAME          READY   STATUS    RESTARTS        AGE\npod/vault-0   0/1     Running   6 (2m10s ago)   8m5s\n\nNAME                     TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)             AGE\nservice/vault            ClusterIP   10.43.76.26     &lt;none&gt;        8200/TCP,8201/TCP   8m5s\nservice/vault-internal   ClusterIP   None            &lt;none&gt;        8200/TCP,8201/TCP   8m5s\nservice/vault-ui         ClusterIP   10.43.131.120   &lt;none&gt;        8200/TCP            8m5s\n\nNAME                     READY   AGE\nstatefulset.apps/vault   0/1     8m5s\n</code></pre> <p>Nous voyons que le pod 'vault-0' \u00e0 un status 'Running' mais qu'il n'est pas 'ready'. V\u00e9rifions l'\u00e9tat de Vault sur le pod : </p> codeoutput <pre><code>kubectl -n vault exec -it vault-0 -- vault status\n</code></pre> <pre><code>Key                      Value\n---                      -----\nSeal Type                gcpckms\nRecovery Seal Type       n/a\nInitialized              false\nSealed                   true\nTotal Recovery Shares    0\nThreshold                0\nUnseal Progress          0/0\nUnseal Nonce             n/a\nVersion                  1.20.4\nBuild Date               2025-09-23T13:22:38Z\nStorage Type             file\nHA Enabled               false\ncommand terminated with exit code 2\n</code></pre> <p>Vault doit \u00eatre initialis\u00e9 !</p>"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/vault_auto-unsealed_external-secrets-operator_fluxcd/#initialisation-de-vault","title":"Initialisation de Vault","text":"<p>L'initialisation de Vault passe par une commande \u00e0 passer directement sur les pods (dans notre cas, nous n'en avons qu'un) :</p> codeoutput <pre><code>kubectl -n vault exec -it vault-0 -- vault operator init\n</code></pre> <pre><code>Recovery Key 1: eE4Os3Z4TSnVLjsROqGIyFdEiDVBTGvolywiY5PJ2tLh\nRecovery Key 2: SBLCPjboyMEdzhpPLwnC4g1FnMH5xG1Z8jHv/X/5fDIB\nRecovery Key 3: toG6YbkGeI6yRYMeeKVsKh+WocVFiqE9ZzFxRFpdw8QH\nRecovery Key 4: YVfC0LIPdZZ8OqC6vc3PlYsRjShbJRYLRaOq8bhBI823\nRecovery Key 5: GD2tpYHrtzBtPVzBHSo9ZeK8RM5/H6gxskUOoHBzSvAn\n\nInitial Root Token: hvs.CQwblgr767wFfJLVU5DgjIi8\n\nSuccess! Vault is initialized\n\nRecovery key initialized with 5 key shares and a key threshold of 3. Please\nsecurely distribute the key shares printed above.\n</code></pre> <p>Warning</p> <p>Le 'Root Token' ainsi que les 'Recovery Keys' doivent \u00eatre conserv\u00e9s, et dans un lieu s\u00fbr !</p> <p>V\u00e9rifions que Vault est bien op\u00e9rationnel :</p> codeoutput <pre><code>kubectl -n vault exec -it vault-0 -- vault status\n</code></pre> <pre><code>Key                      Value\n---                      -----\nSeal Type                gcpckms\nRecovery Seal Type       shamir\nInitialized              true\nSealed                   false\nTotal Recovery Shares    5\nThreshold                3\nVersion                  1.20.4\nBuild Date               2025-09-23T13:22:38Z\nStorage Type             file\nCluster Name             vault-cluster-632ed50e\nCluster ID               7b77d1d9-cb52-e83b-486f-5ce7811c6929\nHA Enabled               false\n</code></pre> <p>Vault est bien initialis\u00e9 (Sealed = false). Assurons-nous malgr\u00e9 tout que le pod est d\u00e9sormais bien 'ready' :</p>  codeoutput <pre><code>kubectl -n vault get pod vault-0\n</code></pre> <pre><code>NAME      READY   STATUS    RESTARTS   AGE\nvault-0   1/1     Running   0          65m\n</code></pre> <p>Connectons-nous \u00e0 la web UI de Vault. Pour ce faire, nous utiliserons le 'root token' r\u00e9cup\u00e9r\u00e9 lors de l'initialisation de Vault.</p> <p>Commen\u00e7ons par utiliser le 'port-forwarding' :</p> <pre><code>kubectl -n vault port-forward service/vault-ui 8200 8200\n</code></pre> <p>Ouvrons ensuite notre navigateur \u00e0 l'URL suivante : <code>http://localhost:8200/</code>. </p> <p>Nous acc\u00e9dons bien \u00e0 Vault via son interface graphique ! </p> <p>Tout est comme attendu ! </p>"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/vault_auto-unsealed_external-secrets-operator_fluxcd/#test-de-lauto-unseal","title":"Test de l'auto-unseal","text":"<p>Vault est install\u00e9 en 'statefulset', sa configuration est p\u00e9renne, aussi allons-nous le d\u00e9sinstaller et attendre que FluxCD le r\u00e9installe pour nous assurer que Vault sera r\u00e9install\u00e9 dans un \u00e9tat initialis\u00e9. Param\u00e9tr\u00e9 en mode 'auto-unseal', notre coffre devrait \u00eatre op\u00e9rationnel :</p> Param\u00e8tre Valeur Initialized true Sealed false codeoutput <pre><code>helm -n vault list\n</code></pre> <pre><code>NAME    NAMESPACE   REVISION    UPDATED                                 STATUS      CHART           APP VERSION\nvault   vault       1           2024-06-01 16:13:04.681229835 +0000 UTC deployed    vault-0.28.0    1.16.1\n</code></pre> <p>D\u00e9sinstallons Vault tout en sachant que FluxCD le r\u00e9installera ensuite :</p> codeoutput <pre><code>helm -n vault uninstall vault\nkubectl -n vault get all\n</code></pre> <pre><code>No resources found in vault namespace.\n</code></pre> <p>Discord nous pr\u00e9vient que FluxCD a red\u00e9ploy\u00e9 la Helm release :</p> <p></p> <p>La Helm Release a bien re-d\u00e9ploy\u00e9 tous les objects n\u00e9cessaires au bon fonctionnement de Vault en mode 'standalone' :</p> codeoutput <pre><code>kubectl -n vault get all\n</code></pre> <pre><code>NAME          READY   STATUS    RESTARTS      AGE\npod/vault-0   1/1     Running   8 (76m ago)   87m\n\nNAME                     TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)             AGE\nservice/vault            ClusterIP   10.43.175.203   &lt;none&gt;        8200/TCP,8201/TCP   26h\nservice/vault-internal   ClusterIP   None            &lt;none&gt;        8200/TCP,8201/TCP   26h\nservice/vault-ui         ClusterIP   10.43.103.17    &lt;none&gt;        8200/TCP            26h\n\nNAME                     READY   AGE\nstatefulset.apps/vault   1/1     26h\n</code></pre> <p>Regardons sur le pod nouvellement re-d\u00e9ploy\u00e9 l'\u00e9tat de Vault :</p> codeoutput <pre><code>kubectl -n vault exec -it vault-0 -- vault status\n</code></pre> <pre><code>Key                      Value\n---                      -----\nSeal Type                gcpckms\nRecovery Seal Type       shamir\nInitialized              true\nSealed                   false\nTotal Recovery Shares    5\nThreshold                3\nVersion                  1.20.4\nBuild Date               2025-09-23T13:22:38Z\nStorage Type             file\nCluster Name             vault-cluster-632ed50e\nCluster ID               7b77d1d9-cb52-e83b-486f-5ce7811c6929\nHA Enabled               false\n</code></pre> <p>Vault est bien 'unsealed'. Parce qu'il s'agit d'un 'statefulset', les donn\u00e9es sont persistantes sur le cluster et Vault se souvient malgr\u00e9 sa r\u00e9cente d\u00e9sinstallation qu'il avait \u00e9t\u00e9 initialis\u00e9 pr\u00e9c\u00e9demment, raison pour laquelle nous n'avons pas eu \u00e0 le refaire.</p> <p>Parce qu'il est initialis\u00e9 et param\u00e9tr\u00e9 correctement en mode 'auto-unseal', notre coffre est compl\u00e8tement op\u00e9rationnel juste apr\u00e8s sa r\u00e9-installation. </p> <p>Success</p> <p>Nous venons de valider le bon fonctionnement de l''auto-unsealing' de Vault. </p> <p>XXXXX</p> <p>podinfo</p> <p>=== code     <pre><code>kubectl -n podinfo get secret k8s-kind-apps-gitrepository-deploykeys -o jsonpath='{.data.identity}' | base64 -d\n</code></pre></p> output <pre><code>-----BEGIN PRIVATE KEY-----\nMIG2AgEAMBAGByqGSM49AgEGBSuBBAAiBIGeMIGbAgEBBDCsmDJSy9FKDpSxH94x\nwkFOcZlBWqwRN4pGC+mp0mxa8YQDlziKNQmcg4gT5B1a6TuhZANiAASm8PcjGzfE\njyCUPT/A2cBlO1iWbsCra3OCjNt2hyxQQkMsfKKeP7+tv3obYmmZ6x+OYPTixgsI\naufN0nPw64Apf3DyUmlmw1ZFxqMX8D2Buboa0tJHo2z11rZUgFGcDhI=\n-----END PRIVATE KEY-----\n</code></pre> <pre><code># Acc\u00e8s au pod du micro-service 'vault'\nkubectl -n vault exec -it vault-0 -- sh\n\n# Login sur Vault avec le Root token\nvault login hvs.CQwblgr767wFfJLVU5DgjIi8\n\n# Activation du 'secret engine' KVv2\nvault secrets enable -version=2 kv\n\n# Ecriture du secret \nvault kv put -mount kv podinfo/gitrepositories/k8s-kind-apps/deploykey \\\n  dentity=\"-----BEGIN PRIVATE KEY-----\n  MIG2AgEAMBAGByqGSM49AgEGBSuBBAAiBIGeMIGbAgEBBDCsmDJSy9FKDpSxH94x\n  wkFOcZlBWqwRN4pGC+mp0mxa8YQDlziKNQmcg4gT5B1a6TuhZANiAASm8PcjGzfE\n  jyCUPT/A2cBlO1iWbsCra3OCjNt2hyxQQkMsfKKeP7+tv3obYmmZ6x+OYPTixgsI\n  aufN0nPw64Apf3DyUmlmw1ZFxqMX8D2Buboa0tJHo2z11rZUgFGcDhI=\"\n</code></pre>"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/vault_auto-unsealed_external-secrets-operator_fluxcd/#integration-de-vault-et-external-secrets-a-la-helm-release-kube-prometheus-stack","title":"Int\u00e9gration de Vault et External-Secrets \u00e0 la Helm Release 'kube-prometheus-stack'","text":"<p>La stack de monitoring d\u00e9finit un mot de passe par d\u00e9faut pour le compte admin de Grafana. Et c'est moche.</p> <p>Pour corriger cela, nous nous proposons de d\u00e9finir un nouveau mot de passe pour ce compte et de le prot\u00e9ger dans Vault.</p> <p>Nous utiliserons l'op\u00e9rateur External Secrets synchroniser le mot de passe h\u00e9berg\u00e9 dans Vault avec une ConfigMap qui sera utilis\u00e9e par Flux pour d\u00e9finir les 'custom values' de la Helm Release 'kube-prometheus-stack'.</p> <p>Tout un programme. ^^</p>"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/vault_auto-unsealed_external-secrets-operator_fluxcd/#ajout-du-secret-dans-vault","title":"Ajout du 'secret' dans Vault","text":"<p>Connectons-nous au pod 'Vault-0' pour activer le 'secret engine' 'KVv2' et y h\u00e9berger le mot de passe du compte d'administration de Grafana :</p> <pre><code># Acc\u00e8s au pod du micro-service 'vault'\nkubectl -n vault exec -it vault-0 -- sh\n\n# Login sur Vault avec le Root token\nvault login hvs.CQwblgr767wFfJLVU5DgjIi8\n\n# Activation du 'secret engine' KVv2\nvault secrets enable -version=2 kv\n\n# Ecriture du secret \nvault kv put -mount kv monitoring/grafana/admin-account login=admin password=my-vaulted-custom-password\n\n\n# V\u00e9rification\nvault kv get -mount=kv monitoring/grafana/admin-account\n\n============== Secret Path ==============\nkv/data/monitoring/grafana/admin-account\n\n======= Metadata =======\nKey                Value\n---                -----\ncreated_time       2024-06-04T14:45:27.639679075Z\ncustom_metadata    &lt;nil&gt;\ndeletion_time      n/a\ndestroyed          false\nversion            2\n\n====== Data ======\nKey         Value\n---         -----\nlogin       admin\npassword    my-vaulted-custom-password\n\n# Deconnexion du pod \nexit\n</code></pre>"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/vault_auto-unsealed_external-secrets-operator_fluxcd/#definition-dune-policy-permettant-dacceder-en-lecture-aux-secrets-dedies-a-grafana","title":"D\u00e9finition d'une 'policy' permettant d'acc\u00e9der en lecture aux secrets d\u00e9di\u00e9s \u00e0 Grafana","text":"<p>Maintenant, \u00e9crivons une 'policy' nous permettant de r\u00e9cup\u00e9rer notre mot de passe :</p> <pre><code># Acc\u00e8s au pod du micro-service 'vault'\nkubectl -n vault exec -it vault-0 -- sh\n\n# Login sur Vault avec le Root token\nvault login hvs.VPcxxUbQjWt66U3jRzMjfIaI\n\n# Definition de la 'policy' donnant acc\u00e8s aux 'secrets' de Grafana en lecture\nvault policy write monitoring-grafana--ro - &lt;&lt; EOF     \npath \"kv/metadata/monitoring/grafana*\" {\n  capabilities = [\"list\",\"read\"]\n}\npath \"kv/data/monitoring/grafana*\" {\n  capabilities = [\"list\",\"read\"]\n}\n\npath \"kv/metadata/monitoring\" {\n  capabilities = [\"list\"]\n}\npath \"kv/data/monitoring\" {\n  capabilities = [\"list\"]\n}\n\npath \"kv/metadata\" {\n  capabilities = [\"list\"]\n}\n\npath \"kv/metadata*\" {\n  capabilities = [\"deny\"]\n}\npath \"kv/data*\" {\n  capabilities = [\"deny\"]\n}\nEOF\n\n# Deconnexion du pod \nexit\n</code></pre>"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/vault_auto-unsealed_external-secrets-operator_fluxcd/#authentification-kubernetes-sur-vault","title":"Authentification Kubernetes sur Vault","text":"<p>L'application Grafana doit pouvoir r\u00e9cup\u00e9rer le mot de passe h\u00e9berg\u00e9 dans Vault. Voici comment nous allons nous y prendre pour arriver \u00e0 nos fins :</p> <ul> <li>nous allons activer sur Vault l'authentification Kubernetes;</li> <li>nous attacherons au service-account avec lequel le pod Grafana sera ex\u00e9cut\u00e9  'ClusterRole' auth-delegator;</li> <li>enfin, il nous restera \u00e0 d\u00e9finit au niveau de Vault un r\u00f4le visant \u00e0 rattacher la policy cr\u00e9\u00e9e pr\u00e9c\u00e9demment \u00e0 notre service-account Kubernetes.</li> </ul> <p>Info</p> <p>https://developer.hashicorp.com/vault/docs/auth/kubernetes#kubernetes-auth-method</p> <p>Use local service account token as the reviewer JWT :</p> <p>When running Vault in a Kubernetes pod the recommended option is to use the pod's local service account token. Vault will periodically re-read the file to support short-lived tokens. To use the local token and CA certificate, omit token_reviewer_jwt and kubernetes_ca_cert when configuring the auth method. Vault will attempt to load them from token and ca.crt respectively inside the default mount folder /var/run/secrets/kubernetes.io/serviceaccount/.</p> <p>Each client of Vault would need the system:auth-delegator ClusterRole</p> <p>Commen\u00e7ons par activer et configurer l'authentification Kubernetes sur Vault :</p> <pre><code># Acc\u00e8s au pod du micro-service 'vault'\nkubectl -n vault exec -it vault-0 -- sh\n\n# Login sur Vault avec le Root token\nvault login hvs.VPcxxUbQjWt66U3jRzMjfIaI\n\n\n# Activation de l'authentification Kubernetes\nvault auth enable kubernetes\nvault auth list\n\n# Configuration de l'authentification Kubernetes \nvault write auth/kubernetes/config kubernetes_host=https://${KUBERNETES_SERVICE_HOST}:${KUBERNETES_SERVICE_PORT}\n\n# Deconnexion du pod \nexit\n</code></pre>"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/vault_auto-unsealed_external-secrets-operator_fluxcd/#etablissement-de-la-relation-entre-le-service-account-kubernetes-et-celui-de-vault","title":"Etablissement de la relation entre le service-account Kubernetes et celui de Vault","text":""},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/vault_auto-unsealed_external-secrets-operator_fluxcd/#service-account-kubernetes","title":"Service-account Kubernetes","text":"<p>Lors du d\u00e9ploiement de la Helm release 'kube-prometheus-stack', Grafana devra \u00eatre en mesure de r\u00e9cup\u00e9rer ses custom values dans un Secret Kubernetes. Ce dernier doit \u00eatre g\u00e9n\u00e9r\u00e9 en amont par l'op\u00e9rateur 'External Secrets' \u00e0 partir d'un template sous forme de ConfigMap indiquant le besoin de r\u00e9cup\u00e9rer le mot de passe du compte d'administration depuis Vault.</p> <p>Le Secret Kubernetes doit donc \u00eatre pr\u00eat avant le d\u00e9ploiement de la Helm release. Or le service-account qui sera utilis\u00e9 par Grafana ne sera cr\u00e9\u00e9 que lors de son d\u00e9ploiement. Il n'est donc pas envisageable de l'utiliser pour s'authentifier \u00e0 Vault et r\u00e9cup\u00e9rer le secret recherch\u00e9.</p> <p>Nous devons donc cr\u00e9er un service-account d\u00e9di\u00e9, que nous nommerons 'eso-grafana'.</p> code'eso-grafana' service-account <pre><code>export LOCAL_GITHUB_REPOS=\"${HOME}/code/github\"\n\nkubectl -n monitoring create serviceaccount eso-grafana --dry-run=client -o yaml | grep -v creationTimestamp &gt; ${LOCAL_GITHUB_REPOS}/k8s-kind-fluxcd/apps/monitoring/eso-grafana.serviceaccount.yaml\n</code></pre> <pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: eso-grafana\n  namespace: monitoring\n</code></pre> <p>Cr\u00e9ons le service-account car nous en aurons besoin pour les tests un peu plus loin :</p> <pre><code>export LOCAL_GITHUB_REPOS=\"${HOME}/code/github\"\n\ncd ${LOCAL_GITHUB_REPOS}/k8s-kind-fluxcd\n\ngit add .\ngit commit -m \"feat: setting up eso-grafana service-account.\"\ngit push\n\nflux reconcile kustomization flux-system --with-source\n</code></pre>"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/vault_auto-unsealed_external-secrets-operator_fluxcd/#clusterrolebinding","title":"ClusterRoleBinding","text":"<p>Nous allons donner \u00e0 notre nouveau service-account Kubernetes le droit de d\u00e9l\u00e9guer son authentification en le rattachant au ClusterRole 'system:auth-delegator'.</p> <p>Info</p> <p>https://kubernetes.io/docs/reference/access-authn-authz/rbac/#other-component-roles</p> <p>\"system:auth-delegator allows delegated authentication and authorization checks. This is commonly used by add-on API servers for unified authentication and authorization.\"</p> codeoutput <pre><code>kubectl create clusterrolebinding eso-grafana-tokenreview-access \\\n  --clusterrole=system:auth-delegator \\\n  --serviceaccount=monitoring:eso-grafana\n</code></pre> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  creationTimestamp: \"2024-07-06T11:06:53Z\"\n  name: eso-grafana-tokenreview-access\n  resourceVersion: \"2860551\"\n  uid: e74382bd-dd09-4b2e-a896-a1d3fd578a24\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: system:auth-delegator\nsubjects:\n- kind: ServiceAccount\n  name: eso-grafana\n  namespace: monitoring\n</code></pre>"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/vault_auto-unsealed_external-secrets-operator_fluxcd/#rattachement-de-la-policy-vault-au-service-account-kubernetes","title":"Rattachement de la policy Vault au service-account Kubernetes","text":"<p>Pour ce faire, nous allons d\u00e9finir un r\u00f4le au niveau de l'authentification Kubernetes de Vault.</p> <pre><code># Acc\u00e8s au pod du micro-service 'vault'\nkubectl -n vault exec -it vault-0 -- sh\n\n# Login sur Vault avec le Root token\nvault login hvs.VPcxxUbQjWt66U3jRzMjfIaI\n\n# Role autorisant le service-account Kubernetes \u00e0 lire les secrets de Grafana\nvault write auth/kubernetes/role/monitoring-grafana--ro \\\n  bound_service_account_names=eso-grafana \\\n  bound_service_account_namespaces=monitoring \\\n  policies=monitoring-grafana--ro \\\n  ttl=1h\n\n# V\u00e9rification\nvault read auth/kubernetes/role/monitoring-grafana--ro\n\n  # Key                                         Value\n  # ---                                         -----\n  # alias_name_source                           serviceaccount_uid\n  # bound_service_account_names                 [eso-grafana]\n  # bound_service_account_namespace_selector    n/a\n  # bound_service_account_namespaces            [monitoring]\n  # policies                                    [monitoring-grafana--ro]\n  # token_bound_cidrs                           []\n  # token_explicit_max_ttl                      0s\n  # token_max_ttl                               0s\n  # token_no_default_policy                     false\n  # token_num_uses                              0\n  # token_period                                0s\n  # token_policies                              [monitoring-grafana--ro]\n  # token_ttl                                   1h\n  # token_type                                  def\n\n# Deconnexion du pod \nexit\n</code></pre>"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/vault_auto-unsealed_external-secrets-operator_fluxcd/#test-de-lacces-du-service-account-kubernetes-au-secret-vault","title":"Test de l'acc\u00e8s du service-account Kubernetes au secret Vault","text":"<p>Pour tester que le service-account Kubernetes 'eso-grafana' du namespace 'monitoring' acc\u00e8de bien au secret de Grafana dans Vault, nous allons d\u00e9ployer un pod temporaire qui s'ex\u00e9cutera avec ce service-account.</p> <p>Voici ce que nous cherchons \u00e0 v\u00e9rifier :</p> <ol> <li>Le pod est ex\u00e9cut\u00e9 avec un service-account Kubernetes auquel est rattach\u00e9 le ClusterRole 'system:auth-delegator';</li> <li>L'application ex\u00e9cut\u00e9e dans le pod s'authentifie \u00e0 Vault (authentification Kubernetes) en utilisant le token de son service-account Kubernetes et rattach\u00e9 le r\u00f4le Vault 'monitoring-grafana--ro' ;</li> <li>Ce r\u00f4le Vault autorise pr\u00e9cis\u00e9ment ce service-account Kubernetes d'utiliser la policy Vault qui donne acc\u00e8s en lecture aux login et mot de passe du compte d'administration de Grafana;</li> <li>Vault valide le token du service-account Kubernetes aupr\u00e8s de Kubernetes et renvoie \u00e0 l'application du pod un token d'authentification \u00e0 Vault, auquel est rattach\u00e9 la policy d'acc\u00e8s aux credentials d'admin de Grafana;</li> <li>L'application peut d\u00e9sormais de loguer \u00e0 Vault avec le token ainsi r\u00e9cup\u00e9r\u00e9 et acc\u00e9der ensuite au compte d'administration de Grafana.</li> </ol>"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/vault_auto-unsealed_external-secrets-operator_fluxcd/#test-en-interrogeant-directement-lapi","title":"Test en interrogeant directement l'API","text":"<pre><code># Lancement d'un pod Alpine avec le service-account 'monitoring:kube-prometheus-stack-grafana'\nkubectl -n monitoring run --tty --stdin test --image=alpine --rm --overrides='{ \"spec\": { \"serviceAccount\": \"eso-grafana\" }  }' -- /bin/sh\n\n# Installation de cURL\napk update &amp;&amp; apk add curl jq\n\n# R\u00e9cup\u00e9ration du service-token JWT\nSA_JWT_TOKEN=$( cat /var/run/secrets/kubernetes.io/serviceaccount/token )\n    # -&gt; Pour regarder son contenu : https://jwt.io/ website.\n\n# Authentification sur Vault et r\u00e9cup\u00e9ration du token de session\nCLIENT_TOKEN=$( curl --silent --request POST --data '{\"jwt\": \"'\"${SA_JWT_TOKEN}\"'\", \"role\": \"monitoring-grafana--ro\"}' http://vault.vault:8200/v1/auth/kubernetes/login | jq -r .auth.client_token )\n\n# R\u00e9cup\u00e9ration du mot de passe du compte admin de Grafana\ncurl --silent --header \"X-Vault-Token:${CLIENT_TOKEN}\"  http://vault.vault:8200/v1/kv/data/monitoring/grafana/admin-account | jq .data.data\n\n# {\n#   \"login\": \"admin\",\n#   \"password\": \"my-vaulted-custom-password\"\n# }\n</code></pre>"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/vault_auto-unsealed_external-secrets-operator_fluxcd/#test-avec-la-cli-vault","title":"Test avec la CLI 'vault'","text":"<pre><code># Lancement d'un pod Alpine avec le service-account 'monitoring:kube-prometheus-stack-grafana'\nkubectl -n monitoring run --tty --stdin fedora --image=fedora --rm --overrides='{ \"spec\": { \"serviceAccount\": \"eso-grafana\" }  }' -- /bin/bash\n\n# Installation de Vault : \ndnf install -y dnf-plugins-core\ndnf config-manager --add-repo https://rpm.releases.hashicorp.com/fedora/hashicorp.repo\ndnf -y install vault jq\n\n# Pour une raison que j'ignore, la CLI 'vault' ne fonctionne pas apr\u00e8s installation, mais une r\u00e9installation semble r\u00e9gler le probl\u00e8me :\nrpm -e vault &amp;&amp; dnf -y install vault\n\n# Test d'acc\u00e8s aux secrets de Grafana\nexport VAULT_ADDR=\"http://vault.vault:8200\"\nSA_TOKEN=$( cat /var/run/secrets/kubernetes.io/serviceaccount/token )\nVAULT_TOKEN=$( vault write auth/kubernetes/login role=monitoring-grafana--ro jwt=${SA_TOKEN} | grep -w ^token | awk '{print $2}' )\n\n\nvault login ${VAULT_TOKEN}\n\n  # Success! You are now authenticated. The token information displayed below\n  # is already stored in the token helper. You do NOT need to run \"vault login\"\n  # again. Future Vault requests will automatically use this token.\n  # \n  # Key                                       Value\n  # ---                                       -----\n  # token                                     hvs.CAESIKPEIR-x0Sx8oK2Yc5wICr13blMQSHWmS6SdTCt5jCExGh4KHGh2cy5zWHByV2ZkY2hHb2Q2VjY2YzBBcVk3QWE\n  # token_accessor                            sTrkp8An57VCbuBWy1fDaHnd\n  # token_duration                            59m49s\n  # token_renewable                           true\n  # token_policies                            [\"default\" \"monitoring-grafana--ro\"]\n  # identity_policies                         []\n  # policies                                  [\"default\" \"monitoring-grafana--ro\"]\n  # token_meta_service_account_namespace      monitoring\n  # token_meta_service_account_secret_name    n/a\n  # token_meta_service_account_uid            665cb92c-90ba-4ad8-9313-9cae30e72203\n  # token_meta_role                           monitoring-grafana--ro\n  # token_meta_service_account_name           eso-grafana\n\n\nvault kv list -mount=kv monitoring/grafana\n\n# Keys\n# ----\n# admin-account\n\n\nvault kv get -mount=kv monitoring/grafana/admin-account\n\n  # ============== Secret Path ==============\n  # kv/data/monitoring/grafana/admin-account\n  # \n  # ======= Metadata =======\n  # Key                Value\n  # ---                -----\n  # created_time       2024-06-08T15:52:46.958211047Z\n  # custom_metadata    &lt;nil&gt;\n  # deletion_time      n/a\n  # destroyed          false\n  # version            1\n  # \n  # ====== Data ======\n  # Key         Value\n  # ---         -----\n  # login       admin\n  # password    my-vaulted-custom-password\n\n\nvault kv get -mount=kv -field=password monitoring/grafana/admin-account\n\n  # my-vaulted-custom-password\n</code></pre> <p>Success</p> <p>Notre pod, par le biais du service-account avec lequel il est ex\u00e9cut\u00e9, r\u00e9cup\u00e8re comme attendu le secret dans Vault!  </p>"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/vault_auto-unsealed_external-secrets-operator_fluxcd/#configuration-dexternal-secrets-operator-eso","title":"Configuration d'External Secrets Operator (ESO)","text":"<p>Cet op\u00e9rateur a pour r\u00f4le de synchroniser des objets Kubernetes de type Secret ou ConfigMap avec des 'secrets' stock\u00e9s dans un Secrets Manager (dans notre cas, HashiCorp Vault).</p>"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/vault_auto-unsealed_external-secrets-operator_fluxcd/#definition-du-secret-store","title":"Definition du Secret Store","text":"<p>Le 'SecretStore' est un objet qui d\u00e9finit dans notre cas de figure l'adresse de Vault, le 'secret engine' \u00e0 utiliser (en renseignant son 'path' et dans le cas de KV, la version du moteur), la mani\u00e8re de s'y authentifier (ici, on choisit l'authentification Kubernetes), avec quel service-account Kubernetes et quel r\u00f4le demander.</p> <pre><code>export LOCAL_GITHUB_REPOS=\"${HOME}/code/github\"\n\n# D\u00e9finition du SecretStore 'grafana' :\ncat &lt;&lt; EOF &gt; ${LOCAL_GITHUB_REPOS}/k8s-kind-fluxcd/apps/monitoring/grafana.secretstore.yaml\napiVersion: external-secrets.io/v1beta1\nkind: SecretStore\nmetadata:\n  name: grafana\n  namespace: monitoring\nspec:\n  provider:\n    vault:\n      server: \"http://vault.vault:8200\"\n      path: \"kv\"\n      version: \"v2\"\n      auth:\n        kubernetes:\n          mountPath: \"kubernetes\"\n          role: \"monitoring-grafana--ro\"\n          serviceAccountRef:\n            name: \"eso-grafana\"\nEOF\n</code></pre>"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/vault_auto-unsealed_external-secrets-operator_fluxcd/#definition-de-lexternal-secret","title":"D\u00e9finition de l'External Secret","text":"<p>Une fois le 'SecretStore' d\u00e9fini, nous pouvons nous int\u00e9resser aux 'External Secrets' : il s'agit cette fois de pr\u00e9ciser quel(s) secret(s) nous souhaitons r\u00e9cup\u00e9rer depuis SecretStore donn\u00e9 :</p> <pre><code>export LOCAL_GITHUB_REPOS=\"${HOME}/code/github\"\n\ncat &lt;&lt; EOF &gt; ${LOCAL_GITHUB_REPOS}/k8s-kind-fluxcd/apps/monitoring/grafana.externalsecret.yaml\napiVersion: external-secrets.io/v1beta1\nkind: ExternalSecret\nmetadata:\n  name: grafana-secrets\n  namespace: monitoring\nspec:\n  refreshInterval: \"15s\"\n  secretStoreRef:\n    name: grafana\n    kind: SecretStore\n  target:\n    name: admin-password\n  data:\n  - secretKey: admin_password\n    remoteRef:\n      key: kv/monitoring/grafana/admin-account\n      property: password\nEOF\n</code></pre>"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/vault_auto-unsealed_external-secrets-operator_fluxcd/#autorisation-dacces-dans-vault-a-ce-service-account-aux-secrets-de-grafana","title":"Autorisation d'acc\u00e8s dans Vault \u00e0 ce service-account aux secrets de Grafana","text":"<p>Bien \u00e9videmment, notre nouveau service-account doit pouvoir acc\u00e9der aux secrets de Grafana contenus dans Vault. Nous devons adapter notre r\u00f4le en cons\u00e9quence :</p> <pre><code># Login sur le pod Vault :\nkubectl -n vault exec -it vault-0  -- sh\n\n# Ouverture d'une session Vault avec le *root token*\nvault login hvs.VPcxxUbQjWt66U3jRzMjfIaI\n\n# Autorisation de lecture des secrets Grafana aux service-accounts 'kube-prometheus-grafana' et 'init-grafana' :\nvault write auth/kubernetes/role/monitoring-grafana--ro \\\n  bound_service_account_names=eso-grafana \\\n  bound_service_account_namespaces=monitoring \\\n  policies=monitoring-grafana--ro \\\n  ttl=1h\n\n# Fin de session sur le pod :\nexit\n</code></pre>"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/vault_auto-unsealed_external-secrets-operator_fluxcd/#prise-en-compte-des-modifications","title":"Prise en compte des modifications","text":"<pre><code>export LOCAL_GITHUB_REPOS=\"${HOME}/code/github\"\n\ncd ${LOCAL_GITHUB_REPOS}/k8s-kind-fluxcd\n\ngit add .\ngit commit -m \"feat: setting up grafana secretstore and external-secret.\"\ngit push\n\nflux reconcile kustomization flux-system --with-source\n</code></pre> <p>V\u00e9rifions la bonne cr\u00e9ation des nouveaux objets ESO :</p> codeoutput <pre><code>kubectl -n monitoring get secretstore,externalsecret\n</code></pre> <pre><code>NAME                                      AGE     STATUS   CAPABILITIES   READY\nsecretstore.external-secrets.io/grafana   3d15h   Valid    ReadWrite      True\n\nNAME                                                                                    STORE     REFRESH INTERVAL   STATUS         READY\nexternalsecret.external-secrets.io/grafana-secrets                                      grafana   15s                SecretSynced   True\nexternalsecret.external-secrets.io/kube-prometheus-stack-custom-values-externalsecret   grafana   1h                 SecretSynced   True\n</code></pre>"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/vault_auto-unsealed_external-secrets-operator_fluxcd/#recuperation-de-lexternal-secret-depuis-un-pod-de-test","title":"R\u00e9cup\u00e9ration de l'External Secret depuis un pod de test","text":"<pre><code># Cr\u00e9ation d'un pod Alpine excut\u00e9 avec le service-account d\u00e9di\u00e9 \u00e0 l'application Grafana\n# et affichant le mot de passe du compte d'administration :\ncat &lt;&lt; EOF | kubectl apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  labels:\n    run: test\n  name: test\n  namespace: monitoring\nspec:\n  containers:\n  - name: test\n    image: alpine\n    command: [\"printenv\"]\n    args: [\"ADMIN_PASSWORD\"]\n    env:\n    - name: ADMIN_PASSWORD\n      valueFrom:\n        secretKeyRef:\n          name: admin-password\n          key: admin_password\n  restartPolicy: Never\n  serviceAccount: eso-grafana\nEOF\n</code></pre> <p>Le pod pase \u00e0 l'\u00e9tat 'Completed'. Consultons ses logs :</p> codeoutput <pre><code>kubectl -n monitoring logs test\n</code></pre> <pre><code>my-vaulted-custom-password\n</code></pre> <p>Success</p> <p>Nous r\u00e9cup\u00e9rons comme attendu le mot de passe du compte d'administration de Grafana pr\u00e9sent dans Vault.  </p> <p>Supprimons le pod :</p> <pre><code>    kubectl -n monioring delete pod test\n</code></pre>"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/vault_auto-unsealed_external-secrets-operator_fluxcd/#integration-de-lexternal-secret-de-la-helm-release-kube-prometheus-stack","title":"Int\u00e9gration de l'external secret de la Helm Release 'kube-prometheus-stack'","text":"<p>Nous avan\u00e7ons \u00e0 petits pas, mais nous avan\u00e7ons!</p>"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/vault_auto-unsealed_external-secrets-operator_fluxcd/#deploiement-de-la-helm-release-kube-prometheus-stack","title":"D\u00e9ploiement de la Helm release 'kube-prometheus-stack'","text":"<p>Nous allons maintenant d\u00e9ployer la Helm Release 'kube-prometheus-stack' avec les valeurs par d\u00e9faut (pr\u00e9sentes dans le fichier 'values.yaml').</p> <p>Nous irons tr\u00e8s vite sur l'installation car nous l'avons d\u00e9j\u00e0 couverte dans le howto 'kube-prometheus-stack' managed with FluxCD.</p> <pre><code>export LOCAL_GITHUB_REPOS=\"${HOME}/code/github\"\n\n\n# R\u00e9pertoire qui contiendra tous les objets Kubernetes :\nmkdir -p ${LOCAL_GITHUB_REPOS}/k8s-kind-fluxcd/apps/monitoring\n\n\n# Namespace 'monitoring' :\nkubectl create namespace monitoring --dry-run=client -o yaml | grep -vE \"creationTimestamp|spec|status\" &gt; ${LOCAL_GITHUB_REPOS}/k8s-kind-fluxcd/apps/monitoring/namespace.yaml\n\n\n# Secret contenant le webhook du salon Discord :\nexport WEBHOOK_FOO=\"https://discord.com/api/webhooks/1242845059800633425/zyTYEpNZGf6vpd6C1sRLqeW_TGyFEMP2EM8BXAzockt20eeennkSHDKoO2-UxEG0K4ah\"\nkubectl -n monitoring create secret generic discord-webhook --from-literal=address=${WEBHOOK_FOO} --dry-run=client -o yaml &gt; ${LOCAL_GITHUB_REPOS}/k8s-kind-fluxcd/apps/monitoring/discord-webhook.secret.yaml\n\n\n# Notification Discord : d\u00e9finition de l'alert-provider :\nflux create alert-provider discord \\\n  --type=discord \\\n  --secret-ref=discord-webhook \\\n  --channel=monitoring \\\n  --username=FluxCD \\\n  --namespace=monitoring \\\n  --export &gt; ${LOCAL_GITHUB_REPOS}/k8s-kind-fluxcd/apps/monitoring/notification-provider.yaml\n\n\n# Notification Discord : d\u00e9finition des alertes :\nflux create alert discord \\\n  --event-severity=info \\\n  --event-source='GitRepository/*,Kustomization/*,ImageRepository/*,ImagePolicy/*,HelmRepository/*,HelmRelease/*' \\\n  --provider-ref=discord \\\n  --namespace=monitoring \\\n  --export &gt;  ${LOCAL_GITHUB_REPOS}/k8s-kind-fluxcd/apps/monitoring/notification-alert.yaml\n\n\n# Helm repository :\nflux create source helm prometheus-community \\\n  --url=https://prometheus-community.github.io/helm-charts \\\n  --namespace=monitoring \\\n  --interval=1m \\\n  --export &gt; ${LOCAL_GITHUB_REPOS}/k8s-kind-fluxcd/apps/monitoring/helm-repository.yaml\n\n\n# Helm Release :\nflux create helmrelease kube-prometheus-stack \\\n  --source=HelmRepository/prometheus-community \\\n  --chart=kube-prometheus-stack \\\n  --namespace=monitoring \\\n  --export &gt; ${LOCAL_GITHUB_REPOS}/k8s-kind-fluxcd/apps/monitoring/helm-release.yaml\n\n\n# Prise en compte des modifications :\ncd ${LOCAL_GITHUB_REPOS}/k8s-kind-fluxcd\ngit add apps/monitoring\ngit commit -m \"feat: init monitoring (namespace, alerting Discord, helm repo and release).\"\ngit push\nflux reconcile kustomization flux-system --with-source\n</code></pre> <p>Tout de suite Discord nous informe du bon d\u00e9ploiement de la Helm release : </p> <p></p> <p>V\u00e9rifions malgr\u00e9 tout notre installation :</p> <pre><code>kubectl get ns monitoring\n\n  # NAME         STATUS   AGE\n  # monitoring   Active   49s\n\n\nkubectl -n monitoring get helmrepositories,helmreleases\n\n  # NAME                                                           URL                                                  AGE   READY   STATUS\n  # helmrepository.source.toolkit.fluxcd.io/prometheus-community   https://prometheus-community.github.io/helm-charts   80s   True    stored artifact: revision 'sha256:10ee9c60cbd4bf6ec4d73e99b80c5c54ca1600edfaea593f0f65f2a92ba1b35d'\n  # \n  # NAME                                                       AGE   READY   STATUS\n  # helmrelease.helm.toolkit.fluxcd.io/kube-prometheus-stack   80s   True    Release reconciliation succeeded\n\n\nkubectl -n monitoring get all\n\n  # NAME                                                            READY   STATUS    RESTARTS   AGE\n  # pod/alertmanager-kube-prometheus-stack-alertmanager-0           2/2     Running   0          97s\n  # pod/kube-prometheus-stack-grafana-86844f6b47-s7wph              3/3     Running   0          98s\n  # pod/kube-prometheus-stack-kube-state-metrics-7c8d64d446-d4fgk   1/1     Running   0          98s\n  # pod/kube-prometheus-stack-operator-75fc8896c7-4stcp             1/1     Running   0          98s\n  # pod/kube-prometheus-stack-prometheus-node-exporter-xmb9j        1/1     Running   0          98s\n  # pod/prometheus-kube-prometheus-stack-prometheus-0               2/2     Running   0          97s\n  # \n  # NAME                                                     TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                      AGE\n  # service/alertmanager-operated                            ClusterIP   None            &lt;none&gt;        9093/TCP,9094/TCP,9094/UDP   97s\n  # service/kube-prometheus-stack-alertmanager               ClusterIP   10.96.222.244   &lt;none&gt;        9093/TCP,8080/TCP            98s\n  # service/kube-prometheus-stack-grafana                    ClusterIP   10.96.24.39     &lt;none&gt;        80/TCP                       98s\n  # service/kube-prometheus-stack-kube-state-metrics         ClusterIP   10.96.99.205    &lt;none&gt;        8080/TCP                     98s\n  # service/kube-prometheus-stack-operator                   ClusterIP   10.96.119.105   &lt;none&gt;        443/TCP                      98s\n  # service/kube-prometheus-stack-prometheus                 ClusterIP   10.96.52.206    &lt;none&gt;        9090/TCP,8080/TCP            98s\n  # service/kube-prometheus-stack-prometheus-node-exporter   ClusterIP   10.96.28.184    &lt;none&gt;        9100/TCP                     98s\n  # service/prometheus-operated                              ClusterIP   None            &lt;none&gt;        9090/TCP                     97s\n  # \n  # NAME                                                            DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE\n  # daemonset.apps/kube-prometheus-stack-prometheus-node-exporter   1         1         1       1            1           kubernetes.io/os=linux   98s\n  # \n  # NAME                                                       READY   UP-TO-DATE   AVAILABLE   AGE\n  # deployment.apps/kube-prometheus-stack-grafana              1/1     1            1           98s\n  # deployment.apps/kube-prometheus-stack-kube-state-metrics   1/1     1            1           98s\n  # deployment.apps/kube-prometheus-stack-operator             1/1     1            1           98s\n  # \n  # NAME                                                                  DESIRED   CURRENT   READY   AGE\n  # replicaset.apps/kube-prometheus-stack-grafana-86844f6b47              1         1         1       98s\n  # replicaset.apps/kube-prometheus-stack-kube-state-metrics-7c8d64d446   1         1         1       98s\n  # replicaset.apps/kube-prometheus-stack-operator-75fc8896c7             1         1         1       98s\n  # \n  # NAME                                                               READY   AGE\n  # statefulset.apps/alertmanager-kube-prometheus-stack-alertmanager   1/1     97s\n  # statefulset.apps/prometheus-kube-prometheus-stack-prometheus       1/1     97s\n</code></pre> <p>Nous avons d\u00e9ploy\u00e9 la stack avec ses valeurs par d\u00e9faut. Regardons tout de suite si nous pouvons effectivement nous connecter \u00e0 Grafana avec le login et le mode de passe par d\u00e9faut du compte d'administration.</p> <p>Pour identifier le mot de passe, nous devons r\u00e9cup\u00e9rer le fichier 'values.yaml' du Helm Chart utilis\u00e9 :</p> <pre><code>export LOCAL_GITHUB_REPOS=\"${HOME}/code/github\"\nhelm show values prometheus-community/kube-prometheus-stack &gt; ${LOCAL_GITHUB_REPOS}/k8s-kind-fluxcd/apps/monitoring/kube-prometheus-stack.default.values.txt\ncat ${LOCAL_GITHUB_REPOS}/k8s-kind-fluxcd/apps/monitoring/kube-prometheus-stack.default.values.txt | yq .grafana.adminPassword\n\n  # prom-operator\n</code></pre> <p>Le mot de passe propos\u00e9 par d\u00e9faut pour le compte 'admin' de Grafana est donc : 'prom-operator'.</p> <p>Note</p> <p>Le fichier 'values.yaml' r\u00e9cup\u00e9r\u00e9 est enregistr\u00e9 en '.txt' et non en '.yaml' pour qu'il ne soit pas interpr\u00e9t\u00e9 plus tard par FluxCD.</p> <p>Tentons une connexion \u00e0 Grafana avec le compte d'administration :</p> <pre><code># Identification du service Grafana (nom et port TCP) :\nkubectl -n monitoring get services | grep -i grafana\n\n  # kube-prometheus-stack-grafana                    ClusterIP   10.96.24.39     &lt;none&gt;        80/TCP                       25m\n\n# Port-forwarding\nkubectl -n monitoring port-forward service/kube-prometheus-stack-grafana 8080:80\n</code></pre> <p>Ouvrons enfin un navigateur \u00e0 l'URL suivante :  <code>http://localhost:8080</code></p> <p></p> <p>Success</p> <p>nous acc\u00e9dons \u00e0 Grafana avec le compte 'admin' et le mot de passe par d\u00e9faut 'prom-operator' </p>"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/vault_auto-unsealed_external-secrets-operator_fluxcd/#modification-du-mot-de-passe-du-compte-dadministration-de-grafana","title":"Modification du mot de passe du compte d'administration de Grafana","text":"<p>Nous allons produire un fichier contenant le seul param\u00e8tre que nous souhaitons surcharger aux valeurs par d\u00e9faut du Helm chart :</p> <pre><code>export LOCAL_GITHUB_REPOS=\"${HOME}/code/github\"\ncat &lt;&lt; EOF &gt;&gt; ${LOCAL_GITHUB_REPOS}/k8s-kind-fluxcd/apps/monitoring/kube-prometheus-stack.custom.values.txt\ngrafana:\n  adminPassword: my-cleartext-custom-password\nEOF\n</code></pre> <p>Appliquons ce nouveau mot de passe \u00e0 notre Helm Release d\u00e9j\u00e0 d\u00e9ploy\u00e9e :</p> codehelm release avant modificationhelm release apr\u00e8s modification <pre><code>export LOCAL_GITHUB_REPOS=\"${HOME}/code/github\"\n\nflux create helmrelease kube-prometheus-stack \\\n  --source=HelmRepository/prometheus-community \\\n  --chart=kube-prometheus-stack \\\n  --namespace=monitoring \\\n  --values=${LOCAL_GITHUB_REPOS}/k8s-kind-fluxcd/apps/monitoring/kube-prometheus-stack.custom.values.txt \\\n  --export &gt; ${LOCAL_GITHUB_REPOS}/k8s-kind-fluxcd/apps/monitoring/helm-release.yaml\n</code></pre> <pre><code>---\napiVersion: helm.toolkit.fluxcd.io/v2beta1\nkind: HelmRelease\nmetadata:\n  name: kube-prometheus-stack\n  namespace: monitoring\nspec:\n  chart:\n    spec:\n      chart: kube-prometheus-stack\n      reconcileStrategy: ChartVersion\n      sourceRef:\n        kind: HelmRepository\n        name: prometheus-community\n  interval: 1m0s\n</code></pre> <pre><code>---\napiVersion: helm.toolkit.fluxcd.io/v2beta1\nkind: HelmRelease\nmetadata:\n  name: kube-prometheus-stack\n  namespace: monitoring\nspec:\n  chart:\n    spec:\n      chart: kube-prometheus-stack\n      reconcileStrategy: ChartVersion\n      sourceRef:\n        kind: HelmRepository\n        name: prometheus-community\n  interval: 1m0s\n  values:\n    grafana:\n      adminPassword: my-cleartext-custom-password\n</code></pre> <p>Appliquons les changements :</p> <pre><code>export LOCAL_GITHUB_REPOS=\"${HOME}/code/github\"\n\ncd ${LOCAL_GITHUB_REPOS}/k8s-kind-fluxcd\n\ngit add .\ngit commit -m \"feat: manually changed the password for grafana's admin account.\"\ngit push\n\nflux reconcile kustomization flux-system --with-source\n</code></pre> <p>Nous observons que les pods Grafana redescendent :</p> <pre><code>kubectl -n monitoring get po -w\n\n  # NAME                                                        READY   STATUS    RESTARTS   AGE\n  # alertmanager-kube-prometheus-stack-alertmanager-0           2/2     Running   0          37m\n  # kube-prometheus-stack-grafana-86844f6b47-s7wph              3/3     Running   0          37m\n  # kube-prometheus-stack-kube-state-metrics-7c8d64d446-d4fgk   1/1     Running   0          37m\n  # kube-prometheus-stack-operator-75fc8896c7-4stcp             1/1     Running   0          37m\n  # kube-prometheus-stack-prometheus-node-exporter-xmb9j        1/1     Running   0          37m\n  # prometheus-kube-prometheus-stack-prometheus-0               2/2     Running   0          37m\n  # kube-prometheus-stack-admission-create-784kh                0/1     Pending   0          0s\n  # kube-prometheus-stack-admission-create-784kh                0/1     Pending   0          1s\n  # kube-prometheus-stack-admission-create-784kh                0/1     ContainerCreating   0          2s\n  # kube-prometheus-stack-admission-create-784kh                1/1     Running             0          18s\n  # kube-prometheus-stack-admission-create-784kh                0/1     Completed           0          24s\n  # kube-prometheus-stack-admission-create-784kh                0/1     Completed           0          30s\n  # kube-prometheus-stack-admission-create-784kh                0/1     Completed           0          30s\n  # kube-prometheus-stack-admission-create-784kh                0/1     Completed           0          32s\n  # kube-prometheus-stack-admission-create-784kh                0/1     Terminating         0          33s\n  # kube-prometheus-stack-admission-create-784kh                0/1     Terminating         0          33s\n  # kube-prometheus-stack-grafana-57595d7d49-wrt4s              0/3     Pending             0          0s\n  # kube-prometheus-stack-grafana-57595d7d49-wrt4s              0/3     Pending             0          0s\n  # kube-prometheus-stack-grafana-57595d7d49-wrt4s              0/3     ContainerCreating   0          1s\n  # kube-prometheus-stack-grafana-57595d7d49-wrt4s              2/3     Running             0          30s\n  # kube-prometheus-stack-grafana-57595d7d49-wrt4s              2/3     Running             1 (10s ago)   3m13s\n  # kube-prometheus-stack-grafana-57595d7d49-wrt4s              3/3     Running             1 (30s ago)   3m33s\n  # kube-prometheus-stack-grafana-86844f6b47-s7wph              3/3     Terminating         0             46m\n  # kube-prometheus-stack-grafana-86844f6b47-s7wph              0/3     Terminating         0             46m\n  # kube-prometheus-stack-grafana-86844f6b47-s7wph              0/3     Terminating         0             46m\n  # kube-prometheus-stack-grafana-86844f6b47-s7wph              0/3     Terminating         0             46m\n  # kube-prometheus-stack-grafana-86844f6b47-s7wph              0/3     Terminating         0             46m\n  # kube-prometheus-stack-admission-patch-grgxs                 0/1     Pending             0             0s\n  # kube-prometheus-stack-admission-patch-grgxs                 0/1     Pending             0             0s\n  # kube-prometheus-stack-admission-patch-grgxs                 0/1     ContainerCreating   0             0s\n  # kube-prometheus-stack-admission-patch-grgxs                 1/1     Running             0             5s\n  # kube-prometheus-stack-admission-patch-grgxs                 0/1     Completed           0             6s\n  # kube-prometheus-stack-admission-patch-grgxs                 0/1     Completed           0             7s\n  # kube-prometheus-stack-admission-patch-grgxs                 0/1     Completed           0             8s\n  # kube-prometheus-stack-admission-patch-grgxs                 0/1     Completed           0             9s\n  # kube-prometheus-stack-admission-patch-grgxs                 0/1     Terminating         0             9s\n  # kube-prometheus-stack-admission-patch-grgxs                 0/1     Terminating         0             9s\n</code></pre> <p>Discord nous informe \u00e9galement de la bonne mise \u00e0 jour de la Helm release :</p> <p></p> <p>V\u00e9rifions la bonne prise en compte de notre mot de passe 'custom' :</p> <pre><code>kubectl -n monitoring port-forward service/kube-prometheus-stack-grafana 8080:80\n</code></pre> <p>Ouvrons un navigateur sur l'adresse de port-forwarding http://localhost:8080 :</p> <p></p> <p>Success</p> <p>nous acc\u00e9dons \u00e0 Grafana avec le compte 'admin' et le mot de passe 'my-cleartext-custom-password' </p>"},{"location":"Vault/vault_auto-unseal_external-secrets-operator_fluxcd/vault_auto-unsealed_external-secrets-operator_fluxcd/#protection-du-mot-de-passe-avec-vault-et-external-secrets-operator-eso","title":"Protection du mot de passe avec Vault et External Secrets Operator (ESO)","text":"<p>Jusque-l\u00e0, rien de bien sorcier : nous avons simplement demand\u00e9 \u00e0 FluxCD de d\u00e9ployer une Helm Release avec des custom values.</p> <p>Mais nous n'avons pas r\u00e9gl\u00e9 notre probl\u00e8me : si le mot de passe n'est plus celui par-d\u00e9faut, il reste en clair dans un fichier au milieu de notre d\u00e9p\u00f4t de code. Et c'est plut\u00f4t moche.</p> <p>Pour le r\u00e9soudre, nous allons faire usage de l'op\u00e9rateur 'External Secrets'.</p> <p>Info</p> <p>https://blog.gitguardian.com/how-to-handle-secrets-in-helm/#external-secrets-operator</p> <p>\"ESO r\u00e9cup\u00e8re automatiquement les 'secrets managers' via des API externes et les injecte dans Kubernetes Secrets.</p> <p>Contrairement \u00e0 helm-secrets qui fait r\u00e9f\u00e9rence \u00e0 des secrets stock\u00e9s dans des 'Cloud secrets managers' dans le fichier 'values', ESO ne n\u00e9cessite pas d'inclure secrets.yaml dans les 'Helm templates'. Il utilise une autre ressource personnalis\u00e9e 'ExternalSecret', qui contient la r\u00e9f\u00e9rence aux gestionnaires de secrets dans le Cloud.\"</p> <p>Info</p> <p>https://external-secrets.io/latest/guides/templating/#templatefrom</p> <p>https://fluxcd.io/flux/cmd/flux_create_helmrelease/#options</p> <p>Il est possible de d\u00e9finir une 'Helm Release' avec la CLI 'flux' en surchargeant les 'default values' \u00e0 partir d'un objet Kubernetes de type 'Secret' ou 'ConfigMap'.</p> <p>Nous allons (re)d\u00e9finir notre 'Helm Release' 'kube-prometheus-stack' en lui indiquant de r\u00e9cup\u00e9rer ses 'custom values' depuis un 'Secret Kubernetes'. </p> <p>Ce 'Secret' (ie. le fichier YAML qui surchargera les valeurs par d\u00e9faut du Helm Chart) aura pr\u00e9alablement \u00e9t\u00e9 forg\u00e9 par l''External Secrets Operator' en r\u00e9cup\u00e9rant le mot de passe du compte d'administration de Grafana depuis Vault et en l'appliquant \u00e0 un template stock\u00e9 dans un objet ConfigMap. Voyons \u00e7a de plus pr\u00e8s...</p> codekube-prometheus-stack.custom.values.ESO.txt <pre><code>export LOCAL_GITHUB_REPOS=\"${HOME}/code/github\"\n\n# D\u00e9finissons le template \u00e0 partir duquel le fichier de *custom values* de la *Helm release* sera g\u00e9n\u00e9r\u00e9 :\ncat &lt;&lt; EOF &gt; ${LOCAL_GITHUB_REPOS}/k8s-kind-fluxcd/apps/monitoring/kube-prometheus-stack.custom.values.ESO.txt\ngrafana:\n  adminPassword: {{ .grafanaadminpassword }}\nEOF\n\n# Encodons ce fichier en base 64 :\ncat ${LOCAL_GITHUB_REPOS}/k8s-kind-fluxcd/apps/monitoring/kube-prometheus-stack.custom.values.ESO.txt | base64\n\n  # Z3JhZmFuYToKICBhZG1pblBhc3N3b3JkOiB7eyAuZ3JhZmFuYWFkbWlucGFzc3dvcmQgfX0K\n</code></pre> <pre><code>grafana:\n  adminPassword: {{ .grafanaadminpassword }}\n</code></pre> <p>D\u00e9finissons les objets n\u00e9cessaires \u00e0 la cr\u00e9ation du Secret Kubernetes contenant le fichier 'values.yaml' qui viendra surcharger les valeurs par \u00e9faut de notre *Helm release :</p> <p>Warning</p> <p>Il est important que la ConfigMap cr\u00e9\u00e9e ait comme cl\u00e9 'values.yaml', car c'est le fichier attendu par la Helm Release pour surcharger ses valeurs par d\u00e9faut !</p> <p>2 objets sont n\u00e9cessaires :</p> <ul> <li>la ConfigMap qui contiendra le template de fichier YAML \u00e0 produire ;</li> <li>l'ExternalSecret permettant de g\u00e9n\u00e9rer le Secret Kubernetes avec le template et le mot de passe stock\u00e9 dans Vault.</li> </ul> codekube-prometheus-stack.custom.values.yaml <pre><code>export LOCAL_GITHUB_REPOS=\"${HOME}/code/github\"\n\ncat &lt;&lt; EOF &gt; ${LOCAL_GITHUB_REPOS}/k8s-kind-fluxcd/apps/monitoring/kube-prometheus-stack.custom.values.yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: kube-prometheus-stack-custom-values-configmap\n  namespace: monitoring\ndata:\n  values.yaml: |\n    grafana:\n      adminPassword: \"{{ .grafanaadminpassword }}\"\n\n---\napiVersion: external-secrets.io/v1beta1\nkind: ExternalSecret\nmetadata:\n  name: kube-prometheus-stack-custom-values-externalsecret\n  namespace: monitoring\nspec:\n  secretStoreRef:\n    kind: SecretStore\n    name: grafana\n  target:\n    name: kube-prometheus-stack-custom-values\n    template:\n      engineVersion: v2\n      templateFrom:\n      - configMap:\n          name: kube-prometheus-stack-custom-values-configmap\n          items:\n          - key: values.yaml\n            templateAs: Values\n  data:\n  - secretKey: grafanaadminpassword\n    remoteRef:\n      key: kv/monitoring/grafana/admin-account\n      property: password\nEOF\n</code></pre> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: kube-prometheus-stack-custom-values-configmap\n  namespace: monitoring\ndata:\n  values.yaml: |\n    grafana:\n      adminPassword: \"{{ .grafanaadminpassword }}\"\n\n---\napiVersion: external-secrets.io/v1beta1\nkind: ExternalSecret\nmetadata:\n  name: kube-prometheus-stack-custom-values-externalsecret\n  namespace: monitoring\nspec:\n  secretStoreRef:\n    kind: SecretStore\n    name: grafana\n  target:\n    name: kube-prometheus-stack-custom-values\n    template:\n      engineVersion: v2\n      templateFrom:\n      - configMap:\n          name: kube-prometheus-stack-custom-values-configmap\n          items:\n          - key: values.yaml\n            templateAs: Values\n  data:\n  - secretKey: grafanaadminpassword\n    remoteRef:\n      key: kv/monitoring/grafana/admin-account\n      property: password\n</code></pre> <p>Il ne nous reste plus qu'\u00e0 d\u00e9finir notre 'HelmRelease' en lui indiquant qu'il doit r\u00e9cup\u00e9rer ses 'custom values' (values.yaml) depuis un objet Kubernetes de type 'ConfigMap' que nous venons de d\u00e9finir plus haut :</p> codeversion pr\u00e9c\u00e9dentenouvelle d\u00e9finition <pre><code>export LOCAL_GITHUB_REPOS=\"${HOME}/code/github\"\n\nflux create helmrelease kube-prometheus-stack \\\n  --source=HelmRepository/prometheus-community \\\n  --chart=kube-prometheus-stack \\\n  --namespace=monitoring \\\n  --values-from=Secret/kube-prometheus-stack-custom-values \\\n  --export &gt; ${LOCAL_GITHUB_REPOS}/k8s-kind-fluxcd/apps/monitoring/helm-release.yaml\n</code></pre> <pre><code>---\napiVersion: helm.toolkit.fluxcd.io/v2beta1\nkind: HelmRelease\nmetadata:\n  name: kube-prometheus-stack\n  namespace: monitoring\nspec:\n  chart:\n    spec:\n      chart: kube-prometheus-stack\n      reconcileStrategy: ChartVersion\n      sourceRef:\n        kind: HelmRepository\n        name: prometheus-community\n  interval: 1m0s\n  values:\n    grafana:\n      adminPassword: my-cleartext-custom-password\n</code></pre> <pre><code>---\napiVersion: helm.toolkit.fluxcd.io/v2beta1\nkind: HelmRelease\nmetadata:\n  name: kube-prometheus-stack\n  namespace: monitoring\nspec:\n  chart:\n    spec:\n      chart: kube-prometheus-stack\n      reconcileStrategy: ChartVersion\n      sourceRef:\n        kind: HelmRepository\n        name: prometheus-community\n  interval: 1m0s\n  valuesFrom:\n  - kind: Secret\n    name: kube-prometheus-stack-custom-values\n</code></pre> <p>Appliquons les modifications sur notre cluster :</p> <pre><code>export LOCAL_GITHUB_REPOS=\"${HOME}/code/github\"\n\ncd ${LOCAL_GITHUB_REPOS}/k8s-kind-fluxcd\n\ngit add .\ngit commit -m \"feat: Helm release is now using vaulted custom values.\"\ngit push\n\nflux reconcile kustomization flux-system --with-source\n</code></pre> <p>Discord nous informe de la bonne mise \u00e0 jour de notre stack Prometheus :</p> <p></p> <p>V\u00e9rifions la bonne prise en compte du mot de passe stock\u00e9 dans Vault :</p> <pre><code>kubectl -n monitoring port-forward service/kube-prometheus-stack-grafana 8080:80\n</code></pre> <p></p> <p>Success</p> <p>Nous acc\u00e9dons \u00e0 Grafana avec le compte 'admin' et le mot de passe 'my-vaulted-custom-password' contenu dans Vault !  </p>"}]}